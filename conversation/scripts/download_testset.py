"""
ä¸‹è½½ LongEmotion æµ‹è¯•é›†
ä» Hugging Face ä¸‹è½½ LongEmotion/LongEmotion æ•°æ®é›†çš„ emotion_conversation å­é›†
"""
import argparse
import json
import sys
from pathlib import Path
from datasets import load_dataset

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))


def download_emotion_conversation_dataset(
    output_file: str = "data/longemotion_testset.json",
    split: str = "default",
    max_samples: int = None
):
    """
    ä¸‹è½½ emotion_conversation æ•°æ®é›†
    
    Args:
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        split: æ•°æ®é›†åˆ†å‰²ï¼ˆdefault, train, test, validationï¼‰
        max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
    """
    print("=" * 60)
    print("LongEmotion æµ‹è¯•é›†ä¸‹è½½å™¨")
    print("=" * 60)
    print(f"æ•°æ®é›†: LongEmotion/LongEmotion")
    print(f"å­é›†: emotion_conversation")
    print(f"åˆ†å‰²: {split}")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®é›†
    print("\næ­£åœ¨ä» Hugging Face ä¸‹è½½æ•°æ®é›†...")
    try:
        # å°è¯•ä½¿ç”¨ subset å‚æ•°
        try:
            if split and split != "default":
                dataset = load_dataset(
                    "LongEmotion/LongEmotion",
                    subset="emotion_conversation",
                    split=split,
                    trust_remote_code=True
                )
            else:
                # å¦‚æœæ˜¯ defaultï¼Œå°è¯•åŠ è½½æ•´ä¸ªæ•°æ®é›†
                dataset_dict = load_dataset(
                    "LongEmotion/LongEmotion",
                    subset="emotion_conversation",
                    trust_remote_code=True
                )
                # å¦‚æœæœ‰å¤šä¸ªåˆ†å‰²ï¼Œä½¿ç”¨ default æˆ–ç¬¬ä¸€ä¸ª
                if isinstance(dataset_dict, dict):
                    if "default" in dataset_dict:
                        dataset = dataset_dict["default"]
                    elif "train" in dataset_dict:
                        dataset = dataset_dict["train"]
                    else:
                        dataset = list(dataset_dict.values())[0]
                else:
                    dataset = dataset_dict
        except Exception as e1:
            print(f"ä½¿ç”¨ subset å‚æ•°åŠ è½½å¤±è´¥: {e1}")
            print("å°è¯•ç›´æ¥åŠ è½½æ•°æ®é›†...")
            # å°è¯•ç›´æ¥åŠ è½½
            dataset_dict = load_dataset(
                "LongEmotion/LongEmotion",
                split=split if split != "default" else None,
                trust_remote_code=True
            )
            if isinstance(dataset_dict, dict):
                # å¦‚æœæœ‰å¤šä¸ªåˆ†å‰²ï¼Œå°è¯•æ‰¾åˆ° emotion_conversation
                if "emotion_conversation" in dataset_dict:
                    dataset = dataset_dict["emotion_conversation"]
                elif "default" in dataset_dict:
                    dataset = dataset_dict["default"]
                else:
                    dataset = list(dataset_dict.values())[0]
            else:
                dataset = dataset_dict
        
        print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ")
        print(f"æ•°æ®é›†åˆ—å: {dataset.column_names}")
        print(f"æ•°æ®é›†å¤§å°: {len(dataset)} æ¡è®°å½•")
        
        # é™åˆ¶æ ·æœ¬æ•°
        if max_samples and max_samples > 0:
            dataset = dataset.select(range(min(max_samples, len(dataset))))
            print(f"é™åˆ¶ä¸ºå‰ {len(dataset)} æ¡è®°å½•")
        
        # æ˜¾ç¤ºç¬¬ä¸€æ¡æ ·æœ¬çš„ç»“æ„
        if len(dataset) > 0:
            print("\nç¬¬ä¸€æ¡æ ·æœ¬ç»“æ„:")
            sample = dataset[0]
            print(f"  ID: {sample.get('id', 'N/A')}")
            print(f"  conversation_history é•¿åº¦: {len(str(sample.get('conversation_history', '')))} å­—ç¬¦")
            print(f"  æ‰€æœ‰å­—æ®µ: {list(sample.keys())}")
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        print("\næ­£åœ¨è½¬æ¢æ•°æ®æ ¼å¼...")
        data_list = []
        for idx, item in enumerate(dataset):
            data_item = {
                "id": item.get('id', idx),
                "conversation_history": item.get('conversation_history', '')
            }
            data_list.append(data_item)
        
        # ä¿å­˜ä¸º JSON æ–‡ä»¶
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        print(f"\næ­£åœ¨ä¿å­˜åˆ°: {output_path}")
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data_list, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ä¸‹è½½å®Œæˆï¼")
        print(f"ğŸ“ ä¿å­˜ä½ç½®: {output_path}")
        print(f"ğŸ“Š æ€»è®°å½•æ•°: {len(data_list)}")
        
        return data_list
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    parser = argparse.ArgumentParser(description="ä¸‹è½½ LongEmotion æµ‹è¯•é›†")
    parser.add_argument(
        "--output_file",
        type=str,
        default="data/longemotion_testset.json",
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: data/longemotion_testset.jsonï¼‰"
    )
    parser.add_argument(
        "--split",
        type=str,
        default="default",
        help="æ•°æ®é›†åˆ†å‰²ï¼ˆdefault, train, test, validationï¼Œé»˜è®¤: defaultï¼‰"
    )
    parser.add_argument(
        "--max_samples",
        type=int,
        default=None,
        help="æœ€å¤§æ ·æœ¬æ•°ï¼ˆå¯é€‰ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰"
    )
    
    args = parser.parse_args()
    
    download_emotion_conversation_dataset(
        output_file=args.output_file,
        split=args.split,
        max_samples=args.max_samples
    )


if __name__ == "__main__":
    main()

