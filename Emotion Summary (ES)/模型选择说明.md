# Emotion Summary (ES) - 模型选择说明

## 🎯 为什么选择 LongT5-base 作为默认模型？

### 任务特点分析

**情绪总结（ES）任务的特殊性：**

1. **输入文本极长** 📝
   - 心理病理报告通常包含详细的病史、症状、治疗过程等
   - 一般长度在2000-10000字符
   - 标准的BERT/T5（512 tokens）无法处理

2. **需要提取多方面信息** 🔍
   - 必须同时关注5个不同方面
   - 需要理解长距离依赖关系
   - 要求模型有强大的语义理解能力

3. **生成质量要求高** ✨
   - GPT-4o会评估事实一致性、完整性、清晰度
   - 需要生成连贯、准确的摘要
   - 不能遗漏关键信息

### LongT5 的优势

#### ✅ 1. 专为长文本设计

```
传统T5:        最大512 tokens    ❌ 无法处理完整报告
LongT5:        最大16,384 tokens ✅ 完全覆盖长文本
```

**技术原理：**
- 使用局部-全局注意力机制（Transient Global Attention）
- 将输入分块处理，同时保持全局理解
- 计算复杂度从O(n²)降低到O(n×k)

#### ✅ 2. 优秀的摘要性能

**在标准摘要数据集上的表现：**

| 数据集 | ROUGE-1 | ROUGE-2 | ROUGE-L |
|--------|---------|---------|---------|
| arXiv  | 44.5    | 16.9    | 39.4    |
| PubMed | 47.2    | 21.3    | 43.1    |
| GovReport | 59.5 | 28.8    | 56.2    |

对比普通T5提升5-10个百分点！

#### ✅ 3. 平衡的资源消耗

**模型对比：**

| 模型 | 参数量 | 最大输入 | 显存需求（训练） | 训练速度 |
|------|--------|---------|----------------|---------|
| T5-base | 220M | 512 | 8GB | ⚡⚡⚡ 快 |
| **LongT5-base** | **250M** | **16k** | **12GB** | **⚡⚡ 中等** |
| LongT5-large | 770M | 16k | 24GB | ⚡ 慢 |
| LED-base | 162M | 16k | 16GB | ⚡⚡ 中等 |
| BART-large | 400M | 1024 | 16GB | ⚡⚡ 中等 |

**结论：** LongT5-base在性能和资源间达到最佳平衡！

#### ✅ 4. 预训练优势

LongT5在以下任务上预训练：
- 长文档摘要
- 问答任务
- 文本理解

**与我们的任务高度相关！** ✨

---

## 🔄 其他可选模型

### 方案1: LED (Longformer Encoder Decoder)

**适用场景：** 超长文本（>8k tokens）

**优点：**
- ✅ 支持最长16k tokens
- ✅ 参数量较少（162M）
- ✅ 高效的滑动窗口注意力

**缺点：**
- ❌ 摘要任务表现略逊于LongT5
- ❌ 训练需要更多调优

**配置方法：**
```python
# 在 config/config.py 中修改
MODEL_NAME = MODEL_OPTIONS["led-base"]
```

---

### 方案2: BART-large-CNN

**适用场景：** 文本不太长（<1024 tokens），追求高质量

**优点：**
- ✅ 在CNN新闻摘要上fine-tune，质量高
- ✅ 生成的摘要流畅自然
- ✅ 社区支持好，资源丰富

**缺点：**
- ❌ 输入长度限制1024 tokens
- ❌ 对于长报告需要截断

**配置方法：**
```python
MODEL_NAME = MODEL_OPTIONS["bart-large-cnn"]
MAX_INPUT_LENGTH = 1024  # 必须调整
```

---

### 方案3: T5-base/large

**适用场景：** 资源受限，快速实验

**优点：**
- ✅ 训练速度快
- ✅ 显存占用少
- ✅ 易于调试

**缺点：**
- ❌ 输入长度限制512 tokens
- ❌ 无法处理完整的长报告

**配置方法：**
```python
MODEL_NAME = MODEL_OPTIONS["t5-base"]
MAX_INPUT_LENGTH = 512  # 必须调整
```

---

### 方案4: Pegasus-large

**适用场景：** 追求极致摘要质量

**优点：**
- ✅ 专门为摘要任务设计
- ✅ 使用Gap Sentence Generation预训练
- ✅ 在多个摘要数据集上SOTA

**缺点：**
- ❌ 参数量大（568M）
- ❌ 训练慢，显存占用高
- ❌ 输入长度限制1024

**配置方法：**
```python
MODEL_NAME = MODEL_OPTIONS["pegasus-large"]
MAX_INPUT_LENGTH = 1024
```

---

## 📊 模型选择决策树

```
输入文本长度？
│
├─ < 512 tokens
│  └─> T5-base (快速，高效)
│
├─ 512-1024 tokens
│  ├─> 追求质量？
│  │  ├─ 是 -> BART-large-CNN
│  │  └─ 否 -> T5-large
│
└─ > 1024 tokens（大部分ES任务）
   ├─> 显存 < 12GB？
   │  ├─ 是 -> LED-base
   │  └─ 否 -> **LongT5-base** ⭐推荐
   │
   └─> 追求极致性能？
      └─> LongT5-large（需要24GB显存）
```

---

## 🎯 推荐配置

### 标准配置（推荐）✨

```python
# config/config.py
MODEL_NAME = "google/long-t5-tglobal-base"
MAX_INPUT_LENGTH = 4096
MAX_OUTPUT_LENGTH = 512
BATCH_SIZE = 4
GRADIENT_ACCUMULATION_STEPS = 4
```

**适用于：** 大多数情况，显存12-16GB

---

### 低配置（显存不足）

```python
MODEL_NAME = "allenai/led-base-16384"
MAX_INPUT_LENGTH = 4096
MAX_OUTPUT_LENGTH = 512
BATCH_SIZE = 2
GRADIENT_ACCUMULATION_STEPS = 8
FP16 = True
```

**适用于：** 显存8-12GB

---

### 高配置（追求性能）

```python
MODEL_NAME = "google/long-t5-tglobal-large"
MAX_INPUT_LENGTH = 8192
MAX_OUTPUT_LENGTH = 512
BATCH_SIZE = 2
GRADIENT_ACCUMULATION_STEPS = 8
FP16 = True
```

**适用于：** 显存24GB+，追求最佳效果

---

## 🔬 实验建议

### 阶段1: 快速验证（1-2天）

1. 使用 **T5-base** 快速训练
2. 验证数据处理流程
3. 确认评估指标

### 阶段2: 正式训练（3-5天）

1. 切换到 **LongT5-base**
2. 完整训练3-5个epoch
3. 调优超参数

### 阶段3: 性能优化（可选）

1. 尝试 **LongT5-large**
2. 数据增强
3. 集成多个模型

---

## 📈 预期效果

### LongT5-base 预期指标

基于类似任务的经验：

```
ROUGE-1:  0.38-0.45  ⭐
ROUGE-2:  0.18-0.25  ⭐
ROUGE-L:  0.33-0.40  ⭐
BLEU:     0.22-0.35  ⭐
```

### 与其他模型对比

| 模型 | ROUGE-L | 训练时间 | 显存 |
|------|---------|---------|------|
| T5-base | 0.25-0.30 | 2小时 | 8GB |
| **LongT5-base** | **0.33-0.40** | **4小时** | **12GB** |
| LongT5-large | 0.38-0.45 | 8小时 | 24GB |
| LED-base | 0.30-0.36 | 5小时 | 16GB |

---

## 💡 最佳实践

1. **开始时使用LongT5-base** - 最佳平衡点
2. **根据显存调整batch size** - 不要强行使用过大的batch
3. **使用混合精度训练(FP16)** - 节省显存
4. **监控验证损失** - 及时停止训练防止过拟合
5. **尝试不同的beam size** - 4-8之间调整

---

## 🔧 切换模型的步骤

1. **修改配置文件**
   ```python
   # config/config.py
   MODEL_NAME = MODEL_OPTIONS["your-choice"]
   ```

2. **调整输入长度**
   ```python
   MAX_INPUT_LENGTH = 适当的值
   ```

3. **调整batch size**
   ```python
   BATCH_SIZE = 根据显存调整
   ```

4. **重新训练**
   ```bash
   python train.py
   ```

---

## 📚 参考资料

- **LongT5论文**: "LongT5: Efficient Text-To-Text Transformer for Long Sequences"
- **LED论文**: "Longformer: The Long-Document Transformer"
- **BART论文**: "BART: Denoising Sequence-to-Sequence Pre-training"

---

## ✅ 总结

**为什么选择LongT5-base？**

1. ✅ 专门为长文本设计，支持4k-16k输入
2. ✅ 摘要任务性能优秀
3. ✅ 资源消耗适中（12GB显存）
4. ✅ 预训练任务与ES高度相关
5. ✅ 社区支持好，文档齐全

**最适合长文本心理病理报告总结任务！** 🎯

---

**更新日期**: 2025-10-30  
**推荐模型**: google/long-t5-tglobal-base ⭐

