# -*- coding: utf-8 -*-
"""æé€Ÿè®­ç»ƒè„šæœ¬ - ä¼˜åŒ–åˆ°æè‡´"""

import json
from pathlib import Path
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq,
)
from datasets import Dataset

# åŠ è½½æ•°æ®
def load_data(file_path, max_samples=None):
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if max_samples and i >= max_samples:
                break
            if line.strip():
                data.append(json.loads(line))
    return data

# æ•°æ®é¢„å¤„ç†
def preprocess_function(examples, tokenizer):
    inputs = []
    targets = []
    
    for ex in examples:
        # è¾“å…¥ï¼šæ¡ˆä¾‹æè¿° + å’¨è¯¢è¿‡ç¨‹ï¼ˆæˆªæ–­ï¼‰
        case_desc = " ".join(ex.get("case_description", []))[:200]  # é™åˆ¶é•¿åº¦
        consult = " ".join(ex.get("consultation_process", []))[:300]  # é™åˆ¶é•¿åº¦
        input_text = case_desc + " " + consult
        inputs.append(input_text)
        
        # è¾“å‡ºï¼šç»éªŒä¸åæ€ï¼ˆæˆªæ–­ï¼‰
        reflection = ex.get("experience_and_reflection", "")[:600]  # é™åˆ¶é•¿åº¦
        targets.append(reflection)
    
    # æçŸ­çš„åºåˆ—é•¿åº¦ä»¥åŠ å¿«é€Ÿåº¦
    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=False)
    labels = tokenizer(targets, max_length=128, truncation=True, padding=False)
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# ä¸»å‡½æ•°
def main():
    print("="*60)
    print("âš¡ æé€Ÿè®­ç»ƒæ¨¡å¼ - mT5-small")
    print("="*60)
    
    # ä½¿ç”¨æ›´å°æ›´å¿«çš„æ¨¡å‹
    model_name = "google/mt5-small"  # 300M å‚æ•°ï¼Œæ¯” base (580M) å¿«ä¸€å€
    
    print(f"\nğŸš€ åŠ è½½æ¨¡å‹: {model_name}")
    print("   (æ›´å°æ›´å¿«çš„æ¨¡å‹)")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    
    # åŠ è½½æŒ‡å®šæ•°é‡çš„æ•°æ®
    print("\nğŸ“Š åŠ è½½æ•°æ®...")
    train_file = Path("data/train/Emotion_Summary.jsonl")
    val_file = Path("data/validation/Emotion_Summary.jsonl")
    
    # è®­ç»ƒé›†8000æ¡ï¼ŒéªŒè¯é›†800æ¡
    train_data = load_data(train_file, max_samples=8000)
    val_data = load_data(val_file, max_samples=800)
    
    print(f"âœ… è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬ï¼ˆé™åˆ¶ï¼‰")
    print(f"âœ… éªŒè¯é›†: {len(val_data)} æ ·æœ¬ï¼ˆé™åˆ¶ï¼‰")
    
    # é¢„å¤„ç†
    print("\nğŸ”„ é¢„å¤„ç†æ•°æ®...")
    
    train_processed = []
    for item in train_data:
        result = preprocess_function([item], tokenizer)
        train_processed.append({
            'input_ids': result['input_ids'][0],
            'attention_mask': result['attention_mask'][0],
            'labels': result['labels'][0]
        })
    
    val_processed = []
    for item in val_data:
        result = preprocess_function([item], tokenizer)
        val_processed.append({
            'input_ids': result['input_ids'][0],
            'attention_mask': result['attention_mask'][0],
            'labels': result['labels'][0]
        })
    
    train_dataset = Dataset.from_list(train_processed)
    val_dataset = Dataset.from_list(val_processed)
    
    # æé€Ÿè®­ç»ƒå‚æ•°
    training_args = Seq2SeqTrainingArguments(
        output_dir="model/mt5_fast",
        num_train_epochs=1,  # åªè®­ç»ƒ1è½®
        per_device_train_batch_size=4,  # å¢åŠ batch size
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=2,  # ç­‰æ•ˆbatch_size=8
        learning_rate=1e-4,  # æ›´é«˜çš„å­¦ä¹ ç‡
        weight_decay=0.01,
        logging_steps=100,
        save_steps=2000,  # æ ¹æ®æ•°æ®é‡è°ƒæ•´
        eval_steps=2000,
        eval_strategy="steps",
        save_total_limit=1,
        predict_with_generate=False,  # å…³é—­ç”Ÿæˆè¯„ä¼°ä»¥åŠ å¿«é€Ÿåº¦
        fp16=False,
        push_to_hub=False,
        report_to="none",
        dataloader_num_workers=0,
        dataloader_pin_memory=False,
        warmup_steps=50,
        max_grad_norm=1.0,
    )
    
    # åˆ›å»º Trainer
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("\nâš¡ å¼€å§‹æé€Ÿè®­ç»ƒ...")
    print("ä¼˜åŒ–é…ç½®:")
    print("  - æ¨¡å‹: mT5-small (300M, æ¯”baseå¿«ä¸€å€)")
    print("  - è®­ç»ƒæ•°æ®: 8000 æ ·æœ¬")
    print("  - éªŒè¯æ•°æ®: 800 æ ·æœ¬")
    print("  - è½®æ•°: 1 epoch")
    print("  - Batch: 4")
    print("  - åºåˆ—é•¿åº¦: 128 tokens")
    print("\né¢„è®¡æ—¶é—´: 45-60 åˆ†é’Ÿ")
    print("-"*60)
    
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
    trainer.save_model("model/mt5_fast/final")
    tokenizer.save_pretrained("model/mt5_fast/final")
    
    print("\n" + "="*60)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: model/mt5_fast/final")
    print("="*60)

if __name__ == "__main__":
    main()

