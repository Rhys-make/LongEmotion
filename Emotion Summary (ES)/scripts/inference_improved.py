# -*- coding: utf-8 -*-
"""æ”¹è¿›ç‰ˆæ¨ç†è„šæœ¬ - æå–å…·ä½“æ¡ˆä¾‹ä¿¡æ¯"""

import json
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from tqdm import tqdm

def create_improved_prompt(case_desc, consult_process, field_name):
    """åˆ›å»ºæ”¹è¿›çš„promptï¼Œå¼ºè°ƒæå–å…·ä½“ä¿¡æ¯"""
    
    # å°†åˆ—è¡¨è½¬ä¸ºæ–‡æœ¬
    case_text = " ".join(case_desc) if isinstance(case_desc, list) else case_desc
    consult_text = " ".join(consult_process) if isinstance(consult_process, list) else consult_process
    
    # å¢åŠ é•¿åº¦é™åˆ¶ï¼Œä¿ç•™æ›´å¤šç»†èŠ‚
    case_text = case_text[:1500]
    consult_text = consult_text[:3500]
    
    # æ ¹æ®å­—æ®µåˆ›å»ºè¯¦ç»†çš„æå–å¼prompt
    prompts = {
        "cause": f"""Extract and summarize the ROOT CAUSE of the psychological problem from this case.
Focus on: underlying issues, family background, past experiences, triggering events.
Provide SPECIFIC DETAILS from the case.

Case: {case_text}

Consultation: {consult_text}

Extracted Cause (provide specific details):""",
        
        "symptoms": f"""Extract and list the SPECIFIC SYMPTOMS displayed by the client.
Include: physical symptoms, behavioral symptoms, psychological symptoms, emotional patterns.
Provide DETAILED descriptions from the case.

Case: {case_text}

Consultation: {consult_text}

Extracted Symptoms (list specific details):""",
        
        "treatment": f"""Extract and describe the TREATMENT PROCESS used in this case.
Include: therapeutic methods, interventions, techniques, session activities.
Provide SPECIFIC steps and approaches mentioned.

Case: {case_text}

Consultation: {consult_text}

Extracted Treatment Process (describe specific methods):""",
        
        "characteristics": f"""Extract the CHARACTERISTICS of this psychological condition.
Include: nature of symptoms, patterns, psychological mechanisms, diagnostic features.
Provide SPECIFIC observations from the case.

Case: {case_text}

Consultation: {consult_text}

Extracted Illness Characteristics (describe specific features):""",
        
        "effect": f"""Extract and describe the TREATMENT OUTCOMES and effects.
Include: emotional changes, behavioral improvements, insights gained, overall progress.
Provide SPECIFIC results mentioned in the case.

Case: {case_text}

Consultation: {consult_text}

Extracted Treatment Effect (describe specific outcomes):"""
    }
    
    return prompts.get(field_name, "")

def generate_single_field(model, tokenizer, prompt, max_length=400):
    """ç”Ÿæˆå•ä¸ªå­—æ®µï¼Œä½¿ç”¨æ›´å¥½çš„å‚æ•°"""
    
    # Tokenize
    inputs = tokenizer(
        prompt,
        max_length=1024,  # å¢åŠ è¾“å…¥é•¿åº¦
        truncation=True,
        padding=False,
        return_tensors="pt"
    )
    
    # ç”Ÿæˆ - ä½¿ç”¨æ›´å¥½çš„å‚æ•°
    outputs = model.generate(
        **inputs,
        max_length=max_length,  # å¢åŠ è¾“å‡ºé•¿åº¦
        min_length=50,  # æœ€å°é•¿åº¦
        num_beams=8,  # å¢åŠ beam search
        early_stopping=True,
        no_repeat_ngram_size=3,  # é¿å…é‡å¤
        length_penalty=1.2,  # é¼“åŠ±ç”Ÿæˆæ›´é•¿çš„è¾“å‡º
        repetition_penalty=1.5,  # æƒ©ç½šé‡å¤
        do_sample=False,  # ä½¿ç”¨è´ªå©ªè§£ç 
    )
    
    # è§£ç 
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    return generated_text.strip()

def main():
    print("="*80)
    print("ğŸ”§ æ”¹è¿›ç‰ˆæ¨ç† - æå–å…·ä½“æ¡ˆä¾‹ä¿¡æ¯")
    print("="*80)
    
    # åŠ è½½æ¨¡å‹
    model_path = "model/mt5_fast/final"
    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_file = Path("data/test/Emotion_Summary.jsonl")
    print(f"\nğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
    
    test_data = []
    with open(test_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                test_data.append(json.loads(line))
    
    print(f"âœ… æµ‹è¯•æ ·æœ¬æ•°: {len(test_data)}")
    
    # æ”¹è¿›è¯´æ˜
    print(f"\nğŸ’¡ æ”¹è¿›è¦ç‚¹:")
    print(f"  1. è¾“å…¥é•¿åº¦: 512 â†’ 1024 tokens (ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡)")
    print(f"  2. è¾“å‡ºé•¿åº¦: 256 â†’ 400 tokens (å…è®¸æ›´è¯¦ç»†çš„è¾“å‡º)")
    print(f"  3. Beamæœç´¢: 4 â†’ 8 (æé«˜ç”Ÿæˆè´¨é‡)")
    print(f"  4. Promptæ”¹è¿›: å¼ºè°ƒ'extract specific details from case'")
    print(f"  5. é‡å¤æƒ©ç½š: repetition_penalty=1.5 (å‡å°‘æ¨¡æ¿åŒ–)")
    
    print(f"\nâ±ï¸  é¢„è®¡æ—¶é—´: ~60-80 åˆ†é’Ÿ")
    print(f"    (æ¯ä¸ªæ ·æœ¬5ä¸ªå­—æ®µ Ã— 150æ ·æœ¬ Ã— 25ç§’/å­—æ®µ)")
    
    # è¿›è¡Œæ¨ç†
    print(f"\nğŸ”® å¼€å§‹æ¨ç†...\n")
    results = []
    
    for item in tqdm(test_data, desc="æ ·æœ¬è¿›åº¦"):
        # è·å–ID
        item_id = item.get("id", 0)
        
        # è·å–è¾“å…¥
        case_desc = item.get("case_description", [])
        consult_process = item.get("consultation_process", [])
        
        # ç”Ÿæˆ5ä¸ªå­—æ®µ
        result = {"id": item_id}
        
        # 1. ç—…å› 
        prompt = create_improved_prompt(case_desc, consult_process, "cause")
        result["predicted_cause"] = generate_single_field(model, tokenizer, prompt, max_length=400)
        
        # 2. ç—‡çŠ¶
        prompt = create_improved_prompt(case_desc, consult_process, "symptoms")
        result["predicted_symptoms"] = generate_single_field(model, tokenizer, prompt, max_length=400)
        
        # 3. æ²»ç–—è¿‡ç¨‹
        prompt = create_improved_prompt(case_desc, consult_process, "treatment")
        result["predicted_treatment_process"] = generate_single_field(model, tokenizer, prompt, max_length=400)
        
        # 4. ç–¾ç—…ç‰¹å¾
        prompt = create_improved_prompt(case_desc, consult_process, "characteristics")
        result["predicted_illness_Characteristics"] = generate_single_field(model, tokenizer, prompt, max_length=400)
        
        # 5. æ²»ç–—æ•ˆæœ
        prompt = create_improved_prompt(case_desc, consult_process, "effect")
        result["predicted_treatment_effect"] = generate_single_field(model, tokenizer, prompt, max_length=400)
        
        results.append(result)
    
    # ä¿å­˜ç»“æœ
    output_file = Path("results/Emotion_Summary_Result.jsonl")
    # å¤‡ä»½æ—§æ–‡ä»¶
    if output_file.exists():
        backup_file = Path("results/Emotion_Summary_Result_old.jsonl")
        output_file.rename(backup_file)
        print(f"\nğŸ“¦ æ—§æ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_file}")
    
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_file}")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    
    print(f"âœ… å®Œæˆï¼å…±ç”Ÿæˆ {len(results)} æ¡ç»“æœ")
    
    # æ˜¾ç¤ºç¤ºä¾‹
    print(f"\nğŸ“ ç¤ºä¾‹è¾“å‡º (æ ·æœ¬ 1):")
    print("="*80)
    sample = results[0]
    
    fields = ["predicted_cause", "predicted_symptoms", "predicted_treatment_process", 
              "predicted_illness_Characteristics", "predicted_treatment_effect"]
    
    for field in fields:
        content = sample[field]
        print(f"\nã€{field}ã€‘")
        print(f"é•¿åº¦: {len(content)} å­—ç¬¦")
        print(f"å†…å®¹: {content[:200]}...")
    
    # éªŒè¯æ”¹è¿›
    print(f"\n" + "="*80)
    print(f"ğŸ” è´¨é‡æ£€æŸ¥")
    print("="*80)
    
    # 1. æ£€æŸ¥é‡å¤
    contents = [sample[field] for field in fields]
    unique = len(set(contents))
    print(f"\n1. é‡å¤åº¦æ£€æŸ¥:")
    print(f"   å”¯ä¸€å†…å®¹æ•°: {unique}/5")
    if unique == 5:
        print(f"   âœ… æ‰€æœ‰å­—æ®µå†…å®¹ä¸åŒ")
    else:
        print(f"   âš ï¸  ä»æœ‰ {5-unique} ä¸ªé‡å¤")
    
    # 2. æ£€æŸ¥é•¿åº¦
    lengths = [len(c) for c in contents]
    avg_len = sum(lengths) / len(lengths)
    print(f"\n2. å†…å®¹é•¿åº¦:")
    print(f"   å¹³å‡: {avg_len:.0f} å­—ç¬¦")
    print(f"   èŒƒå›´: {min(lengths)}-{max(lengths)} å­—ç¬¦")
    if avg_len > 280:
        print(f"   âœ… æ¯”ä¹‹å‰æ›´è¯¦ç»† (ä¹‹å‰å¹³å‡280å­—ç¬¦)")
    
    # 3. æ£€æŸ¥å…³é”®è¯
    keywords = ["adopt", "Shanxi", "hypochondria", "nasopharynx", "throat", 
                "hemorrhoid", "hypnosis", "imagery", "marriage", "wife"]
    
    full_text = " ".join(contents).lower()
    found = [kw for kw in keywords if kw.lower() in full_text]
    
    print(f"\n3. æ¡ˆä¾‹ç»†èŠ‚æå–:")
    print(f"   æå–å…³é”®è¯: {len(found)}/{len(keywords)}")
    if found:
        print(f"   æ‰¾åˆ°: {', '.join(found[:5])}...")
        if len(found) >= 5:
            print(f"   âœ… æå–äº†è¾ƒå¤šæ¡ˆä¾‹ç»†èŠ‚")
        else:
            print(f"   âš ï¸  ä»éœ€æ”¹è¿›")
    else:
        print(f"   âŒ æœªæå–åˆ°æ¡ˆä¾‹ç»†èŠ‚")
    
    print("\n" + "="*80)
    print("âœ… æ¨ç†å®Œæˆï¼")
    print(f"ğŸ“ æäº¤æ–‡ä»¶: {output_file}")
    print("="*80)

if __name__ == "__main__":
    main()

