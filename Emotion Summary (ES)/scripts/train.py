"""
è®­ç»ƒè„šæœ¬ - Emotion Summary (ES) ä»»åŠ¡
è®­ç»ƒæƒ…ç»ªæ€»ç»“æ¨¡å‹
"""

import json
import sys
from pathlib import Path
from typing import Dict, List
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim import AdamW
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    AdamW,
    get_linear_schedule_with_warmup,
)
from tqdm import tqdm
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from config.config import (
    MODEL_NAME,
    TRAIN_FILE,
    VALIDATION_FILE,
    MODEL_CHECKPOINT_DIR,
    MAX_INPUT_LENGTH,
    MAX_OUTPUT_LENGTH,
    BATCH_SIZE,
    EVAL_BATCH_SIZE,
    GRADIENT_ACCUMULATION_STEPS,
    NUM_EPOCHS,
    LEARNING_RATE,
    WEIGHT_DECAY,
    WARMUP_STEPS,
    MAX_GRAD_NORM,
    SAVE_STEPS,
    EVAL_STEPS,
    LOGGING_STEPS,
    DEVICE,
    SEED,
    FP16,
    SUMMARY_ASPECTS,
    T5_PREFIX,
)


class EmotionSummaryDataset(Dataset):
    """
    æƒ…ç»ªæ€»ç»“æ•°æ®é›?
    """
    
    def __init__(
        self,
        data_file: Path,
        tokenizer,
        max_input_length: int,
        max_output_length: int,
        is_test: bool = False
    ):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data_file: æ•°æ®æ–‡ä»¶è·¯å¾„
            tokenizer: åˆ†è¯å™?
            max_input_length: æœ€å¤§è¾“å…¥é•¿åº?
            max_output_length: æœ€å¤§è¾“å‡ºé•¿åº?
            is_test: æ˜¯å¦ä¸ºæµ‹è¯•é›†
        """
        self.tokenizer = tokenizer
        self.max_input_length = max_input_length
        self.max_output_length = max_output_length
        self.is_test = is_test
        
        # åŠ è½½æ•°æ®
        self.samples = self.load_data(data_file)
        
    def load_data(self, data_file: Path) -> List[Dict]:
        """åŠ è½½JSONLæ•°æ®"""
        samples = []
        with open(data_file, 'r', encoding='utf-8') as f:
            for line in f:
                sample = json.loads(line.strip())
                samples.append(sample)
        return samples
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # è¾“å…¥æ–‡æœ¬
        context = sample.get("context", "")
        input_text = f"{T5_PREFIX}{context}"
        
        # Tokenizeè¾“å…¥
        inputs = self.tokenizer(
            input_text,
            max_length=self.max_input_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        
        # å¦‚æœä¸æ˜¯æµ‹è¯•é›†ï¼Œå‡†å¤‡ç›®æ ‡æ–‡æœ¬
        if not self.is_test and "reference_summary" in sample:
            # å°†æ‰€æœ‰æ–¹é¢çš„æ€»ç»“åˆå¹¶ä¸ºä¸€ä¸ªæ–‡æœ?
            reference = sample["reference_summary"]
            target_parts = []
            
            for aspect in SUMMARY_ASPECTS:
                if aspect in reference:
                    target_parts.append(f"{aspect}: {reference[aspect]}")
            
            target_text = " | ".join(target_parts)
            
            # Tokenizeç›®æ ‡
            targets = self.tokenizer(
                target_text,
                max_length=self.max_output_length,
                padding="max_length",
                truncation=True,
                return_tensors="pt"
            )
            
            # å‡†å¤‡labelsï¼?100ä¼šè¢«å¿½ç•¥ï¼?
            labels = targets["input_ids"].clone()
            labels[labels == self.tokenizer.pad_token_id] = -100
            
            return {
                "input_ids": inputs["input_ids"].squeeze(),
                "attention_mask": inputs["attention_mask"].squeeze(),
                "labels": labels.squeeze(),
                "id": sample.get("id", idx)
            }
        else:
            return {
                "input_ids": inputs["input_ids"].squeeze(),
                "attention_mask": inputs["attention_mask"].squeeze(),
                "id": sample.get("id", idx)
            }


def train_epoch(
    model,
    train_loader,
    optimizer,
    scheduler,
    device,
    epoch,
    gradient_accumulation_steps,
    max_grad_norm
):
    """
    è®­ç»ƒä¸€ä¸ªepoch
    """
    model.train()
    total_loss = 0
    optimizer.zero_grad()
    
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch}")
    
    for step, batch in enumerate(progress_bar):
        # ç§»åŠ¨åˆ°è®¾å¤?
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)
        
        # å‰å‘ä¼ æ’­
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        
        loss = outputs.loss
        loss = loss / gradient_accumulation_steps
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        total_loss += loss.item()
        
        # æ¢¯åº¦ç´¯ç§¯
        if (step + 1) % gradient_accumulation_steps == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
        
        # æ›´æ–°è¿›åº¦æ?
        progress_bar.set_postfix({"loss": loss.item() * gradient_accumulation_steps})
    
    avg_loss = total_loss / len(train_loader)
    return avg_loss


def evaluate(model, eval_loader, device):
    """
    è¯„ä¼°æ¨¡å‹
    """
    model.eval()
    total_loss = 0
    
    with torch.no_grad():
        for batch in tqdm(eval_loader, desc="Evaluating"):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)
            
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            
            total_loss += outputs.loss.item()
    
    avg_loss = total_loss / len(eval_loader)
    return avg_loss


def main():
    """
    ä¸»è®­ç»ƒå‡½æ•?
    """
    print("=" * 60)
    print("Emotion Summary (ES) æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(SEED)
    np.random.seed(SEED)
    
    # åŠ è½½tokenizerå’Œæ¨¡å?
    print(f"\nåŠ è½½æ¨¡å‹: {MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
    model.to(DEVICE)
    
    # åˆ›å»ºæ•°æ®é›?
    print("\nåŠ è½½æ•°æ®é›?..")
    train_dataset = EmotionSummaryDataset(
        TRAIN_FILE,
        tokenizer,
        MAX_INPUT_LENGTH,
        MAX_OUTPUT_LENGTH,
        is_test=False
    )
    
    val_dataset = EmotionSummaryDataset(
        VALIDATION_FILE,
        tokenizer,
        MAX_INPUT_LENGTH,
        MAX_OUTPUT_LENGTH,
        is_test=False
    )
    
    print(f"è®­ç»ƒé›? {len(train_dataset)} æ ·æœ¬")
    print(f"éªŒè¯é›? {len(val_dataset)} æ ·æœ¬")
    
    # åˆ›å»ºDataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=0  # Windowsä¸Šè®¾ç½®ä¸º0
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=EVAL_BATCH_SIZE,
        shuffle=False,
        num_workers=0
    )
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™?
    optimizer = AdamW(
        model.parameters(),
        lr=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY
    )
    
    total_steps = len(train_loader) * NUM_EPOCHS // GRADIENT_ACCUMULATION_STEPS
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=WARMUP_STEPS,
        num_training_steps=total_steps
    )
    
    # è®­ç»ƒå¾ªç¯
    print(f"\nå¼€å§‹è®­ç»?..")
    print(f"æ€»è½®æ•? {NUM_EPOCHS}")
    print(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {GRADIENT_ACCUMULATION_STEPS}")
    print(f"å­¦ä¹ ç? {LEARNING_RATE}")
    print(f"è®¾å¤‡: {DEVICE}")
    print()
    
    best_val_loss = float('inf')
    
    for epoch in range(1, NUM_EPOCHS + 1):
        print(f"\n{'=' * 60}")
        print(f"Epoch {epoch}/{NUM_EPOCHS}")
        print('=' * 60)
        
        # è®­ç»ƒ
        train_loss = train_epoch(
            model,
            train_loader,
            optimizer,
            scheduler,
            DEVICE,
            epoch,
            GRADIENT_ACCUMULATION_STEPS,
            MAX_GRAD_NORM
        )
        
        print(f"\nè®­ç»ƒæŸå¤±: {train_loss:.4f}")
        
        # éªŒè¯
        val_loss = evaluate(model, val_loader, DEVICE)
        print(f"éªŒè¯æŸå¤±: {val_loss:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å?
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            print(f"\nâœ?å‘ç°æ›´å¥½çš„æ¨¡å‹ï¼ä¿å­˜åˆ?{MODEL_CHECKPOINT_DIR}")
            MODEL_CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)
            model.save_pretrained(MODEL_CHECKPOINT_DIR)
            tokenizer.save_pretrained(MODEL_CHECKPOINT_DIR)
    
    print("\n" + "=" * 60)
    print("è®­ç»ƒå®Œæˆï¼?)
    print(f"æœ€ä½³éªŒè¯æŸå¤? {best_val_loss:.4f}")
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {MODEL_CHECKPOINT_DIR}")
    print("=" * 60)


if __name__ == "__main__":
    main()

