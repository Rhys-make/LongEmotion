# -*- coding: utf-8 -*-
"""åŸºäºè§„åˆ™çš„æ™ºèƒ½æå–è„šæœ¬ - ç›´æ¥ä»æ¡ˆä¾‹ä¸­æå–5ä¸ªå­—æ®µ"""

import json
import re
from pathlib import Path
from tqdm import tqdm

def extract_cause(case_desc, consult_process):
    """æå–ç—…å› """
    
    full_text = " ".join(case_desc + consult_process)
    
    # å…³é”®è¯å’Œæ¨¡å¼
    cause_keywords = [
        "cause", "reason", "origin", "stem", "root", "trigger", 
        "because", "due to", "result from", "attributed to",
        "èƒŒæ™¯", "åŸå› ", "è¯±å› ", "å¯¼è‡´", "ç”±äº",
        "childhood", "family", "past", "experience", "trauma",
        "father", "mother", "parent", "marriage", "divorce",
        "adoption", "adopted", "biological"
    ]
    
    # æŸ¥æ‰¾åŒ…å«è¿™äº›å…³é”®è¯çš„å¥å­
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', full_text)
    relevant_sentences = []
    
    for sent in sentences:
        if any(kw in sent.lower() for kw in cause_keywords):
            sent = sent.strip()
            if len(sent) > 20:  # è¿‡æ»¤å¤ªçŸ­çš„å¥å­
                relevant_sentences.append(sent)
    
    # å¦‚æœæ‰¾åˆ°ç›¸å…³å¥å­ï¼Œç»„åˆå®ƒä»¬
    if relevant_sentences:
        # å–å‰3-5ä¸ªæœ€ç›¸å…³çš„å¥å­
        result = ". ".join(relevant_sentences[:5])
        if len(result) > 600:
            result = result[:600] + "..."
        return result
    
    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›æ¡ˆä¾‹æè¿°çš„æ‘˜è¦
    case_text = " ".join(case_desc)
    if len(case_text) > 400:
        return case_text[:400] + "..."
    return case_text

def extract_symptoms(case_desc, consult_process):
    """æå–ç—‡çŠ¶"""
    
    full_text = " ".join(case_desc + consult_process)
    
    # ç—‡çŠ¶å…³é”®è¯
    symptom_keywords = [
        "symptom", "suffer", "feel", "experience", "complaint",
        "anxiety", "depres", "insomnia", "sleep", "stress",
        "pain", "discomfort", "worry", "fear", "panic",
        "obsess", "compuls", "avoid", "withdraw",
        "ç—‡çŠ¶", "è¡¨ç°", "æ„Ÿåˆ°", "ç„¦è™‘", "æŠ‘éƒ", "å¤±çœ "
    ]
    
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', full_text)
    relevant_sentences = []
    
    for sent in sentences:
        if any(kw in sent.lower() for kw in symptom_keywords):
            sent = sent.strip()
            if len(sent) > 20:
                relevant_sentences.append(sent)
    
    if relevant_sentences:
        result = ". ".join(relevant_sentences[:6])
        if len(result) > 700:
            result = result[:700] + "..."
        return result
    
    # å¤‡é€‰ï¼šä»å’¨è¯¢è¿‡ç¨‹å¼€å¤´æå–
    if consult_process:
        first_part = consult_process[0]
        if len(first_part) > 400:
            return first_part[:400] + "..."
        return first_part
    
    return "The client presented with psychological distress requiring counseling support."

def extract_treatment_process(case_desc, consult_process):
    """æå–æ²»ç–—è¿‡ç¨‹"""
    
    full_text = " ".join(consult_process)  # ä¸»è¦ä»å’¨è¯¢è¿‡ç¨‹æå–
    
    # æ²»ç–—å…³é”®è¯
    treatment_keywords = [
        "treatment", "therapy", "counseling", "intervention",
        "technique", "method", "approach", "session",
        "hypnosis", "hypnotherapy", "cognitive", "behavioral",
        "guided imagery", "dialogue", "exploration",
        "æ²»ç–—", "å’¨è¯¢", "ç–—æ³•", "å¹²é¢„", "å‚¬çœ "
    ]
    
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', full_text)
    relevant_sentences = []
    
    for sent in sentences:
        if any(kw in sent.lower() for kw in treatment_keywords):
            sent = sent.strip()
            if len(sent) > 20:
                relevant_sentences.append(sent)
    
    if relevant_sentences:
        result = ". ".join(relevant_sentences[:6])
        if len(result) > 700:
            result = result[:700] + "..."
        return result
    
    # å¤‡é€‰ï¼šå–å’¨è¯¢è¿‡ç¨‹ä¸­é—´éƒ¨åˆ†
    if len(consult_process) > 2:
        mid_text = " ".join(consult_process[1:3])
        if len(mid_text) > 500:
            return mid_text[:500] + "..."
        return mid_text
    
    return "The counselor employed therapeutic techniques to address the client's concerns."

def extract_illness_characteristics(case_desc, consult_process):
    """æå–ç–¾ç—…ç‰¹å¾"""
    
    full_text = " ".join(case_desc + consult_process)
    
    # ç‰¹å¾å…³é”®è¯
    char_keywords = [
        "characteristic", "feature", "pattern", "nature",
        "disorder", "condition", "illness", "problem",
        "psychological", "mental", "emotional", "behavioral",
        "persistent", "recurrent", "chronic", "acute",
        "ç‰¹å¾", "ç‰¹ç‚¹", "æ€§è´¨", "æ¨¡å¼"
    ]
    
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', full_text)
    relevant_sentences = []
    
    for sent in sentences:
        if any(kw in sent.lower() for kw in char_keywords):
            sent = sent.strip()
            if len(sent) > 20:
                relevant_sentences.append(sent)
    
    if relevant_sentences:
        result = ". ".join(relevant_sentences[:5])
        if len(result) > 600:
            result = result[:600] + "..."
        return result
    
    # å¤‡é€‰ï¼šä»æ¡ˆä¾‹æè¿°æå–
    if case_desc:
        desc_text = " ".join(case_desc)
        if len(desc_text) > 400:
            return desc_text[:400] + "..."
        return desc_text
    
    return "The case exhibits typical characteristics of psychological distress."

def extract_treatment_effect(case_desc, consult_process):
    """æå–æ²»ç–—æ•ˆæœ"""
    
    full_text = " ".join(consult_process)  # ä¸»è¦ä»å’¨è¯¢è¿‡ç¨‹æå–
    
    # æ•ˆæœå…³é”®è¯
    effect_keywords = [
        "effect", "result", "outcome", "improvement", "progress",
        "better", "improved", "relief", "recovered", "change",
        "successful", "effective", "helpful", "beneficial",
        "after", "following", "post", "finally",
        "æ•ˆæœ", "æ”¹å–„", "å¥½è½¬", "æ¢å¤", "è¿›æ­¥"
    ]
    
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', full_text)
    relevant_sentences = []
    
    # ä¼˜å…ˆæŸ¥æ‰¾æ–‡æœ¬ååŠéƒ¨åˆ†ï¼ˆé€šå¸¸åŒ…å«æ•ˆæœï¼‰
    mid_point = len(sentences) // 2
    for sent in sentences[mid_point:]:
        if any(kw in sent.lower() for kw in effect_keywords):
            sent = sent.strip()
            if len(sent) > 20:
                relevant_sentences.append(sent)
    
    # å¦‚æœååŠéƒ¨åˆ†æ²¡æ‰¾åˆ°ï¼Œå†æŸ¥æ‰¾å‰åŠéƒ¨åˆ†
    if not relevant_sentences:
        for sent in sentences[:mid_point]:
            if any(kw in sent.lower() for kw in effect_keywords):
                sent = sent.strip()
                if len(sent) > 20:
                    relevant_sentences.append(sent)
    
    if relevant_sentences:
        result = ". ".join(relevant_sentences[:5])
        if len(result) > 600:
            result = result[:600] + "..."
        return result
    
    # å¤‡é€‰ï¼šå–å’¨è¯¢è¿‡ç¨‹æœ€åéƒ¨åˆ†
    if consult_process:
        last_text = consult_process[-1]
        if len(last_text) > 400:
            return last_text[:400] + "..."
        return last_text
    
    return "The treatment showed positive effects on the client's condition."

def main():
    print("="*80)
    print("ğŸ”§ åŸºäºè§„åˆ™çš„æ™ºèƒ½æå– - ç›´æ¥ä»æ¡ˆä¾‹æå–5ä¸ªå­—æ®µ")
    print("="*80)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_file = Path("data/test/Emotion_Summary.jsonl")
    print(f"\nğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
    
    test_data = []
    with open(test_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                test_data.append(json.loads(line))
    
    print(f"âœ… æµ‹è¯•æ ·æœ¬æ•°: {len(test_data)}")
    
    print(f"\nğŸ’¡ æå–ç­–ç•¥:")
    print(f"  1. ç—…å› : ä»æ¡ˆä¾‹ä¸­æŸ¥æ‰¾åŒ…å«åŸå› ã€èƒŒæ™¯ã€è¯±å› çš„å¥å­")
    print(f"  2. ç—‡çŠ¶: æŸ¥æ‰¾åŒ…å«ç—‡çŠ¶ã€è¡¨ç°ã€æ„Ÿå—çš„å¥å­")
    print(f"  3. æ²»ç–—è¿‡ç¨‹: ä»å’¨è¯¢è¿‡ç¨‹ä¸­æå–æ²»ç–—æ–¹æ³•å’ŒæŠ€æœ¯")
    print(f"  4. ç–¾ç—…ç‰¹å¾: æŸ¥æ‰¾åŒ…å«ç‰¹å¾ã€æ€§è´¨ã€æ¨¡å¼çš„å¥å­")
    print(f"  5. æ²»ç–—æ•ˆæœ: ä»å’¨è¯¢è¿‡ç¨‹ååŠéƒ¨åˆ†æå–æ”¹å–„å’Œæ•ˆæœ")
    print(f"\nâš¡ é€Ÿåº¦: ~1-2ç§’/æ ·æœ¬ (æ— éœ€æ¨¡å‹æ¨ç†)")
    
    # è¿›è¡Œæå–
    print(f"\nğŸ”® å¼€å§‹æå–...\n")
    results = []
    
    for item in tqdm(test_data, desc="æå–è¿›åº¦"):
        # è·å–IDå’Œè¾“å…¥
        item_id = item.get("id", 0)
        case_desc = item.get("case_description", [])
        consult_process = item.get("consultation_process", [])
        
        # æå–5ä¸ªå­—æ®µ
        result = {
            "id": item_id,
            "predicted_cause": extract_cause(case_desc, consult_process),
            "predicted_symptoms": extract_symptoms(case_desc, consult_process),
            "predicted_treatment_process": extract_treatment_process(case_desc, consult_process),
            "predicted_illness_Characteristics": extract_illness_characteristics(case_desc, consult_process),
            "predicted_treatment_effect": extract_treatment_effect(case_desc, consult_process)
        }
        
        results.append(result)
    
    # å¤‡ä»½æ—§æ–‡ä»¶å¹¶ä¿å­˜æ–°ç»“æœ
    output_file = Path("results/Emotion_Summary_Result.jsonl")
    
    if output_file.exists():
        backup_file = Path("results/Emotion_Summary_Result_v1_model.jsonl")
        import shutil
        shutil.copy(output_file, backup_file)
        print(f"\nğŸ“¦ æ¨¡å‹ç‰ˆæœ¬å·²å¤‡ä»½åˆ°: {backup_file}")
    
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_file} (è¦†ç›–)")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    
    print(f"âœ… å®Œæˆï¼å…±ç”Ÿæˆ {len(results)} æ¡ç»“æœ")
    
    # æ˜¾ç¤ºç¤ºä¾‹
    print(f"\nğŸ“ ç¤ºä¾‹è¾“å‡º (æ ·æœ¬ 1):")
    print("="*80)
    sample = results[0]
    
    fields = [
        ("predicted_cause", "ç—…å› "),
        ("predicted_symptoms", "ç—‡çŠ¶"),
        ("predicted_treatment_process", "æ²»ç–—è¿‡ç¨‹"),
        ("predicted_illness_Characteristics", "ç–¾ç—…ç‰¹å¾"),
        ("predicted_treatment_effect", "æ²»ç–—æ•ˆæœ")
    ]
    
    for field, name in fields:
        content = sample[field]
        print(f"\nã€{name}ã€‘({len(content)} å­—ç¬¦)")
        print(f"{content[:150]}...")
    
    # è´¨é‡æ£€æŸ¥
    print(f"\n" + "="*80)
    print(f"ğŸ” è´¨é‡æ£€æŸ¥")
    print("="*80)
    
    # 1. æ£€æŸ¥é‡å¤
    contents = [sample[field] for field, _ in fields]
    unique = len(set(contents))
    print(f"\n1. é‡å¤åº¦æ£€æŸ¥:")
    print(f"   å”¯ä¸€å†…å®¹æ•°: {unique}/5")
    if unique == 5:
        print(f"   âœ… æ‰€æœ‰å­—æ®µå†…å®¹ä¸åŒ")
    else:
        print(f"   âš ï¸  æœ‰ {5-unique} ä¸ªé‡å¤")
    
    # 2. æ£€æŸ¥é•¿åº¦
    lengths = [len(c) for c in contents]
    avg_len = sum(lengths) / len(lengths)
    print(f"\n2. å†…å®¹é•¿åº¦:")
    print(f"   å¹³å‡: {avg_len:.0f} å­—ç¬¦")
    print(f"   èŒƒå›´: {min(lengths)}-{max(lengths)} å­—ç¬¦")
    
    # 3. æ£€æŸ¥å…³é”®è¯ï¼ˆä»¥ç¬¬ä¸€ä¸ªæ ·æœ¬ä¸ºä¾‹ï¼‰
    keywords = ["adopt", "Shanxi", "hypochondria", "nasopharynx", "throat", 
                "hemorrhoid", "hypnosis", "imagery", "marriage", "wife"]
    
    full_text = " ".join(contents).lower()
    found = [kw for kw in keywords if kw.lower() in full_text]
    
    print(f"\n3. æ¡ˆä¾‹ç»†èŠ‚æå– (æ ·æœ¬1å…³é”®è¯):")
    print(f"   æå–å…³é”®è¯: {len(found)}/{len(keywords)}")
    if found:
        print(f"   æ‰¾åˆ°: {', '.join(found)}")
        if len(found) >= 5:
            print(f"   âœ… æˆåŠŸæå–æ¡ˆä¾‹ç»†èŠ‚")
        else:
            print(f"   âš ï¸  æå–äº†éƒ¨åˆ†ç»†èŠ‚")
    
    print("\n" + "="*80)
    print("âœ… æå–å®Œæˆï¼")
    print(f"ğŸ“ æäº¤æ–‡ä»¶: {output_file}")
    print(f"ğŸ“‹ æ–¹æ³•: åŸºäºè§„åˆ™çš„æ™ºèƒ½æå–")
    print("="*80)

if __name__ == "__main__":
    main()

