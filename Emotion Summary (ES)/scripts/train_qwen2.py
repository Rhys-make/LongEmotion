"""
Qwen2-7B-Instruct è®­ç»ƒè„šæœ¬ - Emotion Summary (ES) ä»»åŠ¡
ä½¿ç”¨ LoRA è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ
"""

import json
import sys
from pathlib import Path
from typing import Dict, List
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from tqdm import tqdm
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from model_qwen2 import Qwen2EmotionSummaryModel


class Qwen2EmotionSummaryDataset(Dataset):
    """
    Qwen2 æƒ…ç»ªæ€»ç»“æ•°æ®é›?
    """
    
    def __init__(
        self,
        data_file: Path,
        tokenizer,
        max_length: int = 8192,
    ):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data_file: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆJSONLæ ¼å¼ï¼?
            tokenizer: Qwen2 tokenizer
            max_length: æœ€å¤§åºåˆ—é•¿åº?
        """
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # åŠ è½½æ•°æ®
        print(f"åŠ è½½æ•°æ®: {data_file}")
        self.samples = self.load_data(data_file)
        print(f"  åŠ è½½äº?{len(self.samples)} ä¸ªæ ·æœ?)
    
    def load_data(self, data_file: Path) -> List[Dict]:
        """åŠ è½½JSONLæ•°æ®"""
        samples = []
        with open(data_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    sample = json.loads(line.strip())
                    samples.append(sample)
        return samples
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # æå–æ•°æ®
        case_description = sample.get("case_description", [])
        consultation_process = sample.get("consultation_process", [])
        experience_and_reflection = sample.get("experience_and_reflection", "")
        
        # æ„å»ºå®Œæ•´çš„è®­ç»ƒæ–‡æœ¬ï¼ˆä½¿ç”¨å¯¹è¯æ¨¡æ¿ï¼?
        case_text = "\n".join(case_description) if isinstance(case_description, list) else case_description
        process_text = "\n".join(consultation_process) if isinstance(consultation_process, list) else consultation_process
        
        input_text = f"""ã€æ¡ˆä¾‹æè¿°ã€?
{case_text}

ã€å’¨è¯¢è¿‡ç¨‹ã€?
{process_text}"""
        
        # ä½¿ç”¨ chat template
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆã€‚è¯·æ ¹æ®æä¾›çš„å¿ƒç†å’¨è¯¢æ¡ˆä¾‹ï¼Œç”Ÿæˆæ·±å…¥ã€ä¸“ä¸šçš„ç»éªŒä¸åæ€æ€»ç»“ã€?
            },
            {
                "role": "user",
                "content": f"è¯·å¯¹ä»¥ä¸‹å¿ƒç†å’¨è¯¢æ¡ˆä¾‹è¿›è¡Œæ·±å…¥åˆ†æå’Œåæ€ï¼š\n\n{input_text}"
            },
            {
                "role": "assistant",
                "content": experience_and_reflection
            }
        ]
        
        # åº”ç”¨ chat template
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )
        
        # Tokenize
        encodings = self.tokenizer(
            text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        input_ids = encodings['input_ids'].squeeze()
        attention_mask = encodings['attention_mask'].squeeze()
        
        # labels ä¸?input_ids ç›¸åŒï¼ˆå› æœè¯­è¨€æ¨¡å‹ï¼?
        labels = input_ids.clone()
        
        # å°?padding token çš?label è®¾ä¸º -100ï¼ˆä¸è®¡ç®—æŸå¤±ï¼?
        labels[attention_mask == 0] = -100
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }


def train_qwen2_model(
    model_name: str = "Qwen/Qwen2-7B-Instruct",
    train_file: Path = None,
    val_file: Path = None,
    output_dir: Path = None,
    num_epochs: int = 3,
    batch_size: int = 2,
    gradient_accumulation_steps: int = 8,
    learning_rate: float = 2e-4,
    max_length: int = 8192,
    use_4bit: bool = True,
    lora_r: int = 64,
    lora_alpha: int = 16,
    lora_dropout: float = 0.05,
):
    """
    è®­ç»ƒ Qwen2 æ¨¡å‹
    
    Args:
        model_name: æ¨¡å‹åç§°
        train_file: è®­ç»ƒæ•°æ®æ–‡ä»¶
        val_file: éªŒè¯æ•°æ®æ–‡ä»¶
        output_dir: è¾“å‡ºç›®å½•
        num_epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        gradient_accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        learning_rate: å­¦ä¹ ç?
        max_length: æœ€å¤§åºåˆ—é•¿åº?
        use_4bit: æ˜¯å¦ä½¿ç”¨4-bité‡åŒ–
        lora_r: LoRA rank
        lora_alpha: LoRA alpha
        lora_dropout: LoRA dropout
    """
    
    print("\n" + "=" * 70)
    print("Qwen2-7B-Instruct å¾®è°ƒè®­ç»ƒ - Emotion Summary ä»»åŠ¡")
    print("=" * 70 + "\n")
    
    # é»˜è®¤è·¯å¾„
    if train_file is None:
        train_file = Path(__file__).parent.parent / "data" / "train" / "Emotion_Summary.jsonl"
    if val_file is None:
        val_file = Path(__file__).parent.parent / "data" / "validation" / "Emotion_Summary.jsonl"
    if output_dir is None:
        output_dir = Path(__file__).parent.parent / "model" / "qwen2_emotion_summary"
    
    print(f"è®­ç»ƒæ•°æ®: {train_file}")
    print(f"éªŒè¯æ•°æ®: {val_file}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print()
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ?
    if not train_file.exists():
        print(f"é”™è¯¯: è®­ç»ƒæ–‡ä»¶ä¸å­˜åœ? {train_file}")
        return
    
    # åˆå§‹åŒ–æ¨¡å?
    print("æ­¥éª¤ 1/4: åˆå§‹åŒ–æ¨¡å?)
    print("-" * 70)
    model_wrapper = Qwen2EmotionSummaryModel(
        model_name=model_name,
        max_input_length=max_length,
        use_4bit=use_4bit,
        use_lora=True,
        lora_r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
    )
    
    # å‡†å¤‡æ•°æ®é›?
    print("\næ­¥éª¤ 2/4: å‡†å¤‡æ•°æ®é›?)
    print("-" * 70)
    train_dataset = Qwen2EmotionSummaryDataset(
        data_file=train_file,
        tokenizer=model_wrapper.tokenizer,
        max_length=max_length,
    )
    
    val_dataset = None
    if val_file.exists():
        val_dataset = Qwen2EmotionSummaryDataset(
            data_file=val_file,
            tokenizer=model_wrapper.tokenizer,
            max_length=max_length,
        )
    
    # é…ç½®è®­ç»ƒå‚æ•°
    print("\næ­¥éª¤ 3/4: é…ç½®è®­ç»ƒå‚æ•°")
    print("-" * 70)
    training_args = TrainingArguments(
        output_dir=str(output_dir),
        num_train_epochs=num_epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        learning_rate=learning_rate,
        fp16=not use_4bit,  # 4-bité‡åŒ–æ—¶ä¸ä½¿ç”¨fp16
        bf16=False,
        logging_steps=10,
        save_steps=100,
        eval_steps=100 if val_dataset else None,
        evaluation_strategy="steps" if val_dataset else "no",
        save_total_limit=3,
        load_best_model_at_end=True if val_dataset else False,
        warmup_steps=100,
        weight_decay=0.01,
        max_grad_norm=1.0,
        optim="paged_adamw_8bit" if use_4bit else "adamw_torch",
        lr_scheduler_type="cosine",
        report_to="none",  # ä¸ä½¿ç”¨wandbç­?
        remove_unused_columns=False,
        gradient_checkpointing=True,  # èŠ‚çœæ˜¾å­˜
    )
    
    print(f"  è®­ç»ƒè½®æ•°: {num_epochs}")
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
    print(f"  æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {batch_size * gradient_accumulation_steps}")
    print(f"  å­¦ä¹ ç? {learning_rate}")
    print(f"  æœ€å¤§åºåˆ—é•¿åº? {max_length}")
    print(f"  4-bité‡åŒ–: {use_4bit}")
    print(f"  æ¢¯åº¦æ£€æŸ¥ç‚¹: True (èŠ‚çœæ˜¾å­˜)")
    
    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=model_wrapper.tokenizer,
        mlm=False,  # å› æœè¯­è¨€æ¨¡å‹ä¸ä½¿ç”¨MLM
    )
    
    # åˆ›å»º Trainer
    print("\næ­¥éª¤ 4/4: å¼€å§‹è®­ç»?)
    print("-" * 70)
    trainer = Trainer(
        model=model_wrapper.model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
    )
    
    # å¼€å§‹è®­ç»?
    print("\nğŸš€ è®­ç»ƒå¼€å§?..\n")
    trainer.train()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å?
    print("\nä¿å­˜æ¨¡å‹...")
    final_model_path = output_dir / "final"
    model_wrapper.model.save_pretrained(final_model_path)
    model_wrapper.tokenizer.save_pretrained(final_model_path)
    
    print("\n" + "=" * 70)
    print("âœ?è®­ç»ƒå®Œæˆï¼?)
    print("=" * 70)
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")
    print()
    
    return model_wrapper, trainer


def main():
    """ä¸»å‡½æ•?""
    import argparse
    
    parser = argparse.ArgumentParser(description="è®­ç»ƒ Qwen2-7B-Instruct ç”¨äºæƒ…ç»ªæ€»ç»“ä»»åŠ¡")
    parser.add_argument("--model_name", type=str, default="Qwen/Qwen2-7B-Instruct", help="æ¨¡å‹åç§°")
    parser.add_argument("--train_file", type=str, default=None, help="è®­ç»ƒæ•°æ®æ–‡ä»¶")
    parser.add_argument("--val_file", type=str, default=None, help="éªŒè¯æ•°æ®æ–‡ä»¶")
    parser.add_argument("--output_dir", type=str, default=None, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--num_epochs", type=int, default=3, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=2, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=8, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--learning_rate", type=float, default=2e-4, help="å­¦ä¹ ç?)
    parser.add_argument("--max_length", type=int, default=8192, help="æœ€å¤§åºåˆ—é•¿åº?)
    parser.add_argument("--use_4bit", action="store_true", default=True, help="ä½¿ç”¨4-bité‡åŒ–")
    parser.add_argument("--lora_r", type=int, default=64, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=16, help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.05, help="LoRA dropout")
    
    args = parser.parse_args()
    
    # è½¬æ¢è·¯å¾„
    train_file = Path(args.train_file) if args.train_file else None
    val_file = Path(args.val_file) if args.val_file else None
    output_dir = Path(args.output_dir) if args.output_dir else None
    
    # å¼€å§‹è®­ç»?
    train_qwen2_model(
        model_name=args.model_name,
        train_file=train_file,
        val_file=val_file,
        output_dir=output_dir,
        num_epochs=args.num_epochs,
        batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        max_length=args.max_length,
        use_4bit=args.use_4bit,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
    )


if __name__ == "__main__":
    main()

