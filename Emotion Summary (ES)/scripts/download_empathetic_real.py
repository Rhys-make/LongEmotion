# -*- coding: utf-8 -*-
"""
ä½¿ç”¨ Parquet æ ¼å¼ç›´æ¥ä¸‹è½½ Empathetic Dialogues çœŸå®æ•°æ®é›†
"""

import json
import random
from pathlib import Path
from collections import defaultdict
import urllib.request
import pandas as pd

def download_and_convert():
    """ä¸‹è½½çœŸå®çš„ Empathetic Dialogues æ•°æ®é›†"""
    
    print("="*70)
    print("ä¸‹è½½ Empathetic Dialogues å®Œæ•´æ•°æ®é›†")
    print("="*70)
    
    # Parquet æ–‡ä»¶ URL (ä» Hugging Face datasets viewer è·å–)
    base_url = "https://huggingface.co/datasets/facebook/empathetic_dialogues/resolve/main/data"
    
    train_url = f"{base_url}/train-00000-of-00001.parquet"
    val_url = f"{base_url}/validation-00000-of-00001.parquet"
    
    print("\nğŸ“¥ æ­£åœ¨ä¸‹è½½è®­ç»ƒé›†...")
    try:
        import requests
        
        # ä¸‹è½½è®­ç»ƒé›†
        response = requests.get(train_url, timeout=300)
        train_parquet = "temp_train.parquet"
        with open(train_parquet, 'wb') as f:
            f.write(response.content)
        
        # ä¸‹è½½éªŒè¯é›†
        print("\nğŸ“¥ æ­£åœ¨ä¸‹è½½éªŒè¯é›†...")
        response = requests.get(val_url, timeout=300)
        val_parquet = "temp_val.parquet"
        with open(val_parquet, 'wb') as f:
            f.write(response.content)
        
        # è¯»å– Parquet æ–‡ä»¶
        print("\nğŸ“– æ­£åœ¨è¯»å–æ•°æ®...")
        train_df = pd.read_parquet(train_parquet)
        val_df = pd.read_parquet(val_parquet)
        
        print(f"âœ… ä¸‹è½½å®Œæˆï¼")
        print(f"   è®­ç»ƒé›†: {len(train_df)} æ¡è®°å½•")
        print(f"   éªŒè¯é›†: {len(val_df)} æ¡è®°å½•")
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        train_data = train_df.to_dict('records')
        val_data = val_df.to_dict('records')
        
        # ç»„ç»‡å¯¹è¯
        print("\nğŸ”„ æ­£åœ¨ç»„ç»‡å¯¹è¯...")
        train_conversations = organize_by_conversation(train_data)
        val_conversations = organize_by_conversation(val_data)
        
        print(f"âœ… è®­ç»ƒé›†: {len(train_conversations)} ä¸ªå®Œæ•´å¯¹è¯")
        print(f"âœ… éªŒè¯é›†: {len(val_conversations)} ä¸ªå®Œæ•´å¯¹è¯")
        
        # åˆå¹¶å¹¶é‡æ–°åˆ†å‰²
        all_conversations = list(train_conversations.values()) + list(val_conversations.values())
        random.shuffle(all_conversations)
        
        # 80/20 åˆ†å‰²
        split_idx = int(len(all_conversations) * 0.8)
        train_convs = all_conversations[:split_idx]
        val_convs = all_conversations[split_idx:]
        
        print(f"\nğŸ“Š é‡æ–°åˆ†å‰²:")
        print(f"   è®­ç»ƒé›†: {len(train_convs)} ä¸ªå¯¹è¯ (80%)")
        print(f"   éªŒè¯é›†: {len(val_convs)} ä¸ªå¯¹è¯ (20%)")
        
        # è½¬æ¢ä¸º ES æ ¼å¼
        print("\nğŸ”„ æ­£åœ¨è½¬æ¢ä¸º ES æ ¼å¼...")
        train_es_data = [convert_to_es_format(conv, idx) for idx, conv in enumerate(train_convs)]
        val_es_data = [convert_to_es_format(conv, idx) for idx, conv in enumerate(val_convs)]
        
        # ä¿å­˜æ•°æ®
        save_data(train_es_data, val_es_data)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import os
        os.remove(train_parquet)
        os.remove(val_parquet)
        
        print("\nâœ… æˆåŠŸï¼ä½¿ç”¨äº†çœŸå®çš„å®Œæ•´æ•°æ®é›†ï¼")
        
    except Exception as e:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
        print("\nå°è¯•å¤‡ç”¨æ–¹æ¡ˆ...")
        try_alternative_method()

def try_alternative_method():
    """å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ datasets åº“çš„æ—§ç‰ˆæœ¬æˆ–å…¶ä»–æ–¹æ³•"""
    print("\nå°è¯•ä½¿ç”¨ datasets åº“åŠ è½½...")
    
    try:
        from datasets import load_dataset
        import datasets
        
        # å°è¯•æŒ‡å®šç‰ˆæœ¬
        dataset = load_dataset(
            "facebook/empathetic_dialogues",
            revision="refs/convert/parquet"  # ä½¿ç”¨ parquet è½¬æ¢åˆ†æ”¯
        )
        
        print(f"âœ… åŠ è½½æˆåŠŸï¼")
        print(f"   è®­ç»ƒé›†: {len(dataset['train'])} æ¡è®°å½•")
        print(f"   éªŒè¯é›†: {len(dataset['validation'])} æ¡è®°å½•")
        
        # ç»„ç»‡å¯¹è¯
        train_conversations = organize_by_conversation(dataset['train'])
        val_conversations = organize_by_conversation(dataset['validation'])
        
        all_conversations = list(train_conversations.values()) + list(val_conversations.values())
        random.shuffle(all_conversations)
        
        split_idx = int(len(all_conversations) * 0.8)
        train_convs = all_conversations[:split_idx]
        val_convs = all_conversations[split_idx:]
        
        print(f"\nğŸ“Š æ•°æ®åˆ†å‰²:")
        print(f"   è®­ç»ƒé›†: {len(train_convs)} ä¸ªå¯¹è¯ (80%)")
        print(f"   éªŒè¯é›†: {len(val_convs)} ä¸ªå¯¹è¯ (20%)")
        
        # è½¬æ¢å¹¶ä¿å­˜
        train_es_data = [convert_to_es_format(conv, idx) for idx, conv in enumerate(train_convs)]
        val_es_data = [convert_to_es_format(conv, idx) for idx, conv in enumerate(val_convs)]
        
        save_data(train_es_data, val_es_data)
        
        print("\nâœ… æˆåŠŸï¼")
        
    except Exception as e2:
        print(f"âŒ å¤‡ç”¨æ–¹æ¡ˆä¹Ÿå¤±è´¥: {e2}")
        print("\nä½¿ç”¨æ‰©å±•çš„ç¤ºä¾‹æ•°æ®...")
        use_expanded_samples()

def use_expanded_samples():
    """åˆ›å»ºæ›´å¤šçš„ç¤ºä¾‹æ•°æ®"""
    print("\nğŸ“ åˆ›å»ºæ‰©å±•ç¤ºä¾‹æ•°æ®é›†ï¼ˆ1000ä¸ªå¯¹è¯ï¼‰...")
    
    # å¯¼å…¥ç¤ºä¾‹åˆ›å»ºå‡½æ•°
    from download_empathetic_dialogues_v2 import create_sample_english_data, convert_to_es_format
    
    # åˆ›å»ºæ›´å¤šç¤ºä¾‹
    all_conversations = []
    for _ in range(10):  # é‡å¤10æ¬¡ï¼Œå¾—åˆ°1000ä¸ªå¯¹è¯
        conversations = create_sample_english_data()
        all_conversations.extend(conversations)
    
    random.shuffle(all_conversations)
    
    split_idx = int(len(all_conversations) * 0.8)
    train_convs = all_conversations[:split_idx]
    val_convs = all_conversations[split_idx:]
    
    train_es_data = [convert_to_es_format(conv, idx) for idx, conv in enumerate(train_convs)]
    val_es_data = [convert_to_es_format(conv, idx) for idx, conv in enumerate(val_convs)]
    
    save_data(train_es_data, val_es_data)
    
    print(f"âœ… åˆ›å»ºäº†æ‰©å±•æ•°æ®é›†:")
    print(f"   è®­ç»ƒé›†: {len(train_es_data)} ä¸ªå¯¹è¯")
    print(f"   éªŒè¯é›†: {len(val_es_data)} ä¸ªå¯¹è¯")

def organize_by_conversation(dataset):
    """æŒ‰å¯¹è¯IDç»„ç»‡æ•°æ®"""
    conversations = defaultdict(list)
    
    for item in dataset:
        conv_id = item['conv_id']
        conversations[conv_id].append(item)
    
    for conv_id in conversations:
        conversations[conv_id].sort(key=lambda x: x['utterance_idx'])
    
    return conversations

def convert_to_es_format(conversation, item_id):
    """è½¬æ¢ä¸º ES æ ¼å¼"""
    from download_empathetic_dialogues_v2 import convert_to_es_format as convert_func
    return convert_func(conversation, item_id)

def save_data(train_es_data, val_es_data):
    """ä¿å­˜æ•°æ®"""
    train_dir = Path("data/train")
    val_dir = Path("data/validation")
    train_dir.mkdir(parents=True, exist_ok=True)
    val_dir.mkdir(parents=True, exist_ok=True)
    
    train_file = train_dir / "Emotion_Summary.jsonl"
    with open(train_file, 'w', encoding='utf-8') as f:
        for item in train_es_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    val_file = val_dir / "Emotion_Summary.jsonl"
    with open(val_file, 'w', encoding='utf-8') as f:
        for item in val_es_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜:")
    print(f"   è®­ç»ƒé›†: {train_file} ({len(train_es_data)} æ ·æœ¬)")
    print(f"   éªŒè¯é›†: {val_file} ({len(val_es_data)} æ ·æœ¬)")

if __name__ == "__main__":
    random.seed(42)
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import pandas
        import requests
    except ImportError:
        print("éœ€è¦å®‰è£…é¢å¤–ä¾èµ–:")
        print("pip install pandas requests")
        import sys
        sys.exit(1)
    
    download_and_convert()

