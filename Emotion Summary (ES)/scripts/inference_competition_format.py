# -*- coding: utf-8 -*-
"""æ¯”èµ›æ ¼å¼æ¨ç†è„šæœ¬ - åˆ†åˆ«ç”Ÿæˆ5ä¸ªå­—æ®µ"""

import json
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from tqdm import tqdm

def create_field_prompt(case_desc, consult_process, field_name):
    """ä¸ºæ¯ä¸ªå­—æ®µåˆ›å»ºä¸“é—¨çš„prompt"""
    
    # å°†åˆ—è¡¨è½¬ä¸ºæ–‡æœ¬
    case_text = " ".join(case_desc) if isinstance(case_desc, list) else case_desc
    consult_text = " ".join(consult_process) if isinstance(consult_process, list) else consult_process
    
    # æˆªæ–­
    case_text = case_text[:400]
    consult_text = consult_text[:800]
    
    # æ ¹æ®å­—æ®µç±»å‹åˆ›å»ºä¸åŒçš„prompt
    prompts = {
        "cause": f"Based on this psychological case, summarize the CAUSE of the problem:\n\nCase: {case_text}\n\nConsultation: {consult_text}\n\nCause:",
        
        "symptoms": f"Based on this psychological case, summarize the SYMPTOMS:\n\nCase: {case_text}\n\nConsultation: {consult_text}\n\nSymptoms:",
        
        "treatment": f"Based on this psychological case, summarize the TREATMENT PROCESS:\n\nCase: {case_text}\n\nConsultation: {consult_text}\n\nTreatment:",
        
        "characteristics": f"Based on this psychological case, summarize the ILLNESS CHARACTERISTICS:\n\nCase: {case_text}\n\nConsultation: {consult_text}\n\nCharacteristics:",
        
        "effect": f"Based on this psychological case, summarize the TREATMENT EFFECT:\n\nCase: {case_text}\n\nConsultation: {consult_text}\n\nEffect:"
    }
    
    return prompts.get(field_name, "")

def generate_single_field(model, tokenizer, prompt, max_length=128):
    """ç”Ÿæˆå•ä¸ªå­—æ®µ"""
    
    # Tokenize
    inputs = tokenizer(
        prompt,
        max_length=512,
        truncation=True,
        padding=False,
        return_tensors="pt"
    )
    
    # ç”Ÿæˆ
    outputs = model.generate(
        **inputs,
        max_length=max_length,
        num_beams=4,
        early_stopping=True,
        no_repeat_ngram_size=2,
        temperature=0.7,
    )
    
    # è§£ç 
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    return generated_text.strip()

def main():
    print("="*80)
    print("ğŸ† ç”Ÿæˆæ¯”èµ›è¦æ±‚æ ¼å¼çš„æ¨ç†ç»“æœ")
    print("="*80)
    
    # åŠ è½½æ¨¡å‹
    model_path = "model/mt5_fast/final"
    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_file = Path("data/test/Emotion_Summary.jsonl")
    print(f"\nğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
    
    test_data = []
    with open(test_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                test_data.append(json.loads(line))
    
    print(f"âœ… æµ‹è¯•æ ·æœ¬æ•°: {len(test_data)}")
    print(f"\nâš ï¸  æ³¨æ„: æ¯ä¸ªæ ·æœ¬éœ€è¦ç”Ÿæˆ5ä¸ªå­—æ®µï¼Œæ€»å…± {len(test_data) * 5} æ¬¡æ¨¡å‹è°ƒç”¨")
    print(f"   é¢„è®¡æ—¶é—´: {len(test_data) * 5 * 3 // 60} åˆ†é’Ÿ\n")
    
    # è¿›è¡Œæ¨ç†
    print(f"ğŸ”® å¼€å§‹æ¨ç†...\n")
    results = []
    
    for item in tqdm(test_data, desc="æ ·æœ¬è¿›åº¦"):
        # è·å–ID
        item_id = item.get("id", 0)
        
        # è·å–è¾“å…¥
        case_desc = item.get("case_description", [])
        consult_process = item.get("consultation_process", [])
        
        # ç”Ÿæˆ5ä¸ªå­—æ®µ
        result = {"id": item_id}
        
        # 1. ç—…å› 
        prompt = create_field_prompt(case_desc, consult_process, "cause")
        result["predicted_cause"] = generate_single_field(model, tokenizer, prompt)
        
        # 2. ç—‡çŠ¶
        prompt = create_field_prompt(case_desc, consult_process, "symptoms")
        result["predicted_symptoms"] = generate_single_field(model, tokenizer, prompt)
        
        # 3. æ²»ç–—è¿‡ç¨‹
        prompt = create_field_prompt(case_desc, consult_process, "treatment")
        result["predicted_treatment_process"] = generate_single_field(model, tokenizer, prompt)
        
        # 4. ç–¾ç—…ç‰¹å¾
        prompt = create_field_prompt(case_desc, consult_process, "characteristics")
        result["predicted_illness_Characteristics"] = generate_single_field(model, tokenizer, prompt)
        
        # 5. æ²»ç–—æ•ˆæœ
        prompt = create_field_prompt(case_desc, consult_process, "effect")
        result["predicted_treatment_effect"] = generate_single_field(model, tokenizer, prompt)
        
        results.append(result)
    
    # ä¿å­˜ç»“æœ
    output_file = Path("results/Emotion_Summary_Result.jsonl")
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_file}")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    
    print(f"âœ… å®Œæˆï¼å…±ç”Ÿæˆ {len(results)} æ¡ç»“æœ")
    
    # æ˜¾ç¤ºç¤ºä¾‹
    print(f"\nğŸ“ ç¤ºä¾‹è¾“å‡º (å‰2æ¡):")
    print("-"*80)
    for i, result in enumerate(results[:2], 1):
        print(f"\nã€æ ·æœ¬ {i}ã€‘ ID: {result['id']}")
        print(f"  ç—…å› : {result['predicted_cause'][:100]}...")
        print(f"  ç—‡çŠ¶: {result['predicted_symptoms'][:100]}...")
        print(f"  æ²»ç–—è¿‡ç¨‹: {result['predicted_treatment_process'][:100]}...")
        print(f"  ç–¾ç—…ç‰¹å¾: {result['predicted_illness_Characteristics'][:100]}...")
        print(f"  æ²»ç–—æ•ˆæœ: {result['predicted_treatment_effect'][:100]}...")
    
    # éªŒè¯æ ¼å¼
    print(f"\nğŸ” æ ¼å¼éªŒè¯:")
    required_fields = ["id", "predicted_cause", "predicted_symptoms", 
                      "predicted_treatment_process", "predicted_illness_Characteristics", 
                      "predicted_treatment_effect"]
    
    all_valid = True
    for i, r in enumerate(results):
        missing = [f for f in required_fields if f not in r]
        empty = [f for f in required_fields if f in r and not r[f]]
        
        if missing:
            print(f"  âŒ æ ·æœ¬ {r['id']}: ç¼ºå°‘å­—æ®µ {missing}")
            all_valid = False
        elif empty:
            print(f"  âš ï¸  æ ·æœ¬ {r['id']}: ç©ºå­—æ®µ {empty}")
    
    if all_valid:
        print(f"  âœ… æ‰€æœ‰æ ·æœ¬æ ¼å¼æ­£ç¡®")
    
    # ç»Ÿè®¡ç”Ÿæˆé•¿åº¦
    print(f"\nğŸ“Š ç”Ÿæˆå†…å®¹ç»Ÿè®¡:")
    for field in required_fields[1:]:  # è·³è¿‡id
        lengths = [len(r[field]) for r in results]
        avg_len = sum(lengths) / len(lengths) if lengths else 0
        print(f"  {field}: å¹³å‡ {avg_len:.0f} å­—ç¬¦ (èŒƒå›´: {min(lengths)}-{max(lengths)})")
    
    print("\n" + "="*80)
    print("âœ… æ¨ç†å®Œæˆï¼")
    print(f"ğŸ“ æäº¤æ–‡ä»¶: {output_file}")
    print(f"ğŸ“‹ æ ¼å¼: ç¬¦åˆæ¯”èµ›è¦æ±‚ âœ“")
    print("="*80)

if __name__ == "__main__":
    main()

