# -*- coding: utf-8 -*-
"""ç®€å•è®­ç»ƒè„šæœ¬ - ä½¿ç”¨ Trainer API"""

import json
from pathlib import Path
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq,
)
from datasets import Dataset

# åŠ è½½æ•°æ®
def load_data(file_path):
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data

# æ•°æ®é¢„å¤„ç†
def preprocess_function(examples, tokenizer):
    # åˆå¹¶è¾“å…¥
    inputs = []
    targets = []
    
    for ex in examples:
        # è¾“å…¥ï¼šæ¡ˆä¾‹æè¿° + å’¨è¯¢è¿‡ç¨‹
        input_text = " ".join(ex.get("case_description", []) + ex.get("consultation_process", []))
        inputs.append(input_text)
        
        # è¾“å‡ºï¼šç»éªŒä¸åæ€
        targets.append(ex.get("experience_and_reflection", ""))
    
    # Tokenizeï¼ˆæ ¹æ®è®­ç»ƒé›†å®é™…é•¿åº¦ä¼˜åŒ–ï¼‰
    # è®­ç»ƒé›†å¹³å‡: è¾“å…¥~400å­—ç¬¦(~150 tokens), è¾“å‡º~1500å­—ç¬¦(~400 tokens)
    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=False)
    labels = tokenizer(targets, max_length=256, truncation=True, padding=False)
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# ä¸»å‡½æ•°
def main():
    print("="*60)
    print("å¼€å§‹è®­ç»ƒ mT5 æ¨¡å‹")
    print("="*60)
    
    # æ¨¡å‹å’Œå‚æ•°
    model_name = "google/mt5-base"
    
    print(f"\nåŠ è½½æ¨¡å‹: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    
    # åŠ è½½æ•°æ®
    print("\nåŠ è½½æ•°æ®...")
    train_file = Path("data/train/Emotion_Summary.jsonl")
    val_file = Path("data/validation/Emotion_Summary.jsonl")
    
    train_data = load_data(train_file)
    val_data = load_data(val_file)
    
    print(f"è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
    print(f"éªŒè¯é›†: {len(val_data)} æ ·æœ¬")
    
    # è½¬æ¢ä¸º Dataset
    print("\né¢„å¤„ç†æ•°æ®...")
    
    # å¤„ç†è®­ç»ƒé›†
    train_processed = []
    for item in train_data:
        result = preprocess_function([item], tokenizer)
        # å±•å¼€å­—å…¸
        train_processed.append({
            'input_ids': result['input_ids'][0],
            'attention_mask': result['attention_mask'][0],
            'labels': result['labels'][0]
        })
    
    # å¤„ç†éªŒè¯é›†
    val_processed = []
    for item in val_data:
        result = preprocess_function([item], tokenizer)
        val_processed.append({
            'input_ids': result['input_ids'][0],
            'attention_mask': result['attention_mask'][0],
            'labels': result['labels'][0]
        })
    
    train_dataset = Dataset.from_list(train_processed)
    val_dataset = Dataset.from_list(val_processed)
    
    # è®­ç»ƒå‚æ•°ï¼ˆä¼˜åŒ–é€Ÿåº¦ï¼‰
    training_args = Seq2SeqTrainingArguments(
        output_dir="model/mt5_emotion_summary",
        num_train_epochs=3,  # å‡å°‘åˆ°3è½®ï¼ˆæ•°æ®å‡å°‘20%åè¶³å¤Ÿï¼‰
        per_device_train_batch_size=2,  # å¢åŠ åˆ°2ï¼ˆåŠ å¿«è®­ç»ƒï¼‰
        per_device_eval_batch_size=2,  # å¢åŠ åˆ°2
        gradient_accumulation_steps=4,  # å‡å°‘åˆ°4æ­¥ï¼Œç­‰æ•ˆbatch_size=8
        learning_rate=5e-5,
        weight_decay=0.01,
        logging_steps=50,  # å‡å°‘æ—¥å¿—é¢‘ç‡
        save_steps=1000,  # å‡å°‘ä¿å­˜é¢‘ç‡ä»¥åŠ å¿«é€Ÿåº¦
        eval_steps=1000,
        eval_strategy="steps",
        save_total_limit=1,  # åªä¿ç•™1ä¸ªcheckpointèŠ‚çœç©ºé—´
        predict_with_generate=True,
        fp16=False,
        push_to_hub=False,
        report_to="none",
        dataloader_num_workers=0,
        dataloader_pin_memory=False,
        warmup_steps=200,
        max_grad_norm=1.0,  # æ·»åŠ æ¢¯åº¦è£å‰ª
    )
    
    # åˆ›å»º Trainer
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("\nå¼€å§‹è®­ç»ƒ...")
    print("é¢„è®¡æ—¶é—´: 2-3 å°æ—¶ï¼ˆå·²ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼‰")
    print("-"*60)
    
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    print("\nä¿å­˜æ¨¡å‹...")
    trainer.save_model("model/mt5_emotion_summary/final")
    tokenizer.save_pretrained("model/mt5_emotion_summary/final")
    
    print("\n" + "="*60)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: model/mt5_emotion_summary/final")
    print("="*60)

if __name__ == "__main__":
    main()