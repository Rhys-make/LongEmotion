# -*- coding: utf-8 -*-
"""è¯­ä¹‰ç†è§£ç‰ˆæå– - V4æœ€ç»ˆç‰ˆ"""

import json
import re
from pathlib import Path
from tqdm import tqdm
from collections import defaultdict

def clean_sentence(text):
    """æ¸…ç†å¥å­ï¼Œç¡®ä¿å®Œæ•´æ€§"""
    text = text.strip()
    # å¦‚æœå¥å­è¢«æˆªæ–­ï¼Œå°è¯•è¡¥å…¨
    if text and not text[-1] in '.!?ã€‚ï¼ï¼Ÿ':
        # æŸ¥æ‰¾æœ€åä¸€ä¸ªå®Œæ•´å¥å­
        last_period = max(text.rfind('.'), text.rfind('!'), text.rfind('?'))
        if last_period > len(text) * 0.5:  # å¦‚æœæœ‰è¶…è¿‡ä¸€åŠçš„å®Œæ•´å†…å®¹
            text = text[:last_period+1]
    return text

def deduplicate_sentences(sentences):
    """å»é‡ï¼šç§»é™¤é«˜åº¦ç›¸ä¼¼çš„å¥å­"""
    unique = []
    for sent in sentences:
        # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰å¥å­é‡å¤
        is_duplicate = False
        for existing in unique:
            # å¦‚æœä¸¤ä¸ªå¥å­æœ‰80%ä»¥ä¸Šç›¸åŒï¼Œè®¤ä¸ºæ˜¯é‡å¤
            overlap = sum(1 for word in sent.split() if word in existing.split())
            if overlap / max(len(sent.split()), 1) > 0.8:
                is_duplicate = True
                break
        if not is_duplicate:
            unique.append(sent)
    return unique

def extract_cause_semantic(case_desc, consult_process):
    """
    ç—…å› æå– - å®Œæ•´é€»è¾‘é“¾ç‰ˆæœ¬
    ç›®æ ‡ï¼šå…³é”®äº‹ä»¶ â†’ æƒ…æ„Ÿå†²çª â†’ å…³ç³»é˜»ç¢ â†’ å¿ƒç†è½¬åŒ–
    """
    full_text = " ".join(case_desc + consult_process)
    sentences = re.split(r'(?<=[.!?])\s+', full_text)
    
    cause_info = {
        'events': [],      # å…³é”®äº‹ä»¶
        'emotions': [],    # æƒ…æ„Ÿå†²çª
        'relationships': [], # å…³ç³»é—®é¢˜
        'transformation': [] # å¿ƒç†è½¬åŒ–
    }
    
    # 1. å…³é”®äº‹ä»¶ï¼ˆå®¶åº­èƒŒæ™¯ã€é‡å¤§äº‹ä»¶ï¼‰
    event_keywords = ['adopted', 'adoption', 'discovered', 'found out', 'after marriage', 
                     'biological', 'father passed', 'mother left', 'divorce', 'death']
    for sent in sentences:
        if any(kw in sent.lower() for kw in event_keywords):
            if len(sent) > 40:
                cause_info['events'].append(sent.strip())
    
    # 2. æƒ…æ„Ÿå†²çª
    emotion_keywords = ['guilt', 'shame', 'anger', 'resentment', 'abandoned', 'rejected',
                       'confused', 'conflicted', 'irritated', 'frustrated']
    for sent in sentences:
        if any(kw in sent.lower() for kw in emotion_keywords):
            if len(sent) > 40:
                cause_info['emotions'].append(sent.strip())
    
    # 3. å…³ç³»é˜»ç¢
    relation_keywords = ['refused', 'opposed', 'unwilling', 'conflict with', 'disagreement',
                        'wife', 'husband', 'parents', 'family', 'divorce']
    for sent in sentences:
        if any(kw in sent.lower() for kw in relation_keywords):
            # åªä¿ç•™æ¶‰åŠå†²çª/é˜»ç¢çš„
            if any(neg in sent.lower() for neg in ['refused', 'opposed', 'unwilling', 'conflict', 'disagree']):
                if len(sent) > 40:
                    cause_info['relationships'].append(sent.strip())
    
    # 4. å¿ƒç†è½¬åŒ–ï¼ˆå¦‚ä½•å¯¼è‡´å½“å‰é—®é¢˜ï¼‰
    transform_keywords = ['led to', 'resulted in', 'caused', 'triggered', 'manifested as',
                         'developed', 'began to', 'started to']
    for sent in sentences:
        if any(kw in sent.lower() for kw in transform_keywords):
            if len(sent) > 40:
                cause_info['transformation'].append(sent.strip())
    
    # å»é‡å¹¶ç»„ç»‡
    result_parts = []
    
    # ä¼˜å…ˆçº§ï¼šäº‹ä»¶ â†’ æƒ…æ„Ÿ â†’ å…³ç³» â†’ è½¬åŒ–
    for key in ['events', 'emotions', 'relationships', 'transformation']:
        items = deduplicate_sentences(cause_info[key][:2])  # æ¯ç±»æœ€å¤š2å¥
        result_parts.extend(items)
    
    if result_parts:
        result = " ".join(result_parts)
        return clean_sentence(result[:600])
    
    # å¤‡é€‰
    return clean_sentence(" ".join(case_desc)[:400])

def extract_symptoms_semantic(case_desc, consult_process):
    """
    ç—‡çŠ¶æå– - å®Œæ•´åˆ†ç±»ç‰ˆæœ¬
    ç›®æ ‡ï¼šèº¯ä½“ç—‡çŠ¶ + å¿ƒç†ç—‡çŠ¶ + è¡Œä¸ºç—‡çŠ¶
    """
    full_text = " ".join(case_desc + consult_process)
    sentences = re.split(r'(?<=[.!?])\s+', full_text)
    
    symptom_categories = {
        'physical': [],    # èº¯ä½“ç—‡çŠ¶
        'psychological': [], # å¿ƒç†ç—‡çŠ¶
        'behavioral': []   # è¡Œä¸ºç—‡çŠ¶
    }
    
    # 1. èº¯ä½“ç—‡çŠ¶
    physical_keywords = ['pain', 'discomfort', 'dizzy', 'nausea', 'throat', 'chest', 
                        'stomach', 'headache', 'insomnia', 'sleep', 'fatigue',
                        'nasopharynx', 'bowel', 'hemorrhoid']
    
    # 2. å¿ƒç†ç—‡çŠ¶
    psychological_keywords = ['anxiety', 'anxious', 'worry', 'worried', 'fear', 'afraid',
                             'panic', 'depress', 'sad', 'hopeless', 'obsess', 'compulsive']
    
    # 3. è¡Œä¸ºç—‡çŠ¶
    behavioral_keywords = ['check', 'repeatedly', 'constantly', 'avoid', 'unable to',
                          'difficulty', 'search', 'looked up', 'diagnosed', 'self-diagnose']
    
    # æ’é™¤è¯ï¼ˆæ”¹å–„ã€æ¢å¤ç›¸å…³ï¼‰
    exclude_keywords = ['improved', 'better', 'recovered', 'relief', 'no longer',
                       'stopped', 'able to', 'began to improve']
    
    for sent in sentences:
        sent_lower = sent.lower()
        
        # å¦‚æœåŒ…å«æ’é™¤è¯ï¼Œè·³è¿‡
        if any(ex in sent_lower for ex in exclude_keywords):
            continue
        
        if len(sent) < 40:
            continue
        
        # åˆ†ç±»
        if any(kw in sent_lower for kw in physical_keywords):
            symptom_categories['physical'].append(sent.strip())
        if any(kw in sent_lower for kw in psychological_keywords):
            symptom_categories['psychological'].append(sent.strip())
        if any(kw in sent_lower for kw in behavioral_keywords):
            symptom_categories['behavioral'].append(sent.strip())
    
    # å»é‡å¹¶ç»„ç»‡
    result_parts = []
    
    # æ¯ç±»é€‰æœ€ç›¸å…³çš„2å¥
    for category in ['physical', 'psychological', 'behavioral']:
        items = deduplicate_sentences(symptom_categories[category][:2])
        result_parts.extend(items)
    
    if result_parts:
        result = " ".join(result_parts)
        return clean_sentence(result[:700])
    
    # å¤‡é€‰
    return clean_sentence(" ".join(case_desc[:2])[:500])

def extract_treatment_semantic(case_desc, consult_process):
    """
    æ²»ç–—è¿‡ç¨‹æå– - ä¸“ä¸šæ²»ç–—ä¸ºä¸»
    ç›®æ ‡ï¼šä¸“ä¸šå¹²é¢„æ­¥éª¤ï¼ˆæ’é™¤è‡ªè¡Œå°è¯•ã€é€šç”¨ç†è®ºï¼‰
    """
    full_text = " ".join(consult_process)
    sentences = re.split(r'(?<=[.!?])\s+', full_text)
    
    treatment_info = {
        'professional': [],  # ä¸“ä¸šæ²»ç–—
        'techniques': [],    # å…·ä½“æŠ€æœ¯
        'process': []        # æ²»ç–—æ­¥éª¤
    }
    
    # ä¸“ä¸šæ²»ç–—å…³é”®è¯
    professional_keywords = ['therapist', 'counselor', 'psychologist', 'conducted',
                           'guided', 'facilitated', 'explored', 'addressed', 'helped',
                           'session', 'consultation']
    
    # å…·ä½“æŠ€æœ¯
    technique_keywords = ['hypnosis', 'hypnotherapy', 'imagery', 'dialogue', 'cognitive',
                         'behavioral', 'exposure', 'mindfulness', 'relaxation']
    
    # æ’é™¤è¯ï¼ˆè‡ªè¡Œå°è¯•ã€é€šç”¨ç†è®ºï¼‰
    exclude_keywords = ['tried', 'attempted', 'without effect', 'ineffective', 'failed',
                       'generally', 'typically', 'usually', 'medication can only']
    
    for sent in sentences:
        sent_lower = sent.lower()
        
        # æ’é™¤è‡ªè¡Œå°è¯•å’Œé€šç”¨ç†è®º
        if any(ex in sent_lower for ex in exclude_keywords):
            continue
        
        if len(sent) < 40:
            continue
        
        # ä¼˜å…ˆä¸“ä¸šæ²»ç–—
        if any(kw in sent_lower for kw in technique_keywords):
            treatment_info['techniques'].append(sent.strip())
        elif any(kw in sent_lower for kw in professional_keywords):
            treatment_info['professional'].append(sent.strip())
    
    # å»é‡å¹¶ç»„ç»‡
    result_parts = []
    
    # ä¼˜å…ˆæŠ€æœ¯æè¿°
    for key in ['techniques', 'professional']:
        items = deduplicate_sentences(treatment_info[key][:3])
        result_parts.extend(items)
    
    if result_parts:
        result = " ".join(result_parts)
        return clean_sentence(result[:700])
    
    # å¤‡é€‰
    return clean_sentence(" ".join(consult_process[1:3])[:600])

def extract_characteristics_semantic(case_desc, consult_process):
    """
    ç–¾ç—…ç‰¹å¾æå– - å½’çº³æ€»ç»“ç‰ˆæœ¬
    ç›®æ ‡ï¼šæœ¬è´¨å±æ€§ + ç—‡çŠ¶è§„å¾‹ï¼ˆéé‡å¤ç—‡çŠ¶ï¼‰
    """
    full_text = " ".join(case_desc + consult_process)
    sentences = re.split(r'(?<=[.!?])\s+', full_text)
    
    char_info = {
        'nature': [],     # æœ¬è´¨å±æ€§ï¼ˆå¿ƒç†æ€§ã€éå™¨è´¨æ€§ï¼‰
        'patterns': [],   # è¡¨ç°è§„å¾‹ï¼ˆæ³¢åŠ¨æ€§ã€äº¤æ›¿æ€§ï¼‰
        'mechanisms': []  # å¿ƒç†æœºåˆ¶ï¼ˆé˜²å¾¡ã€å¼ºè¿«ï¼‰
    }
    
    # æœ¬è´¨å±æ€§
    nature_keywords = ['psychological', 'mental', 'emotional', 'not physical',
                      'no organic', 'psychosomatic', 'disorder', 'condition']
    
    # è¡¨ç°è§„å¾‹
    pattern_keywords = ['fluctuate', 'vary', 'depend on', 'influenced by', 'when',
                       'attention', 'distract', 'focus', 'disappear', 'reappear']
    
    # å¿ƒç†æœºåˆ¶
    mechanism_keywords = ['defense', 'mechanism', 'cope', 'avoid', 'transfer',
                         'obsessive', 'compulsive', 'anxiety', 'hypochondria']
    
    # æ’é™¤è¯ï¼ˆå…·ä½“ç—‡çŠ¶æè¿°ï¼‰
    exclude_keywords = ['throat', 'stomach', 'pain', 'discomfort', 'specific symptom']
    
    for sent in sentences:
        sent_lower = sent.lower()
        
        if len(sent) < 40:
            continue
        
        # å½’çº³æ€§æè¿°ï¼Œéå…·ä½“ç—‡çŠ¶
        if any(kw in sent_lower for kw in nature_keywords):
            char_info['nature'].append(sent.strip())
        if any(kw in sent_lower for kw in pattern_keywords):
            char_info['patterns'].append(sent.strip())
        if any(kw in sent_lower for kw in mechanism_keywords):
            char_info['mechanisms'].append(sent.strip())
    
    # å»é‡å¹¶ç»„ç»‡
    result_parts = []
    
    for key in ['nature', 'patterns', 'mechanisms']:
        items = deduplicate_sentences(char_info[key][:2])
        result_parts.extend(items)
    
    if result_parts:
        result = " ".join(result_parts)
        return clean_sentence(result[:600])
    
    # å¤‡é€‰
    return clean_sentence(" ".join(case_desc)[:400])

def extract_effect_semantic(case_desc, consult_process):
    """
    æ²»ç–—æ•ˆæœæå– - å®é™…æ”¹å–„ç‰ˆæœ¬
    ç›®æ ‡ï¼šæƒ…ç»ª/è¡Œä¸º/è®¤çŸ¥æ”¹å–„ï¼ˆæ’é™¤å¤±è´¥å°è¯•ã€é€šç”¨ä¿¡æ¯ï¼‰
    """
    full_text = " ".join(consult_process)
    sentences = re.split(r'(?<=[.!?])\s+', full_text)
    
    effect_info = {
        'emotional': [],   # æƒ…ç»ªæ”¹å–„
        'behavioral': [],  # è¡Œä¸ºå˜åŒ–
        'cognitive': []    # è®¤çŸ¥æ”¹å˜
    }
    
    # æƒ…ç»ªæ”¹å–„
    emotional_keywords = ['felt better', 'relief', 'relieved', 'happy', 'smiled', 'smile',
                         'calm', 'peaceful', 'relaxed', 'burden lifted', 'changed person']
    
    # è¡Œä¸ºå˜åŒ–
    behavioral_keywords = ['no longer', 'stopped', 'able to', 'began to', 'started to',
                          'returned to', 'resumed', 'improved']
    
    # è®¤çŸ¥æ”¹å˜
    cognitive_keywords = ['realized', 'understood', 'recognized', 'insight', 'aware',
                         'acknowledged', 'accepted']
    
    # æ’é™¤è¯ï¼ˆå¤±è´¥å°è¯•ã€é€šç”¨ç†è®ºï¼‰
    exclude_keywords = ['tried but failed', 'without effect', 'ineffective', 'unable to',
                       'generally', 'typically', 'medication can only', 'hoped to']
    
    # ä¼˜å…ˆä»ååŠéƒ¨åˆ†æå–
    second_half_start = len(sentences) // 2
    
    for i, sent in enumerate(sentences):
        sent_lower = sent.lower()
        
        # æ’é™¤å¤±è´¥å’Œé€šç”¨ä¿¡æ¯
        if any(ex in sent_lower for ex in exclude_keywords):
            continue
        
        if len(sent) < 40:
            continue
        
        # ååŠéƒ¨åˆ†æƒé‡æ›´é«˜
        weight = 2 if i >= second_half_start else 1
        
        if any(kw in sent_lower for kw in emotional_keywords):
            effect_info['emotional'].append((weight, sent.strip()))
        if any(kw in sent_lower for kw in behavioral_keywords):
            # ç¡®ä¿æ˜¯ç§¯æå˜åŒ–
            if any(pos in sent_lower for pos in ['better', 'improved', 'able', 'began', 'started']):
                effect_info['behavioral'].append((weight, sent.strip()))
        if any(kw in sent_lower for kw in cognitive_keywords):
            effect_info['cognitive'].append((weight, sent.strip()))
    
    # æŒ‰æƒé‡æ’åºå¹¶å»é‡
    result_parts = []
    
    for key in ['emotional', 'behavioral', 'cognitive']:
        # æ’åº
        items = sorted(effect_info[key], key=lambda x: x[0], reverse=True)
        sents = [item[1] for item in items[:2]]
        sents = deduplicate_sentences(sents)
        result_parts.extend(sents)
    
    if result_parts:
        result = " ".join(result_parts)
        return clean_sentence(result[:600])
    
    # å¤‡é€‰
    return clean_sentence(" ".join(consult_process[-2:])[:500])

def main():
    print("="*80)
    print("ğŸ§  è¯­ä¹‰ç†è§£ç‰ˆæå– - V4æœ€ç»ˆç‰ˆ")
    print("="*80)
    
    test_file = Path("data/test/Emotion_Summary.jsonl")
    print(f"\nğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
    
    test_data = []
    with open(test_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                test_data.append(json.loads(line))
    
    print(f"âœ… æµ‹è¯•æ ·æœ¬æ•°: {len(test_data)}")
    
    print(f"\nğŸ’¡ V4æ”¹è¿›è¦ç‚¹:")
    print(f"  1. ç—…å› : å®Œæ•´é€»è¾‘é“¾ï¼ˆäº‹ä»¶â†’æƒ…æ„Ÿâ†’å…³ç³»â†’è½¬åŒ–ï¼‰")
    print(f"  2. ç—‡çŠ¶: å®Œæ•´åˆ†ç±»ï¼ˆèº¯ä½“+å¿ƒç†+è¡Œä¸ºï¼‰ï¼Œå»é‡")
    print(f"  3. æ²»ç–—: ä¸“ä¸šå¹²é¢„ä¸ºä¸»ï¼ˆæ’é™¤è‡ªè¡Œå°è¯•ï¼‰")
    print(f"  4. ç‰¹å¾: å½’çº³æ€»ç»“ï¼ˆéé‡å¤ç—‡çŠ¶ï¼‰")
    print(f"  5. æ•ˆæœ: å®é™…æ”¹å–„ï¼ˆæ’é™¤å¤±è´¥å’Œé€šç”¨ä¿¡æ¯ï¼‰")
    
    print(f"\nğŸ”® å¼€å§‹è¯­ä¹‰æå–...\n")
    results = []
    
    for item in tqdm(test_data, desc="æå–è¿›åº¦"):
        item_id = item.get("id", 0)
        case_desc = item.get("case_description", [])
        consult_process = item.get("consultation_process", [])
        
        result = {
            "id": item_id,
            "predicted_cause": extract_cause_semantic(case_desc, consult_process),
            "predicted_symptoms": extract_symptoms_semantic(case_desc, consult_process),
            "predicted_treatment_process": extract_treatment_semantic(case_desc, consult_process),
            "predicted_illness_Characteristics": extract_characteristics_semantic(case_desc, consult_process),
            "predicted_treatment_effect": extract_effect_semantic(case_desc, consult_process)
        }
        
        results.append(result)
    
    # å¤‡ä»½å¹¶ä¿å­˜
    output_file = Path("results/Emotion_Summary_Result.jsonl")
    
    if output_file.exists():
        backup_file = Path("results/Emotion_Summary_Result_v3_precise.jsonl")
        import shutil
        shutil.copy(output_file, backup_file)
        print(f"\nğŸ“¦ V3ç‰ˆæœ¬å·²å¤‡ä»½åˆ°: {backup_file}")
    
    print(f"\nğŸ’¾ ä¿å­˜V4ç»“æœåˆ°: {output_file} (è¦†ç›–)")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    
    print(f"âœ… å®Œæˆï¼å…±ç”Ÿæˆ {len(results)} æ¡ç»“æœ")
    
    # æ˜¾ç¤ºç¤ºä¾‹
    print(f"\nğŸ“ ç¤ºä¾‹è¾“å‡º (æ ·æœ¬ 1):")
    print("="*80)
    sample = results[0]
    
    fields = [
        ("predicted_cause", "ç—…å› "),
        ("predicted_symptoms", "ç—‡çŠ¶"),
        ("predicted_treatment_process", "æ²»ç–—è¿‡ç¨‹"),
        ("predicted_illness_Characteristics", "ç–¾ç—…ç‰¹å¾"),
        ("predicted_treatment_effect", "æ²»ç–—æ•ˆæœ")
    ]
    
    for field, name in fields:
        content = sample[field]
        print(f"\nã€{name}ã€‘({len(content)} å­—ç¬¦)")
        print(f"{content[:200]}...")
    
    # è´¨é‡æ£€æŸ¥
    print(f"\n" + "="*80)
    print(f"ğŸ” è´¨é‡æ£€æŸ¥")
    print("="*80)
    
    contents = [sample[field] for field, _ in fields]
    unique = len(set(contents))
    print(f"\n1. é‡å¤åº¦: {unique}/5 å”¯ä¸€")
    
    lengths = [len(c) for c in contents]
    print(f"2. å¹³å‡é•¿åº¦: {sum(lengths)/len(lengths):.0f} å­—ç¬¦")
    
    # æ£€æŸ¥å…³é”®ä¿¡æ¯
    full_text = " ".join(contents)
    checks = {
        "æ”¶å…»èƒŒæ™¯": any(kw in full_text.lower() for kw in ['adopt', 'biological']),
        "æƒ…æ„Ÿå†²çª": any(kw in full_text.lower() for kw in ['anger', 'guilt', 'conflict']),
        "å‚¬çœ æ²»ç–—": any(kw in full_text.lower() for kw in ['hypnosis', 'imagery']),
        "ç—‡çŠ¶åˆ†ç±»": any(kw in full_text.lower() for kw in ['throat', 'anxiety', 'check']),
        "æ²»ç–—æ”¹å–„": any(kw in full_text.lower() for kw in ['smile', 'relief', 'better'])
    }
    
    print(f"\n3. å…³é”®ä¿¡æ¯è¦†ç›–:")
    for key, value in checks.items():
        status = "âœ…" if value else "âŒ"
        print(f"   {status} {key}")
    
    print("\n" + "="*80)
    print("âœ… V4è¯­ä¹‰æå–å®Œæˆï¼")
    print(f"ğŸ“ æäº¤æ–‡ä»¶: {output_file}")
    print("="*80)

if __name__ == "__main__":
    main()

