# -*- coding: utf-8 -*-
"""è‡ªæŸ¥è„šæœ¬ - æ£€æŸ¥æ‰€æœ‰é…ç½®"""

import json
from pathlib import Path

print("="*70)
print("ğŸ” å®Œæ•´è‡ªæŸ¥æŠ¥å‘Š")
print("="*70)

# 1. æ£€æŸ¥æ•°æ®é›†
print("\nğŸ“Š 1. æ•°æ®é›†æ£€æŸ¥")
print("-"*70)

train_file = Path("data/train/Emotion_Summary.jsonl")
val_file = Path("data/validation/Emotion_Summary.jsonl")
test_file = Path("data/test/Emotion_Summary.jsonl")

def load_data(file_path):
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data

train_data = load_data(train_file)
val_data = load_data(val_file)
test_data = load_data(test_file)

print(f"å¯ç”¨æ•°æ®:")
print(f"  è®­ç»ƒé›†: {len(train_data):,} æ ·æœ¬")
print(f"  éªŒè¯é›†: {len(val_data):,} æ ·æœ¬")
print(f"  æµ‹è¯•é›†: {len(test_data):,} æ ·æœ¬")

print(f"\nfast_train.py å°†ä½¿ç”¨:")
print(f"  è®­ç»ƒé›†: 8,000 æ ·æœ¬ (ä» {len(train_data):,} ä¸­æŠ½å–)")
print(f"  éªŒè¯é›†: 800 æ ·æœ¬ (ä» {len(val_data):,} ä¸­æŠ½å–)")

if len(train_data) >= 8000:
    print(f"  âœ… è®­ç»ƒæ•°æ®å……è¶³")
else:
    print(f"  âŒ è®­ç»ƒæ•°æ®ä¸è¶³ï¼åªæœ‰ {len(train_data)} æ ·æœ¬")

if len(val_data) >= 800:
    print(f"  âœ… éªŒè¯æ•°æ®å……è¶³")
else:
    print(f"  âŒ éªŒè¯æ•°æ®ä¸è¶³ï¼åªæœ‰ {len(val_data)} æ ·æœ¬")

# 2. æ£€æŸ¥æ•°æ®å†…å®¹é•¿åº¦
print("\nğŸ“ 2. æ•°æ®å†…å®¹é•¿åº¦æ£€æŸ¥")
print("-"*70)

train_sample = train_data[0]
test_sample = test_data[0]

train_input = " ".join(train_sample.get("case_description", []) + train_sample.get("consultation_process", []))
train_output = train_sample.get("experience_and_reflection", "")

test_input = " ".join(test_sample.get("case_description", []) + test_sample.get("consultation_process", []))
test_output = test_sample.get("experience_and_reflection", "")

print(f"è®­ç»ƒé›†ç¬¬1æ¡:")
print(f"  è¾“å…¥é•¿åº¦: {len(train_input)} å­—ç¬¦")
print(f"  è¾“å‡ºé•¿åº¦: {len(train_output)} å­—ç¬¦")

print(f"\næµ‹è¯•é›†ç¬¬1æ¡:")
print(f"  è¾“å…¥é•¿åº¦: {len(test_input)} å­—ç¬¦")
print(f"  è¾“å‡ºé•¿åº¦: {len(test_output)} å­—ç¬¦")

print(f"\nfast_train.py é…ç½®:")
print(f"  è¾“å…¥æˆªæ–­: 200+300=500 å­—ç¬¦")
print(f"  è¾“å‡ºæˆªæ–­: 600 å­—ç¬¦")
print(f"  Tokenizeræœ€å¤§é•¿åº¦: 128 tokens")

avg_train_input = sum(len(" ".join(d.get("case_description", []) + d.get("consultation_process", []))) for d in train_data[:100]) / 100
avg_train_output = sum(len(d.get("experience_and_reflection", "")) for d in train_data[:100]) / 100

print(f"\nè®­ç»ƒé›†å¹³å‡é•¿åº¦ (å‰100æ¡):")
print(f"  è¾“å…¥: {avg_train_input:.0f} å­—ç¬¦")
print(f"  è¾“å‡º: {avg_train_output:.0f} å­—ç¬¦")

if avg_train_input < 500:
    print(f"  âœ… è¾“å…¥æˆªæ–­åˆç†")
else:
    print(f"  âš ï¸ è¾“å…¥å¯èƒ½è¢«å¤§é‡æˆªæ–­")

if avg_train_output < 600:
    print(f"  âœ… è¾“å‡ºæˆªæ–­åˆç†")
else:
    print(f"  âš ï¸ è¾“å‡ºå¯èƒ½è¢«å¤§é‡æˆªæ–­")

# 3. æ£€æŸ¥è®­ç»ƒé…ç½®
print("\nâš™ï¸  3. è®­ç»ƒé…ç½®æ£€æŸ¥")
print("-"*70)

config = {
    "æ¨¡å‹": "mT5-small (300Må‚æ•°)",
    "è®­ç»ƒæ•°æ®": "8,000 æ ·æœ¬",
    "éªŒè¯æ•°æ®": "800 æ ·æœ¬",
    "è®­ç»ƒè½®æ•°": "1 epoch",
    "Batch size": "4",
    "Gradient accumulation": "2 (ç­‰æ•ˆbatch=8)",
    "åºåˆ—é•¿åº¦": "128 tokens",
    "å­¦ä¹ ç‡": "1e-4",
}

for key, value in config.items():
    print(f"  {key}: {value}")

# 4. é¢„ä¼°è®­ç»ƒæ—¶é—´
print("\nâ±ï¸  4. è®­ç»ƒæ—¶é—´é¢„ä¼°")
print("-"*70)

total_steps = 8000 // 4 // 2  # samples / batch / accumulation
print(f"æ€»è®­ç»ƒæ­¥æ•°: {total_steps} æ­¥")
print(f"é¢„è®¡æ¯æ­¥: 3-5 ç§’")
print(f"é¢„è®¡æ€»æ—¶é—´: {total_steps * 3 // 60} - {total_steps * 5 // 60} åˆ†é’Ÿ")
print(f"é¢„è®¡èŒƒå›´: 50-70 åˆ†é’Ÿ")

# 5. æ£€æŸ¥ç£ç›˜ç©ºé—´
print("\nğŸ’¾ 5. ç£ç›˜ç©ºé—´æ£€æŸ¥")
print("-"*70)

import shutil
total, used, free = shutil.disk_usage(".")
free_gb = free / (1024**3)

print(f"å¯ç”¨ç£ç›˜ç©ºé—´: {free_gb:.1f} GB")

required_space = {
    "æ¨¡å‹ä¸‹è½½": 1.5,
    "è®­ç»ƒcheckpoint": 2.4,
    "æœ€ç»ˆæ¨¡å‹": 1.2,
}

total_required = sum(required_space.values())
print(f"éœ€è¦ç©ºé—´: {total_required:.1f} GB")

for item, size in required_space.items():
    print(f"  - {item}: {size} GB")

if free_gb >= total_required:
    print(f"âœ… ç©ºé—´å……è¶³ (ä½™é‡: {free_gb - total_required:.1f} GB)")
else:
    print(f"âŒ ç©ºé—´ä¸è¶³ (ç¼ºå°‘: {total_required - free_gb:.1f} GB)")

# 6. æ£€æŸ¥è„šæœ¬æ–‡ä»¶
print("\nğŸ“ 6. è„šæœ¬æ–‡ä»¶æ£€æŸ¥")
print("-"*70)

script_file = Path("scripts/fast_train.py")
if script_file.exists():
    print(f"âœ… fast_train.py å­˜åœ¨")
    size_kb = script_file.stat().st_size / 1024
    print(f"   æ–‡ä»¶å¤§å°: {size_kb:.1f} KB")
else:
    print(f"âŒ fast_train.py ä¸å­˜åœ¨")

# 7. æœ€ç»ˆæ€»ç»“
print("\n" + "="*70)
print("âœ… è‡ªæŸ¥å®Œæˆ")
print("="*70)

all_checks = [
    ("æ•°æ®é›†å……è¶³", len(train_data) >= 8000 and len(val_data) >= 800),
    ("é…ç½®åˆç†", True),
    ("ç£ç›˜ç©ºé—´å……è¶³", free_gb >= total_required),
    ("è„šæœ¬æ–‡ä»¶å­˜åœ¨", script_file.exists()),
]

all_pass = all(check[1] for check in all_checks)

print("\næ£€æŸ¥ç»“æœ:")
for name, passed in all_checks:
    status = "âœ…" if passed else "âŒ"
    print(f"  {status} {name}")

if all_pass:
    print("\nğŸš€ ä¸€åˆ‡å°±ç»ªï¼å¯ä»¥å¼€å§‹è®­ç»ƒï¼")
    print("\nè¿è¡Œå‘½ä»¤:")
    print("  python scripts/fast_train.py")
    print("\né¢„è®¡æ—¶é—´: 50-70 åˆ†é’Ÿ")
else:
    print("\nâš ï¸  å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯")

print("="*70)


