# -*- coding: utf-8 -*-
"""å¯¹æ¯”è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†"""

import json
from pathlib import Path

def load_data(file_path):
    """åŠ è½½JSONLæ•°æ®"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data

def analyze_dataset(data, name):
    """åˆ†ææ•°æ®é›†ç‰¹å¾"""
    print(f"\n{'='*70}")
    print(f"ğŸ“Š {name}")
    print(f"{'='*70}")
    
    print(f"\næ•°é‡: {len(data):,} ä¸ªæ ·æœ¬")
    
    if len(data) > 0:
        first = data[0]
        print(f"\nå­—æ®µ: {list(first.keys())}")
        
        # æ£€æŸ¥è¯­è¨€
        case_desc = first.get('case_description', [''])[0]
        consult_proc = first.get('consultation_process', [''])[0] if first.get('consultation_process') else ''
        
        is_english = any(word in (case_desc + consult_proc).lower() for word in ['the', 'and', 'is', 'was', 'visitor', 'client'])
        language = "è‹±æ–‡ (English)" if is_english else "ä¸­æ–‡ (Chinese)"
        print(f"\nè¯­è¨€: {language}")
        
        # æ˜¾ç¤ºç¤ºä¾‹
        print(f"\ncase_description ç¤ºä¾‹:")
        print(f"  {case_desc[:100]}...")
        
        if consult_proc:
            print(f"\nconsultation_process ç¤ºä¾‹:")
            print(f"  {consult_proc[:100]}...")
        
        # ç»Ÿè®¡é•¿åº¦
        case_desc_lens = [len(item.get('case_description', [])) for item in data]
        consult_lens = [len(item.get('consultation_process', [])) for item in data]
        
        print(f"\nå†…å®¹é•¿åº¦ç»Ÿè®¡:")
        print(f"  case_description æ®µæ•° - å¹³å‡: {sum(case_desc_lens)/len(case_desc_lens):.1f}, æœ€å°: {min(case_desc_lens)}, æœ€å¤§: {max(case_desc_lens)}")
        print(f"  consultation_process æ®µæ•° - å¹³å‡: {sum(consult_lens)/len(consult_lens):.1f}, æœ€å°: {min(consult_lens)}, æœ€å¤§: {max(consult_lens)}")
        
        # æ£€æŸ¥ experience_and_reflection
        has_reflection = 'experience_and_reflection' in first
        if has_reflection:
            reflection_lens = [len(item.get('experience_and_reflection', '')) for item in data]
            print(f"  experience_and_reflection å­—ç¬¦æ•° - å¹³å‡: {sum(reflection_lens)/len(reflection_lens):.0f}, æœ€å°: {min(reflection_lens)}, æœ€å¤§: {max(reflection_lens)}")
            print(f"\nâœ… åŒ…å« experience_and_reflection (è®­ç»ƒç›®æ ‡)")
        else:
            print(f"\nâŒ ä¸åŒ…å« experience_and_reflection (éœ€è¦æ¨¡å‹ç”Ÿæˆ)")

def main():
    print("="*70)
    print("ğŸ” è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†å®Œæ•´å¯¹æ¯”")
    print("="*70)
    
    # åŠ è½½æ•°æ®
    train_file = Path("data/train/Emotion_Summary.jsonl")
    val_file = Path("data/validation/Emotion_Summary.jsonl")
    test_file = Path("data/test/Emotion_Summary.jsonl")
    
    train_data = load_data(train_file)
    val_data = load_data(val_file)
    test_data = load_data(test_file)
    
    # åˆ†ææ¯ä¸ªæ•°æ®é›†
    analyze_dataset(train_data, "è®­ç»ƒé›† (Training Set)")
    analyze_dataset(val_data, "éªŒè¯é›† (Validation Set)")
    analyze_dataset(test_data, "æµ‹è¯•é›† (Test Set)")
    
    # å¯¹æ¯”æ€»ç»“
    print(f"\n{'='*70}")
    print("ğŸ“ å¯¹æ¯”æ€»ç»“")
    print(f"{'='*70}")
    
    print(f"\næ•°é‡å¯¹æ¯”:")
    print(f"  è®­ç»ƒé›†: {len(train_data):,} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(val_data):,} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(test_data):,} æ ·æœ¬")
    print(f"  è®­ç»ƒ/éªŒè¯æ¯”ä¾‹: {len(train_data)/len(val_data):.1f}:1")
    print(f"  è®­ç»ƒé›†æ˜¯æµ‹è¯•é›†çš„: {len(train_data)/len(test_data):.1f} å€")
    
    # è¯­è¨€ä¸€è‡´æ€§æ£€æŸ¥
    train_sample = train_data[0]['case_description'][0]
    test_sample = test_data[0]['case_description'][0]
    
    train_is_english = any(word in train_sample.lower() for word in ['the', 'and', 'is', 'client', 'counselor'])
    test_is_english = any(word in test_sample.lower() for word in ['the', 'and', 'is', 'visitor'])
    
    print(f"\nè¯­è¨€ä¸€è‡´æ€§:")
    if train_is_english and test_is_english:
        print(f"  âœ… è®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½æ˜¯è‹±æ–‡ - å®Œç¾åŒ¹é…ï¼")
    elif not train_is_english and not test_is_english:
        print(f"  âœ… è®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½æ˜¯ä¸­æ–‡ - å®Œç¾åŒ¹é…ï¼")
    else:
        print(f"  âŒ è­¦å‘Šï¼šè®­ç»ƒé›†å’Œæµ‹è¯•é›†è¯­è¨€ä¸ä¸€è‡´ï¼")
        print(f"     è®­ç»ƒé›†: {'è‹±æ–‡' if train_is_english else 'ä¸­æ–‡'}")
        print(f"     æµ‹è¯•é›†: {'è‹±æ–‡' if test_is_english else 'ä¸­æ–‡'}")
    
    # ä»»åŠ¡è¯´æ˜
    print(f"\n{'='*70}")
    print("ğŸ¯ è®­ç»ƒä»»åŠ¡")
    print(f"{'='*70}")
    print(f"\nä»»åŠ¡ç±»å‹: æ–‡æœ¬ç”Ÿæˆ (Text Generation)")
    print(f"è¾“å…¥: case_description + consultation_process")
    print(f"è¾“å‡º: experience_and_reflection")
    print(f"\næ¨¡å‹ç›®æ ‡:")
    print(f"  1. å­¦ä¹ ä» {len(train_data):,} ä¸ªè®­ç»ƒæ ·æœ¬ä¸­ç†è§£å’¨è¯¢æ¡ˆä¾‹")
    print(f"  2. åœ¨ {len(val_data):,} ä¸ªéªŒè¯æ ·æœ¬ä¸Šè¯„ä¼°æ€§èƒ½")
    print(f"  3. æœ€ç»ˆåœ¨ {len(test_data):,} ä¸ªæµ‹è¯•æ ·æœ¬ä¸Šç”Ÿæˆé«˜è´¨é‡çš„ç»éªŒåæ€")
    
    # æ•°æ®å……åˆ†æ€§è¯„ä¼°
    print(f"\næ•°æ®å……åˆ†æ€§è¯„ä¼°:")
    if len(train_data) > 10000:
        print(f"  âœ… è®­ç»ƒæ•°æ®å……è¶³ ({len(train_data):,} æ ·æœ¬) - å¯ä»¥è®­ç»ƒå‡ºè‰¯å¥½çš„æ¨¡å‹")
    elif len(train_data) > 1000:
        print(f"  âš ï¸  è®­ç»ƒæ•°æ®ä¸­ç­‰ ({len(train_data):,} æ ·æœ¬) - å»ºè®®å¤šè®­ç»ƒå‡ è½®")
    else:
        print(f"  âŒ è®­ç»ƒæ•°æ®è¾ƒå°‘ ({len(train_data):,} æ ·æœ¬) - å¯èƒ½éœ€è¦æ›´å¤šæ•°æ®æˆ–æ•°æ®å¢å¼º")
    
    print(f"\n{'='*70}")
    print("âœ… å¯¹æ¯”å®Œæˆï¼ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼")
    print(f"{'='*70}")
    print(f"\nä¸‹ä¸€æ­¥: python scripts/simple_train.py")

if __name__ == "__main__":
    main()

