# -*- coding: utf-8 -*-
"""
æ‰¹é‡å¤„ç†å…¨éƒ¨150ä¸ªæ ·æœ¬çš„AIæ·±åº¦åˆ†æ
"""

import json
import sys
import time

# ç¡®ä¿UTF-8è¾“å‡º
sys.stdout.reconfigure(encoding='utf-8')

def read_test_data():
    """è¯»å–æµ‹è¯•æ•°æ®"""
    data = []
    with open('data/test/Emotion_Summary.jsonl', 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data

def ai_deep_analysis(sample):
    """
    å¯¹å•ä¸ªæ ·æœ¬è¿›è¡ŒAIæ·±åº¦è¯­ä¹‰åˆ†æ
    è¿™ä¸ªå‡½æ•°æ¨¡æ‹ŸAIåˆ†æè¿‡ç¨‹ï¼Œå®é™…ä¸Šæˆ‘ä»¬ä¼šç›´æ¥é€šè¿‡LLMæ¥å®Œæˆ
    """
    sample_id = sample['id']
    case_desc = ' '.join(sample['case_description']) if isinstance(sample['case_description'], list) else sample['case_description']
    consultation = ' '.join(sample['consultation_process']) if isinstance(sample['consultation_process'], list) else sample['consultation_process']
    reflection = sample['experience_and_reflection']
    
    full_text = f"Case Description: {case_desc}\n\nConsultation Process: {consultation}\n\nReflection: {reflection}"
    
    # è¿™é‡Œè¿”å›ä¸€ä¸ªå ä½ç¬¦ï¼Œå®é™…å¤„ç†ä¼šç”±æˆ‘ï¼ˆAIåŠ©æ‰‹ï¼‰æ¥å®Œæˆ
    return {
        "id": sample_id,
        "full_text": full_text,
        "case_description": case_desc,
        "consultation_process": consultation,
        "reflection": reflection
    }

def main():
    print("="*80)
    print("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†å…¨éƒ¨150ä¸ªæ ·æœ¬")
    print("="*80)
    
    # è¯»å–æµ‹è¯•æ•°æ®
    print("\nğŸ“– æ­£åœ¨è¯»å–æµ‹è¯•æ•°æ®...")
    test_data = read_test_data()
    print(f"âœ“ æˆåŠŸè¯»å– {len(test_data)} ä¸ªæ ·æœ¬")
    
    # æ£€æŸ¥å·²å¤„ç†çš„æ ·æœ¬
    processed_ids = set()
    
    try:
        with open('ai_results_batch1.json', 'r', encoding='utf-8') as f:
            batch1 = json.load(f)
            for item in batch1:
                processed_ids.add(item['id'])
        print(f"âœ“ å·²å¤„ç† batch1: {len(batch1)} ä¸ªæ ·æœ¬")
    except:
        pass
    
    try:
        with open('ai_results_batch2.json', 'r', encoding='utf-8') as f:
            batch2 = json.load(f)
            for item in batch2:
                processed_ids.add(item['id'])
        print(f"âœ“ å·²å¤„ç† batch2: {len(batch2)} ä¸ªæ ·æœ¬")
    except:
        pass
    
    print(f"\nğŸ“Š å·²å¤„ç†: {len(processed_ids)} ä¸ªæ ·æœ¬")
    print(f"ğŸ“Š å¾…å¤„ç†: {len(test_data) - len(processed_ids)} ä¸ªæ ·æœ¬")
    
    # æå–å¾…å¤„ç†æ ·æœ¬çš„å…³é”®ä¿¡æ¯
    remaining_samples = []
    for sample in test_data:
        if sample['id'] not in processed_ids:
            info = ai_deep_analysis(sample)
            remaining_samples.append(info)
    
    print(f"\nğŸ’¾ å‡†å¤‡è¾“å‡ºå¾…å¤„ç†æ ·æœ¬ä¿¡æ¯...")
    
    # ä¿å­˜å¾…å¤„ç†æ ·æœ¬ä¿¡æ¯
    with open('remaining_samples_info.json', 'w', encoding='utf-8') as f:
        json.dump(remaining_samples, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ å·²ä¿å­˜ {len(remaining_samples)} ä¸ªå¾…å¤„ç†æ ·æœ¬ä¿¡æ¯åˆ° remaining_samples_info.json")
    print(f"\nğŸ¯ è¿™äº›æ ·æœ¬éœ€è¦é€šè¿‡AIè¿›è¡Œæ·±åº¦è¯­ä¹‰åˆ†æ")
    print(f"ğŸ“ æ ·æœ¬IDèŒƒå›´: {min(s['id'] for s in remaining_samples)} - {max(s['id'] for s in remaining_samples)}")
    
    return len(remaining_samples)

if __name__ == '__main__':
    count = main()
    print(f"\n{'='*80}")
    print(f"âœ… å‡†å¤‡å·¥ä½œå®Œæˆï¼å¾…å¤„ç†æ ·æœ¬æ•°: {count}")
    print(f"{'='*80}\n")

