# -*- coding: utf-8 -*-
"""é—®é¢˜åˆ†æ"""

import json

print("="*80)
print("âŒ ç”Ÿæˆç»“æœé—®é¢˜åˆ†æ")
print("="*80)

# è¯»å–ç”Ÿæˆçš„ç»“æœ
with open("results/Emotion_Summary_Result.jsonl", 'r', encoding='utf-8') as f:
    results = [json.loads(line) for line in f if line.strip()]

# åˆ†æç¬¬ä¸€ä¸ªæ ·æœ¬
sample = results[0]

print(f"\nã€æ ·æœ¬ 1ã€‘")
print(f"ID: {sample['id']}")
print("-"*80)

fields = ["predicted_cause", "predicted_symptoms", "predicted_treatment_process", 
          "predicted_illness_Characteristics", "predicted_treatment_effect"]

print("\nğŸ” æ£€æŸ¥5ä¸ªå­—æ®µå†…å®¹:")
for i, field in enumerate(fields, 1):
    content = sample[field]
    print(f"\n{i}. {field}:")
    print(f"   é•¿åº¦: {len(content)} å­—ç¬¦")
    print(f"   å†…å®¹: {content[:150]}...")

# æ£€æŸ¥é‡å¤åº¦
print("\n"+"="*80)
print("âš ï¸  é‡å¤åº¦åˆ†æ")
print("="*80)

contents = [sample[field] for field in fields]
unique_contents = set(contents)

print(f"\næ€»å­—æ®µæ•°: {len(contents)}")
print(f"å”¯ä¸€å†…å®¹æ•°: {len(unique_contents)}")

if len(unique_contents) < len(contents):
    print(f"\nâŒ å‘ç°é‡å¤ï¼æœ‰ {len(contents) - len(unique_contents)} ä¸ªå­—æ®µå†…å®¹å®Œå…¨ç›¸åŒ")
    
    # æ£€æŸ¥å“ªäº›å­—æ®µç›¸åŒ
    from collections import Counter
    counter = Counter(contents)
    for content, count in counter.items():
        if count > 1:
            print(f"\né‡å¤å†…å®¹ (å‡ºç°{count}æ¬¡):")
            print(f"  {content[:100]}...")
            print(f"  ç›¸å…³å­—æ®µ:", [fields[i] for i, c in enumerate(contents) if c == content])
else:
    print(f"\nâœ… æ‰€æœ‰å­—æ®µå†…å®¹ä¸åŒ")

# æ£€æŸ¥æ˜¯å¦åŒ…å«æ¡ˆä¾‹ç»†èŠ‚
print("\n"+"="*80)
print("ğŸ” æ¡ˆä¾‹ç»†èŠ‚æå–æ£€æŸ¥")
print("="*80)

# å…³é”®è¯åˆ—è¡¨ï¼ˆä»æµ‹è¯•é›†ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰
keywords = [
    "adopt", "adoption", "Shanxi", "hypochondria", 
    "nasopharynx", "throat", "hemorrhoid", "hypnosis",
    "guided imagery", "marriage", "wife", "biological parent"
]

print(f"\næ£€æŸ¥æ˜¯å¦åŒ…å«æ¡ˆä¾‹å…³é”®è¯:")
found_keywords = []
for keyword in keywords:
    found = any(keyword.lower() in sample[field].lower() for field in fields)
    status = "âœ…" if found else "âŒ"
    print(f"  {status} {keyword}")
    if found:
        found_keywords.append(keyword)

print(f"\næå–å…³é”®è¯æ•°: {len(found_keywords)}/{len(keywords)}")

if len(found_keywords) < 3:
    print(f"\nâŒ ä¸¥é‡é—®é¢˜ï¼šå‡ ä¹æ²¡æœ‰æå–åˆ°æ¡ˆä¾‹å…·ä½“ä¿¡æ¯ï¼")
else:
    print(f"\nâœ… æå–äº†éƒ¨åˆ†æ¡ˆä¾‹ä¿¡æ¯")

# åˆ†æé—®é¢˜åŸå› 
print("\n"+"="*80)
print("ğŸ“‹ é—®é¢˜åŸå› åˆ†æ")
print("="*80)

print("\n1. æ¨¡å‹è®­ç»ƒä¸åŒ¹é…:")
print("   - è®­ç»ƒæ•°æ®: Empathetic Dialogues (æƒ…æ„Ÿå¯¹è¯)")
print("   - ç›®æ ‡ä»»åŠ¡: ä¿¡æ¯æå– (ä»æ¡ˆä¾‹ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯)")
print("   - ç»“è®º: æ¨¡å‹æ²¡æœ‰å­¦ä¼šæå–ä¿¡æ¯ï¼Œåªä¼šç”Ÿæˆæ¨¡æ¿åŒ–æ–‡æœ¬")

print("\n2. Promptè®¾è®¡é—®é¢˜:")
print("   - å½“å‰promptå¤ªç®€å•ï¼Œåªæ˜¯è¦æ±‚ 'summarize the CAUSE'")
print("   - æ¨¡å‹ä¸ç†è§£éœ€è¦ä»è¾“å…¥ä¸­æå–å…·ä½“ç»†èŠ‚")
print("   - éœ€è¦æ›´æ˜ç¡®çš„æŒ‡ä»¤")

print("\n3. åºåˆ—é•¿åº¦é™åˆ¶:")
print("   - è¾“å…¥: 512 tokens (å¯èƒ½ä¸å¤Ÿå®¹çº³å®Œæ•´æ¡ˆä¾‹)")
print("   - è¾“å‡º: 256 tokens (é™åˆ¶äº†è¯¦ç»†ç¨‹åº¦)")
print("   - å®é™…è¾“å‡º: ~280 å­—ç¬¦ (çº¦70 tokens)")

print("\n"+"="*80)
print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ")
print("="*80)

print("\næ–¹æ¡ˆ1: æ”¹è¿›Prompt (æ¨è)")
print("  - ä½¿ç”¨æ›´æ˜ç¡®çš„æå–å¼prompt")
print("  - åœ¨promptä¸­ç»™å‡ºç¤ºä¾‹")
print("  - å¼ºè°ƒ 'extract specific details from the case'")

print("\næ–¹æ¡ˆ2: å¢åŠ åºåˆ—é•¿åº¦")
print("  - è¾“å…¥: 512 â†’ 1024 tokens")
print("  - è¾“å‡º: 256 â†’ 512 tokens")
print("  - ç¼ºç‚¹: é€Ÿåº¦æ…¢5-10å€")

print("\næ–¹æ¡ˆ3: ä½¿ç”¨æ›´å¥½çš„ç”Ÿæˆå‚æ•°")
print("  - å¢åŠ  num_beams: 4 â†’ 8")
print("  - è°ƒæ•´ temperature å’Œ top_p")
print("  - ä½¿ç”¨ do_sample=True")

print("\næ–¹æ¡ˆ4: åå¤„ç†ä¼˜åŒ–")
print("  - æ£€æµ‹é‡å¤å†…å®¹")
print("  - ä½¿ç”¨è§„åˆ™æå–å…³é”®ä¿¡æ¯")
print("  - ç»“åˆå…³é”®è¯åŒ¹é…")

print("\n"+"="*80)
print("æ¨è: å…ˆå°è¯•æ–¹æ¡ˆ1ï¼ˆæ”¹è¿›Promptï¼‰ + æ–¹æ¡ˆ2ï¼ˆå¢åŠ åºåˆ—é•¿åº¦ï¼‰")
print("="*80)

