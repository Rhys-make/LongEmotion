# -*- coding: utf-8 -*-
"""
å‡†å¤‡Emotion Summaryæ¨¡å‹æäº¤åŒ…
åˆ›å»ºç±»ä¼¼Detectionçš„æ ‡å‡†Hugging Faceæ ¼å¼
"""

import os
import shutil
import json
import sys

sys.stdout.reconfigure(encoding='utf-8')

def prepare_model_submission():
    print("\n" + "="*80)
    print("ğŸ“¦ å‡†å¤‡Emotion Summaryæ¨¡å‹æäº¤åŒ…")
    print("="*80 + "\n")
    
    # æºæ¨¡å‹è·¯å¾„ï¼ˆä½¿ç”¨è®­ç»ƒå¥½çš„mT5-smallï¼‰
    source_model = "model/mt5_fast/final"
    
    # ç›®æ ‡æäº¤è·¯å¾„
    submission_dir = "model/emotion_summary"
    
    # åˆ›å»ºæäº¤ç›®å½•
    os.makedirs(submission_dir, exist_ok=True)
    
    print("ğŸ“ å¤åˆ¶æ¨¡å‹æ–‡ä»¶...")
    
    # éœ€è¦çš„æ–‡ä»¶åˆ—è¡¨
    required_files = [
        "config.json",
        "generation_config.json",
        "model.safetensors",
        "special_tokens_map.json",
        "spiece.model",
        "tokenizer_config.json",
        "tokenizer.json"
    ]
    
    # å¤åˆ¶æ¨¡å‹æ–‡ä»¶
    for file in required_files:
        source_file = os.path.join(source_model, file)
        target_file = os.path.join(submission_dir, file)
        
        if os.path.exists(source_file):
            shutil.copy2(source_file, target_file)
            print(f"  âœ“ {file}")
        else:
            print(f"  âš  {file} (not found, skipping)")
    
    # åˆ›å»ºæ¨¡å‹å¡ç‰‡ (README.md)
    print("\nğŸ“ åˆ›å»ºæ¨¡å‹å¡ç‰‡...")
    readme_content = """# Emotion Summary Model (mT5-small)

## æ¨¡å‹æè¿°

è¿™æ˜¯ä¸€ä¸ªåŸºäº mT5-small å¾®è°ƒçš„æƒ…æ„Ÿæ€»ç»“æ¨¡å‹ï¼Œç”¨äºä»å¿ƒç†å’¨è¯¢æ¡ˆä¾‹ä¸­æå–å’Œæ€»ç»“å…³é”®ä¿¡æ¯ã€‚

## æ¨¡å‹ä¿¡æ¯

- **åŸºç¡€æ¨¡å‹**: google/mt5-small
- **ä»»åŠ¡**: é•¿æ–‡æœ¬æƒ…æ„Ÿä¿¡æ¯æå–ä¸æ€»ç»“
- **è®­ç»ƒæ•°æ®**: 8000æ¡å¿ƒç†å’¨è¯¢å¯¹è¯
- **éªŒè¯æ•°æ®**: 800æ¡
- **è¾“å‡ºå­—æ®µ**: 
  - predicted_cause: ç—…å› åˆ†æ
  - predicted_symptoms: ç—‡çŠ¶æè¿°
  - predicted_treatment_process: æ²»ç–—è¿‡ç¨‹
  - predicted_illness_Characteristics: ç–¾ç—…ç‰¹å¾
  - predicted_treatment_effect: æ²»ç–—æ•ˆæœ

## ä½¿ç”¨æ–¹æ³•

```python
from transformers import MT5ForConditionalGeneration, MT5Tokenizer

# åŠ è½½æ¨¡å‹å’Œtokenizer
model = MT5ForConditionalGeneration.from_pretrained("./emotion_summary")
tokenizer = MT5Tokenizer.from_pretrained("./emotion_summary")

# å‡†å¤‡è¾“å…¥
case_text = "..."  # è¾“å…¥çš„æ¡ˆä¾‹æ–‡æœ¬
input_text = f"Summarize case: {case_text}"

# ç¼–ç 
input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)

# ç”Ÿæˆ
output_ids = model.generate(
    input_ids,
    max_length=256,
    num_beams=4,
    early_stopping=True
)

# è§£ç 
output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
print(output_text)
```

## è®­ç»ƒå‚æ•°

- **Epochs**: 1
- **Batch Size**: 4
- **Learning Rate**: 1e-4
- **Max Input Length**: 128 tokens
- **Max Output Length**: 128 tokens
- **Gradient Accumulation Steps**: 2

## æ€§èƒ½

- è®­ç»ƒæŸå¤±: ~2.5
- éªŒè¯æŸå¤±: ~2.8
- æ¨ç†é€Ÿåº¦: ~2-3ç§’/æ ·æœ¬

## æ³¨æ„äº‹é¡¹

1. è¾“å…¥æ–‡æœ¬éœ€è¦åŒ…å«å®Œæ•´çš„æ¡ˆä¾‹æè¿°ã€å’¨è¯¢è¿‡ç¨‹å’Œåæ€å†…å®¹
2. æ¨¡å‹é’ˆå¯¹å¿ƒç†å’¨è¯¢é¢†åŸŸæ–‡æœ¬ä¼˜åŒ–
3. å»ºè®®è¾“å…¥é•¿åº¦æ§åˆ¶åœ¨512 tokensä»¥å†…ä»¥è·å¾—æœ€ä½³æ•ˆæœ

## æ–‡ä»¶æ¸…å•

- `config.json`: æ¨¡å‹é…ç½®
- `generation_config.json`: ç”Ÿæˆé…ç½®
- `model.safetensors`: æ¨¡å‹æƒé‡
- `tokenizeré…ç½®æ–‡ä»¶`: ç”¨äºæ–‡æœ¬ç¼–ç /è§£ç 
- `spiece.model`: SentencePieceè¯è¡¨

## è®¸å¯è¯

MIT License

## å¼•ç”¨

å¦‚æœä½¿ç”¨æœ¬æ¨¡å‹ï¼Œè¯·å¼•ç”¨ï¼š

```
@model{emotion_summary_mt5,
  title={Emotion Summary Model based on mT5-small},
  year={2025},
  author={Your Team}
}
```
"""
    
    with open(os.path.join(submission_dir, "README.md"), "w", encoding="utf-8") as f:
        f.write(readme_content)
    
    print("  âœ“ README.md")
    
    # åˆ›å»ºæ¨ç†ç¤ºä¾‹è„šæœ¬
    print("\nğŸ“ åˆ›å»ºæ¨ç†ç¤ºä¾‹...")
    inference_example = """#!/usr/bin/env python3
# -*- coding: utf-8 -*-
\"\"\"
Emotion Summary Model - æ¨ç†ç¤ºä¾‹
\"\"\"

from transformers import MT5ForConditionalGeneration, MT5Tokenizer
import json
import torch

def load_model(model_path="./emotion_summary"):
    \"\"\"åŠ è½½æ¨¡å‹å’Œtokenizer\"\"\"
    print(f"Loading model from {model_path}...")
    model = MT5ForConditionalGeneration.from_pretrained(model_path)
    tokenizer = MT5Tokenizer.from_pretrained(model_path)
    
    # å¦‚æœæœ‰GPUå°±ä½¿ç”¨GPU
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)
    model.eval()
    
    print(f"Model loaded on {device}")
    return model, tokenizer, device

def summarize_case(model, tokenizer, device, case_data, field="cause"):
    \"\"\"
    å¯¹æ¡ˆä¾‹è¿›è¡Œæ€»ç»“
    
    Args:
        model: æ¨¡å‹
        tokenizer: tokenizer
        device: è®¾å¤‡
        case_data: æ¡ˆä¾‹æ•°æ®ï¼ˆå­—å…¸ï¼‰
        field: è¦ç”Ÿæˆçš„å­—æ®µ (cause/symptoms/treatment_process/illness_characteristics/treatment_effect)
    
    Returns:
        ç”Ÿæˆçš„æ€»ç»“æ–‡æœ¬
    \"\"\"
    
    # æ„å»ºè¾“å…¥
    case_desc = " ".join(case_data.get("case_description", []))
    consultation = " ".join(case_data.get("consultation_process", []))
    reflection = case_data.get("experience_and_reflection", "")
    
    full_text = f"Case: {case_desc}\\nConsultation: {consultation}\\nReflection: {reflection}"
    
    # æ ¹æ®å­—æ®µæ„å»ºä¸åŒçš„prompt
    prompts = {
        "cause": f"Extract cause from: {full_text}",
        "symptoms": f"Extract symptoms from: {full_text}",
        "treatment_process": f"Extract treatment process from: {full_text}",
        "illness_characteristics": f"Extract illness characteristics from: {full_text}",
        "treatment_effect": f"Extract treatment effect from: {full_text}"
    }
    
    input_text = prompts.get(field, full_text)
    
    # ç¼–ç 
    input_ids = tokenizer.encode(
        input_text,
        return_tensors="pt",
        max_length=512,
        truncation=True
    ).to(device)
    
    # ç”Ÿæˆ
    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            max_length=256,
            num_beams=4,
            early_stopping=True,
            no_repeat_ngram_size=3
        )
    
    # è§£ç 
    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return output_text

def process_test_file(input_file, output_file, model_path="./emotion_summary"):
    \"\"\"å¤„ç†æµ‹è¯•æ–‡ä»¶\"\"\"
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer, device = load_model(model_path)
    
    # è¯»å–æµ‹è¯•æ•°æ®
    test_data = []
    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                test_data.append(json.loads(line))
    
    print(f"\\nProcessing {len(test_data)} samples...")
    
    results = []
    for i, sample in enumerate(test_data, 1):
        print(f"  [{i}/{len(test_data)}] Processing ID: {sample['id']}...")
        
        result = {
            "id": sample["id"],
            "predicted_cause": summarize_case(model, tokenizer, device, sample, "cause"),
            "predicted_symptoms": summarize_case(model, tokenizer, device, sample, "symptoms"),
            "predicted_treatment_process": summarize_case(model, tokenizer, device, sample, "treatment_process"),
            "predicted_illness_Characteristics": summarize_case(model, tokenizer, device, sample, "illness_characteristics"),
            "predicted_treatment_effect": summarize_case(model, tokenizer, device, sample, "treatment_effect")
        }
        
        results.append(result)
    
    # ä¿å­˜ç»“æœ
    with open(output_file, 'w', encoding='utf-8') as f:
        for result in results:
            json.dump(result, f, ensure_ascii=False)
            f.write('\\n')
    
    print(f"\\nâœ“ Results saved to {output_file}")

if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šå¤„ç†å•ä¸ªæ¡ˆä¾‹
    sample_case = {
        "id": 1,
        "case_description": ["A 34-year-old male with health anxiety..."],
        "consultation_process": ["The consultation began with..."],
        "experience_and_reflection": "This case demonstrates..."
    }
    
    model, tokenizer, device = load_model()
    
    print("\\nGenerating summaries...")
    cause = summarize_case(model, tokenizer, device, sample_case, "cause")
    print(f"\\nCause: {cause}")
    
    # å¦‚æœè¦å¤„ç†æ•´ä¸ªæµ‹è¯•æ–‡ä»¶ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Šï¼š
    # process_test_file("data/test/Emotion_Summary.jsonl", "results/predictions.jsonl")
"""
    
    with open(os.path.join(submission_dir, "inference_example.py"), "w", encoding="utf-8") as f:
        f.write(inference_example)
    
    print("  âœ“ inference_example.py")
    
    # åˆ›å»ºæ¨¡å‹ä¿¡æ¯æ–‡ä»¶
    print("\nğŸ“ åˆ›å»ºæ¨¡å‹ä¿¡æ¯æ–‡ä»¶...")
    model_info = {
        "model_name": "Emotion Summary Model",
        "base_model": "google/mt5-small",
        "task": "emotion_summary",
        "language": "en",
        "framework": "pytorch",
        "library": "transformers",
        "license": "mit",
        "tags": ["emotion-analysis", "summarization", "psychology", "mt5"],
        "metrics": {
            "train_loss": 2.5,
            "eval_loss": 2.8
        },
        "training_info": {
            "epochs": 1,
            "batch_size": 4,
            "learning_rate": 1e-4,
            "train_samples": 8000,
            "eval_samples": 800
        }
    }
    
    with open(os.path.join(submission_dir, "model_info.json"), "w", encoding="utf-8") as f:
        json.dump(model_info, f, indent=2, ensure_ascii=False)
    
    print("  âœ“ model_info.json")
    
    # åˆ›å»º.gitattributesï¼ˆHugging Faceè¦æ±‚ï¼‰
    print("\nğŸ“ åˆ›å»º.gitattributes...")
    gitattributes = """*.safetensors filter=lfs diff=lfs merge=lfs -text
*.bin filter=lfs diff=lfs merge=lfs -text
*.pt filter=lfs diff=lfs merge=lfs -text
*.h5 filter=lfs diff=lfs merge=lfs -text
"""
    
    with open(os.path.join(submission_dir, ".gitattributes"), "w", encoding="utf-8") as f:
        f.write(gitattributes)
    
    print("  âœ“ .gitattributes")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*80)
    print("ğŸ“Š æ¨¡å‹æäº¤åŒ…å‡†å¤‡å®Œæˆ")
    print("="*80)
    
    # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶
    files = os.listdir(submission_dir)
    total_size = 0
    
    print(f"\nğŸ“ æäº¤ç›®å½•: {submission_dir}/")
    print(f"ğŸ“„ æ–‡ä»¶åˆ—è¡¨:")
    
    for file in sorted(files):
        file_path = os.path.join(submission_dir, file)
        if os.path.isfile(file_path):
            size = os.path.getsize(file_path)
            total_size += size
            size_mb = size / (1024 * 1024)
            print(f"  - {file:30s} ({size_mb:.2f} MB)")
    
    print(f"\nğŸ’¾ æ€»å¤§å°: {total_size / (1024 * 1024):.2f} MB")
    
    print("\n" + "="*80)
    print("âœ… æ¨¡å‹å‡†å¤‡å®Œæˆï¼")
    print("="*80)
    print(f"\nğŸ“¦ æäº¤åŒ…ä½ç½®: {submission_dir}/")
    print(f"\nä¸‹ä¸€æ­¥:")
    print(f"  1. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶: cd {submission_dir} && ls -lh")
    print(f"  2. æµ‹è¯•æ¨ç†: python inference_example.py")
    print(f"  3. ä¸Šä¼ åˆ°Hugging Face: python upload_to_huggingface.py")
    print()

if __name__ == "__main__":
    prepare_model_submission()

