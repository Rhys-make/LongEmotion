# QAï¼ˆé—®ç­”ï¼‰ä»»åŠ¡è®­ç»ƒæŒ‡å—

æœ¬æŒ‡å—å°†å¸®åŠ©ä½ å®Œæˆ LongEmotion QA ä»»åŠ¡çš„å®Œæ•´è®­ç»ƒæµç¨‹ã€‚

## ğŸ“‹ ç›®å½•

1. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
2. [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
3. [æ¨¡å‹é€‰æ‹©](#æ¨¡å‹é€‰æ‹©)
4. [è®­ç»ƒæ¨¡å‹](#è®­ç»ƒæ¨¡å‹)
5. [æ¨ç†é¢„æµ‹](#æ¨ç†é¢„æµ‹)
6. [è¯„ä¼°ç»“æœ](#è¯„ä¼°ç»“æœ)
7. [ä¼˜åŒ–å»ºè®®](#ä¼˜åŒ–å»ºè®®)
8. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–

ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹ä¾èµ–ï¼š

```bash
pip install torch transformers datasets accelerate
pip install matplotlib numpy tqdm
```

### 2. æ£€æŸ¥ GPU

```bash
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

å¦‚æœæ²¡æœ‰ GPUï¼Œå¯ä»¥ä½¿ç”¨ CPU è®­ç»ƒï¼Œä½†é€Ÿåº¦ä¼šè¾ƒæ…¢ã€‚

---

## æ•°æ®å‡†å¤‡

### æ­¥éª¤ 1ï¼šä¸‹è½½å¹¶é¢„å¤„ç†æ•°æ®

è¿è¡Œæ•°æ®å‡†å¤‡è„šæœ¬ï¼š

```bash
# ä¸‹è½½ LongEmotion QA æµ‹è¯•é›† + å…¶ä»–è®­ç»ƒæ•°æ®é›†
python scripts/qa/prepare_datasets.py
```

**å¯é€‰å‚æ•°ï¼š**

```bash
# è‡ªå®šä¹‰æ•°æ®æ¥æº
python scripts/qa/prepare_datasets.py \
  --datasets squad narrativeqa hotpotqa \
  --max_samples_per_dataset 10000 \
  --use_synthetic \
  --output_dir data/qa
```

**ç”Ÿæˆçš„æ–‡ä»¶ï¼š**
- `data/qa/train.jsonl` - è®­ç»ƒé›†ï¼ˆ~15K+ æ ·æœ¬ï¼‰
- `data/qa/validation.jsonl` - éªŒè¯é›†ï¼ˆ~1.5K+ æ ·æœ¬ï¼‰
- `data/qa/test.jsonl` - æµ‹è¯•é›†ï¼ˆæ¥è‡ª LongEmotionï¼‰

### æ­¥éª¤ 2ï¼šæŸ¥çœ‹æ•°æ®

```bash
# æŸ¥çœ‹å‰ 3 æ¡è®­ç»ƒæ•°æ®
head -n 3 data/qa/train.jsonl
```

**æ•°æ®æ ¼å¼ç¤ºä¾‹ï¼š**

```json
{
  "id": 0,
  "problem": "æ ¹æ®ä¸Šè¿°å¿ƒç†å­¦æ–‡çŒ®ï¼Œä»€ä¹ˆæ˜¯è®¤çŸ¥å¤±è°ƒç†è®ºï¼Ÿ",
  "context": "è®¤çŸ¥å¤±è°ƒç†è®ºæ˜¯ç”±å¿ƒç†å­¦å®¶è±æ˜‚Â·è´¹æ–¯å»·æ ¼äº1957å¹´æå‡º...",
  "answer": "è®¤çŸ¥å¤±è°ƒç†è®ºæ˜¯æŒ‡å½“ä¸ªä½“åŒæ—¶æŒæœ‰çŸ›ç›¾çš„è®¤çŸ¥æ—¶äº§ç”Ÿçš„ä¸èˆ’é€‚å¿ƒç†çŠ¶æ€ã€‚"
}
```

---

## æ¨¡å‹é€‰æ‹©

æ ¹æ®ä½ çš„ç¡¬ä»¶èµ„æºé€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼š

### é€‰é¡¹ 1ï¼šBERT-baseï¼ˆæ¨èå…¥é—¨ï¼‰

**ä¼˜ç‚¹ï¼š** å¿«é€Ÿã€æ˜“ç”¨ã€èµ„æºéœ€æ±‚ä½  
**ç¼ºç‚¹ï¼š** ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼ˆ512 tokensï¼‰

```bash
--model_name bert-base-uncased
--model_type extractive
--max_length 512
```

### é€‰é¡¹ 2ï¼šLongformerï¼ˆæ¨èé•¿ä¸Šä¸‹æ–‡ï¼‰âœ…

**ä¼˜ç‚¹ï¼š** æ”¯æŒ 4096 tokensã€ä¸“ä¸ºé•¿æ–‡æ¡£è®¾è®¡  
**ç¼ºç‚¹ï¼š** è®­ç»ƒè¾ƒæ…¢ã€æ˜¾å­˜éœ€æ±‚è¾ƒé«˜

```bash
--model_name allenai/longformer-base-4096
--model_type extractive
--max_length 2048
```

### é€‰é¡¹ 3ï¼šMistral-7Bï¼ˆæ¨èé«˜è´¨é‡ç”Ÿæˆï¼‰

**ä¼˜ç‚¹ï¼š** ç”Ÿæˆè´¨é‡é«˜ã€ç†è§£èƒ½åŠ›å¼º  
**ç¼ºç‚¹ï¼š** æ˜¾å­˜éœ€æ±‚æé«˜ï¼ˆéœ€ 16GB+ GPUï¼‰

```bash
--model_name mistralai/Mistral-7B-Instruct-v0.2
--model_type generative
--max_length 4096
```

### é€‰é¡¹ 4ï¼šT5/mT5ï¼ˆseq2seqï¼‰

**ä¼˜ç‚¹ï¼š** é€‚åˆç”Ÿæˆå¼ä»»åŠ¡ã€ä¸­æ–‡æ”¯æŒå¥½  
**ç¼ºç‚¹ï¼š** éœ€è¦è°ƒæ•´è®­ç»ƒç­–ç•¥

```bash
--model_name google/mt5-base
--model_type seq2seq
--max_length 1024
```

---

## è®­ç»ƒæ¨¡å‹

### ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆBERT-baseï¼‰

```bash
python scripts/qa/train.py \
  --model_name bert-base-uncased \
  --model_type extractive \
  --batch_size 8 \
  --num_epochs 3 \
  --learning_rate 3e-5
```

### ğŸ”¥ æ¨èé…ç½®ï¼ˆLongformerï¼‰

```bash
python scripts/qa/train.py \
  --model_name allenai/longformer-base-4096 \
  --model_type extractive \
  --max_length 2048 \
  --batch_size 4 \
  --gradient_accumulation_steps 4 \
  --num_epochs 3 \
  --learning_rate 3e-5 \
  --use_amp \
  --patience 3
```

### ğŸ’» ä½èµ„æºé…ç½®ï¼ˆå°æ˜¾å­˜/CPUï¼‰

```bash
python scripts/qa/train.py \
  --model_name bert-base-uncased \
  --model_type extractive \
  --max_length 256 \
  --batch_size 2 \
  --gradient_accumulation_steps 8 \
  --num_epochs 5 \
  --learning_rate 3e-5 \
  --use_amp \
  --num_workers 0
```

### ğŸ¯ ç”Ÿæˆå¼æ¨¡å‹ï¼ˆMistralï¼‰

```bash
python scripts/qa/train.py \
  --model_name mistralai/Mistral-7B-Instruct-v0.2 \
  --model_type generative \
  --max_length 4096 \
  --batch_size 1 \
  --gradient_accumulation_steps 16 \
  --num_epochs 2 \
  --learning_rate 2e-5 \
  --use_amp \
  --patience 5
```

### ğŸ“Š ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ

```bash
python scripts/qa/train.py \
  --resume_from_checkpoint checkpoint/qa/latest_model \
  --num_epochs 2
```

---

## è®­ç»ƒå‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `--model_name` | é¢„è®­ç»ƒæ¨¡å‹åç§° | `allenai/longformer-base-4096` |
| `--model_type` | æ¨¡å‹ç±»å‹ | `extractive` / `generative` |
| `--max_length` | æœ€å¤§åºåˆ—é•¿åº¦ | 512-4096 |
| `--batch_size` | æ‰¹æ¬¡å¤§å° | 2-8 |
| `--gradient_accumulation_steps` | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° | 4-16 |
| `--num_epochs` | è®­ç»ƒè½®æ•° | 3-5 |
| `--learning_rate` | å­¦ä¹ ç‡ | 2e-5 ~ 5e-5 |
| `--use_amp` | æ··åˆç²¾åº¦è®­ç»ƒ | å»ºè®®å¼€å¯ |
| `--patience` | æ—©åœè€å¿ƒå€¼ | 3 |

---

## æ¨ç†é¢„æµ‹

### å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹

```bash
python scripts/qa/inference.py \
  --model_path checkpoint/qa/best_model \
  --test_data data/qa/test.jsonl \
  --output_file result/Emotion_QA_Result.jsonl \
  --model_type extractive
```

### ç”Ÿæˆå¼æ¨¡å‹æ¨ç†

```bash
python scripts/qa/inference.py \
  --model_path checkpoint/qa/best_model \
  --test_data data/qa/test.jsonl \
  --output_file result/Emotion_QA_Result.jsonl \
  --model_type generative \
  --max_answer_length 256
```

### æŸ¥çœ‹é¢„æµ‹ç»“æœ

```bash
# æŸ¥çœ‹å‰ 3 æ¡é¢„æµ‹
head -n 3 result/Emotion_QA_Result.jsonl
```

**è¾“å‡ºæ ¼å¼ï¼š**

```json
{"id": 0, "predicted_answer": "è®¤çŸ¥å¤±è°ƒç†è®ºæ˜¯æŒ‡å½“ä¸ªä½“åŒæ—¶æŒæœ‰çŸ›ç›¾çš„è®¤çŸ¥æ—¶..."}
{"id": 1, "predicted_answer": "æ ¹æ®æ–‡çŒ®ï¼Œæƒ…ç»ªè°ƒèŠ‚ç­–ç•¥åŒ…æ‹¬..."}
```

---

## è¯„ä¼°ç»“æœ

### è¯„ä¼°æ¨¡å‹æ€§èƒ½

å¦‚æœä½ æœ‰æ ‡æ³¨çš„æµ‹è¯•é›†ç­”æ¡ˆï¼Œå¯ä»¥è¿è¡Œï¼š

```bash
python scripts/qa/evaluate.py \
  --predictions result/Emotion_QA_Result.jsonl \
  --ground_truth data/qa/test.jsonl \
  --output_dir evaluation/qa
```

### è¯„ä¼°è¾“å‡º

**ç”Ÿæˆçš„æ–‡ä»¶ï¼š**
- `evaluation/qa/evaluation_metrics.json` - JSON æ ¼å¼æŒ‡æ ‡
- `evaluation/qa/evaluation_report.txt` - æ–‡æœ¬æ ¼å¼æŠ¥å‘Š
- `evaluation/qa/detailed_predictions.jsonl` - è¯¦ç»†é¢„æµ‹ï¼ˆå« F1ï¼‰
- `evaluation/qa/f1_distribution.png` - F1 åˆ†æ•°åˆ†å¸ƒå›¾
- `evaluation/qa/prediction_examples.txt` - æœ€ä½³/æœ€å·®é¢„æµ‹ç¤ºä¾‹

**ç¤ºä¾‹æŠ¥å‘Šï¼š**

```
============================================================
QA æ¨¡å‹è¯„ä¼°æŠ¥å‘Š
============================================================

æ ·æœ¬æ•°é‡: 500

F1 åˆ†æ•°:
  å¹³å‡å€¼: 0.7234
  æ ‡å‡†å·®: 0.1523
  æœ€å°å€¼: 0.2100
  æœ€å¤§å€¼: 1.0000
  ä¸­ä½æ•°: 0.7450

ç²¾ç¡®åŒ¹é… (Exact Match):
  å‡†ç¡®ç‡: 0.4520
  åŒ¹é…æ•°é‡: 226/500
============================================================
```

---

## ä¼˜åŒ–å»ºè®®

### 1. æå‡ F1 åˆ†æ•°

#### ç­–ç•¥ 1ï¼šä½¿ç”¨æ›´å¤§çš„æ¨¡å‹

```bash
# ä» BERT-base å‡çº§åˆ° Longformer
python scripts/qa/train.py \
  --model_name allenai/longformer-base-4096 \
  --max_length 2048
```

#### ç­–ç•¥ 2ï¼šå¢åŠ è®­ç»ƒæ•°æ®

```bash
# ä¸‹è½½æ›´å¤šæ•°æ®é›†
python scripts/qa/prepare_datasets.py \
  --datasets squad narrativeqa hotpotqa \
  --max_samples_per_dataset 20000
```

#### ç­–ç•¥ 3ï¼šè°ƒæ•´è¶…å‚æ•°

```bash
# é™ä½å­¦ä¹ ç‡ï¼Œå¢åŠ è®­ç»ƒè½®æ•°
python scripts/qa/train.py \
  --learning_rate 2e-5 \
  --num_epochs 5 \
  --warmup_ratio 0.1
```

#### ç­–ç•¥ 4ï¼šæ•°æ®å¢å¼º

åœ¨ `prepare_datasets.py` ä¸­æ·»åŠ å›è¯‘ã€åŒä¹‰è¯æ›¿æ¢ç­‰æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚

### 2. é™ä½å†…å­˜å ç”¨

#### ç­–ç•¥ 1ï¼šæ··åˆç²¾åº¦è®­ç»ƒ

```bash
python scripts/qa/train.py --use_amp
```

#### ç­–ç•¥ 2ï¼šæ¢¯åº¦ç´¯ç§¯

```bash
# ç­‰æ•ˆ batch_size = 2 Ã— 8 = 16
python scripts/qa/train.py \
  --batch_size 2 \
  --gradient_accumulation_steps 8
```

#### ç­–ç•¥ 3ï¼šå‡å°åºåˆ—é•¿åº¦

```bash
python scripts/qa/train.py --max_length 256
```

#### ç­–ç•¥ 4ï¼šæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆå¾…å®ç°ï¼‰

åœ¨æ¨¡å‹é…ç½®ä¸­å¯ç”¨ `gradient_checkpointing`ã€‚

### 3. åŠ é€Ÿè®­ç»ƒ

#### ç­–ç•¥ 1ï¼šä½¿ç”¨æ›´å°çš„éªŒè¯é›†

ä¿®æ”¹ `prepare_datasets.py` ä¸­çš„ `val_ratio` å‚æ•°ã€‚

#### ç­–ç•¥ 2ï¼šå‡å°‘æ—©åœè€å¿ƒå€¼

```bash
python scripts/qa/train.py --patience 2
```

#### ç­–ç•¥ 3ï¼šä½¿ç”¨å¤š GPUï¼ˆå¾…å®ç°ï¼‰

ä½¿ç”¨ `accelerate` æˆ– `torch.nn.DataParallel`ã€‚

---

## å¸¸è§é—®é¢˜

### Q1: CUDA Out of Memory é”™è¯¯

**è§£å†³æ–¹æ¡ˆï¼š**

```bash
# æ–¹æ¡ˆ 1ï¼šå‡å° batch_size å’Œ max_length
python scripts/qa/train.py \
  --batch_size 1 \
  --max_length 256 \
  --gradient_accumulation_steps 16

# æ–¹æ¡ˆ 2ï¼šä½¿ç”¨æ··åˆç²¾åº¦
python scripts/qa/train.py --use_amp

# æ–¹æ¡ˆ 3ï¼šä½¿ç”¨ CPUï¼ˆæ…¢ï¼‰
python scripts/qa/train.py --device cpu
```

### Q2: F1 åˆ†æ•°å¾ˆä½ï¼ˆ< 0.3ï¼‰

**å¯èƒ½åŸå› ï¼š**
1. è®­ç»ƒæ•°æ®ä¸æµ‹è¯•æ•°æ®åˆ†å¸ƒä¸åŒï¼ˆé¢†åŸŸä¸åŒ¹é…ï¼‰
2. æ¨¡å‹å®¹é‡ä¸è¶³
3. è®­ç»ƒä¸å……åˆ†

**è§£å†³æ–¹æ¡ˆï¼š**
1. æ”¶é›†å¿ƒç†å­¦é¢†åŸŸçš„ QA æ•°æ®
2. ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆLongformer / Mistralï¼‰
3. å¢åŠ è®­ç»ƒè½®æ•°

### Q3: è®­ç»ƒæ—¶é—´å¤ªé•¿

**è§£å†³æ–¹æ¡ˆï¼š**
1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆBERT-baseï¼‰
2. å‡å°‘è®­ç»ƒæ•°æ®é‡
3. ä½¿ç”¨ GPU è€Œé CPU
4. å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

### Q4: æŠ½å–å¼ vs ç”Ÿæˆå¼è¯¥é€‰å“ªä¸ªï¼Ÿ

| æ¨¡å‹ç±»å‹ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|---------|------|------|----------|
| **æŠ½å–å¼** | å¿«é€Ÿã€å‡†ç¡®ã€ç­”æ¡ˆæ¥è‡ªåŸæ–‡ | æ— æ³•æ€»ç»“ã€ç­”æ¡ˆå¿…é¡»åœ¨æ–‡ä¸­ | ç­”æ¡ˆåœ¨åŸæ–‡ä¸­æ˜ç¡®å‡ºç° |
| **ç”Ÿæˆå¼** | å¯æ€»ç»“ã€çµæ´»ã€ç†è§£æ›´æ·± | æ…¢ã€å¯èƒ½å¹»è§‰ã€éœ€å¤§æ¨¡å‹ | éœ€è¦æ€»ç»“æˆ–æ¨ç†çš„é—®ç­” |

**æ¨èï¼š**
- å¦‚æœæµ‹è¯•é›†ç­”æ¡ˆéƒ½æ¥è‡ªåŸæ–‡ï¼šä½¿ç”¨**æŠ½å–å¼**ï¼ˆLongformerï¼‰
- å¦‚æœéœ€è¦æ€»ç»“æˆ–æ¨ç†ï¼šä½¿ç”¨**ç”Ÿæˆå¼**ï¼ˆMistralï¼‰

### Q5: å¦‚ä½•å¤„ç†è¶…é•¿æ–‡æ¡£ï¼ˆ> 4096 tokensï¼‰ï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š**

1. **ä½¿ç”¨ Longformer** - æ”¯æŒæœ€é•¿ 4096 tokens
2. **æ–‡æ¡£åˆ†å—** - å°†é•¿æ–‡æ¡£åˆ‡åˆ†ä¸ºå¤šä¸ªç‰‡æ®µ
3. **æ»‘åŠ¨çª—å£** - ä½¿ç”¨é‡å çª—å£æå–ç­”æ¡ˆ
4. **å±‚æ¬¡åŒ–æ¨¡å‹** - å…ˆæ£€ç´¢ç›¸å…³æ®µè½ï¼Œå†è¿›è¡Œ QA

### Q6: æ²¡æœ‰ GPU èƒ½è®­ç»ƒå—ï¼Ÿ

å¯ä»¥ï¼Œä½†ä¼šå¾ˆæ…¢ã€‚å»ºè®®ï¼š

```bash
# CPU è®­ç»ƒé…ç½®
python scripts/qa/train.py \
  --model_name bert-base-uncased \
  --batch_size 2 \
  --max_length 128 \
  --num_epochs 1 \
  --device cpu
```

æˆ–è€…ä½¿ç”¨ Google Colab å…è´¹ GPUã€‚

---

## å®Œæ•´è®­ç»ƒæµç¨‹ç¤ºä¾‹

### æ–¹æ¡ˆ Aï¼šå¿«é€Ÿæµ‹è¯•ï¼ˆ10 åˆ†é’Ÿï¼‰

```bash
# 1. å‡†å¤‡å°‘é‡æ•°æ®
python scripts/qa/prepare_datasets.py \
  --datasets squad \
  --max_samples_per_dataset 1000

# 2. å¿«é€Ÿè®­ç»ƒ
python scripts/qa/train.py \
  --model_name bert-base-uncased \
  --batch_size 8 \
  --num_epochs 1 \
  --max_length 256

# 3. æ¨ç†
python scripts/qa/inference.py \
  --model_path checkpoint/qa/best_model \
  --model_type extractive

# 4. è¯„ä¼°
python scripts/qa/evaluate.py \
  --predictions result/Emotion_QA_Result.jsonl \
  --ground_truth data/qa/test.jsonl
```

### æ–¹æ¡ˆ Bï¼šå®Œæ•´è®­ç»ƒï¼ˆ2-4 å°æ—¶ï¼‰

```bash
# 1. å‡†å¤‡å®Œæ•´æ•°æ®
python scripts/qa/prepare_datasets.py \
  --datasets squad narrativeqa hotpotqa \
  --max_samples_per_dataset 10000 \
  --use_synthetic

# 2. ä½¿ç”¨ Longformer è®­ç»ƒ
python scripts/qa/train.py \
  --model_name allenai/longformer-base-4096 \
  --model_type extractive \
  --max_length 2048 \
  --batch_size 4 \
  --gradient_accumulation_steps 4 \
  --num_epochs 3 \
  --learning_rate 3e-5 \
  --use_amp \
  --patience 3

# 3. æ¨ç†
python scripts/qa/inference.py \
  --model_path checkpoint/qa/best_model \
  --model_type extractive \
  --max_length 2048

# 4. è¯„ä¼°
python scripts/qa/evaluate.py \
  --predictions result/Emotion_QA_Result.jsonl \
  --ground_truth data/qa/test.jsonl
```

### æ–¹æ¡ˆ Cï¼šæœ€ä½³æ€§èƒ½ï¼ˆ8+ å°æ—¶ï¼Œéœ€å¤§ GPUï¼‰

```bash
# 1. å‡†å¤‡æœ€å¤šæ•°æ®
python scripts/qa/prepare_datasets.py \
  --datasets squad narrativeqa hotpotqa \
  --max_samples_per_dataset 20000 \
  --use_synthetic

# 2. ä½¿ç”¨ Mistral-7B è®­ç»ƒ
python scripts/qa/train.py \
  --model_name mistralai/Mistral-7B-Instruct-v0.2 \
  --model_type generative \
  --max_length 4096 \
  --batch_size 1 \
  --gradient_accumulation_steps 16 \
  --num_epochs 2 \
  --learning_rate 2e-5 \
  --use_amp \
  --patience 5

# 3. æ¨ç†
python scripts/qa/inference.py \
  --model_path checkpoint/qa/best_model \
  --model_type generative \
  --max_length 4096 \
  --max_answer_length 256

# 4. è¯„ä¼°
python scripts/qa/evaluate.py \
  --predictions result/Emotion_QA_Result.jsonl \
  --ground_truth data/qa/test.jsonl
```

---

## ç›‘æ§è®­ç»ƒè¿›åº¦

### æŸ¥çœ‹è®­ç»ƒå†å²

```python
import json
import matplotlib.pyplot as plt

# åŠ è½½è®­ç»ƒå†å²
with open('checkpoint/qa/training_history.json', 'r') as f:
    history = json.load(f)

# ç»˜åˆ¶æŸå¤±æ›²çº¿
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Train Loss')
plt.plot(history['val_loss'], label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss Curve')

plt.subplot(1, 2, 2)
plt.plot(history['val_f1'], label='Val F1', color='green')
plt.xlabel('Epoch')
plt.ylabel('F1 Score')
plt.legend()
plt.title('F1 Score Curve')

plt.tight_layout()
plt.savefig('training_curves.png')
plt.show()
```

---

## ä¸‹ä¸€æ­¥

- ğŸ” **ä¼˜åŒ–æ¨¡å‹**ï¼šå°è¯•ä¸åŒçš„æ¨¡å‹æ¶æ„å’Œè¶…å‚æ•°
- ğŸ“Š **æ•°æ®å¢å¼º**ï¼šæ·»åŠ æ›´å¤šé¢†åŸŸç›¸å…³çš„è®­ç»ƒæ•°æ®
- ğŸš€ **æ¨¡å‹é›†æˆ**ï¼šç»“åˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ
- ğŸ“ **é”™è¯¯åˆ†æ**ï¼šåˆ†æè¯„ä¼°æŠ¥å‘Šä¸­çš„å¤±è´¥æ¡ˆä¾‹
- ğŸ¯ **é¢†åŸŸé€‚åº”**ï¼šåœ¨å¿ƒç†å­¦è¯­æ–™ä¸Šç»§ç»­é¢„è®­ç»ƒ

---

## å‚è€ƒèµ„æ–™

- [Longformer è®ºæ–‡](https://arxiv.org/abs/2004.05150)
- [SQuAD æ•°æ®é›†](https://rajpurkar.github.io/SQuAD-explorer/)
- [Hugging Face Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers/)
- [NarrativeQA æ•°æ®é›†](https://github.com/deepmind/narrativeqa)

---

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ [README.md](README.md) æˆ–æäº¤ Issueã€‚

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸ‰

