#!/usr/bin/env python3
"""
æ•°æ®å‡†å¤‡è„šæœ¬ï¼šä¸‹è½½å¹¶æ ¼å¼åŒ– GoEmotions å’Œ LongEmotion æ•°æ®é›†
ç”¨äºæƒ…ç»ªåˆ†ç±»ä»»åŠ¡çš„è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†å‡†å¤‡
"""

import json
import os
import random
import argparse
from pathlib import Path
from datasets import load_dataset
from tqdm import tqdm
import spacy
import numpy as np

# å…¨å±€å˜é‡ï¼šspacy æ¨¡å‹
nlp = None


def load_spacy_model():
    """åŠ è½½ spacy è‹±æ–‡æ¨¡å‹"""
    global nlp
    if nlp is None:
        try:
            print("ğŸ“¦ æ­£åœ¨åŠ è½½ spacy è‹±æ–‡æ¨¡å‹...")
            nlp = spacy.load("en_core_web_sm")
            print("âœ… spacy æ¨¡å‹åŠ è½½æˆåŠŸ")
        except OSError:
            print("âš ï¸  spacy è‹±æ–‡æ¨¡å‹æœªå®‰è£…ï¼Œæ­£åœ¨ä¸‹è½½...")
            print("   è¯·ç¨ç­‰ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
            os.system("python -m spacy download en_core_web_sm")
            nlp = spacy.load("en_core_web_sm")
            print("âœ… spacy æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½æˆåŠŸ")
    return nlp


def extract_subject(text):
    """
    ä½¿ç”¨ spacy æå–å¥å­çš„æƒ…æ„Ÿä¸»ä½“
    
    ç­–ç•¥ï¼š
    1. ä¼˜å…ˆæŸ¥æ‰¾ä¸»è¯­ï¼ˆåŒ…æ‹¬ä»£è¯ï¼‰
    2. å¦‚æœæ²¡æœ‰ä¸»è¯­ï¼ŒæŸ¥æ‰¾ç›´æ¥å®¾è¯­
    3. æŸ¥æ‰¾å‘½åå®ä½“ï¼ˆäººåã€åœ°åã€ç»„ç»‡ç­‰ï¼‰
    4. æå–ç¬¬ä¸€ä¸ªåè¯å—
    5. ä»¥ä¸Šéƒ½å¤±è´¥åˆ™è¿”å› "unknown"
    
    Args:
        text: è‹±æ–‡æ–‡æœ¬
    
    Returns:
        ä¸»ä½“å­—ç¬¦ä¸²
    """
    try:
        nlp_model = load_spacy_model()
        doc = nlp_model(text)
        
        # ç­–ç•¥1: æŸ¥æ‰¾ä¸»è¯­ï¼ˆnsubj, nsubjpassï¼‰- åŒ…æ‹¬ä»£è¯
        for token in doc:
            if token.dep_ in ("nsubj", "nsubjpass"):
                # è·å–ä¸»è¯­åŠå…¶ä¿®é¥°è¯
                subject_tokens = [token]
                for child in token.children:
                    if child.dep_ in ("compound", "amod", "det", "poss"):
                        subject_tokens.append(child)
                
                subject_tokens.sort(key=lambda x: x.i)
                subject = " ".join([t.text for t in subject_tokens])
                
                # å¯¹äºä»£è¯ï¼Œä¿ç•™å¸¸è§çš„äººç§°ä»£è¯
                if token.pos_ == "PRON":
                    # ä¿ç•™ I, you, he, she, we, they ç­‰
                    if token.text.lower() in ["i", "you", "he", "she", "we", "they", "it"]:
                        return token.text
                    else:
                        continue  # è·³è¿‡å…¶ä»–ä»£è¯ï¼Œç»§ç»­æŸ¥æ‰¾
                
                if len(subject) <= 50:
                    return subject
        
        # ç­–ç•¥2: æŸ¥æ‰¾ç›´æ¥å®¾è¯­ï¼ˆdobjï¼‰
        for token in doc:
            if token.dep_ == "dobj" and token.pos_ in ("NOUN", "PROPN"):
                dobj_tokens = [token]
                for child in token.children:
                    if child.dep_ in ("compound", "amod", "det"):
                        dobj_tokens.append(child)
                
                dobj_tokens.sort(key=lambda x: x.i)
                dobj = " ".join([t.text for t in dobj_tokens])
                if len(dobj) <= 50:
                    return dobj
        
        # ç­–ç•¥3: æŸ¥æ‰¾å‘½åå®ä½“ï¼ˆäººåã€ç»„ç»‡ç­‰ä¼˜å…ˆï¼‰
        for ent in doc.ents:
            if ent.label_ in ("PERSON", "ORG", "GPE", "PRODUCT"):
                if len(ent.text) <= 50:
                    return ent.text
        
        # ç­–ç•¥4: æå–ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„åè¯å—
        for chunk in doc.noun_chunks:
            # æ’é™¤å¤ªçŸ­æˆ–å¤ªé•¿çš„å—
            if 1 <= len(chunk.text.split()) <= 5 and len(chunk.text) <= 50:
                # æ’é™¤çº¯ä»£è¯å—
                if chunk.root.pos_ != "PRON" or chunk.root.text.lower() in ["i", "you", "he", "she", "we", "they"]:
                    return chunk.text
        
        # ç­–ç•¥5: æŸ¥æ‰¾ä»»æ„åè¯
        for token in doc:
            if token.pos_ in ("NOUN", "PROPN") and len(token.text) <= 50:
                return token.text
        
        # æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œè¿”å› unknown
        return "unknown"
    
    except Exception as e:
        return "unknown"


def ensure_dir(file_path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    directory = os.path.dirname(file_path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
        print(f"âœ… åˆ›å»ºç›®å½•: {directory}")


def convert_go_emotions_to_format(dataset, split_name, choices):
    """
    å°† GoEmotions æ•°æ®é›†è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼
    
    Args:
        dataset: GoEmotions æ•°æ®é›†çš„æŸä¸ªåˆ‡åˆ†
        split_name: åˆ‡åˆ†åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        choices: æƒ…ç»ªç±»åˆ«åˆ—è¡¨
    
    Returns:
        è½¬æ¢åçš„æ•°æ®åˆ—è¡¨
    """
    converted_data = []
    
    print(f"\nğŸ”„ æ­£åœ¨è½¬æ¢ {split_name} æ•°æ®é›†...")
    
    # ç¡®ä¿ spacy æ¨¡å‹å·²åŠ è½½
    load_spacy_model()
    
    for idx, example in enumerate(tqdm(dataset, desc=f"å¤„ç† {split_name}")):
        # è·å–æ–‡æœ¬
        text = example['text']
        
        # è·å–æ ‡ç­¾ï¼ˆGoEmotions æ˜¯å¤šæ ‡ç­¾ï¼Œæˆ‘ä»¬å–ç¬¬ä¸€ä¸ªæ ‡ç­¾ï¼‰
        labels = example['labels']
        
        # å¦‚æœæœ‰å¤šä¸ªæ ‡ç­¾ï¼Œå–ç¬¬ä¸€ä¸ªï¼›å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œè·³è¿‡
        if len(labels) == 0:
            continue
        
        # è·å–ç¬¬ä¸€ä¸ªæ ‡ç­¾çš„åç§°
        label_idx = labels[0]
        answer = choices[label_idx]
        
        # ä½¿ç”¨ spacy æå–ä¸»è¯­
        subject = extract_subject(text)
        
        # æ„å»ºè½¬æ¢åçš„æ ¼å¼
        converted_example = {
            "id": idx,
            "Context": text,
            "Subject": subject,
            "Choices": choices,
            "Answer": answer
        }
        
        converted_data.append(converted_example)
    
    print(f"âœ… {split_name} è½¬æ¢å®Œæˆï¼Œå…± {len(converted_data)} æ¡æ•°æ®")
    return converted_data


def save_to_jsonl(data, file_path):
    """ä¿å­˜æ•°æ®ä¸º JSONL æ ¼å¼"""
    ensure_dir(file_path)
    
    with open(file_path, 'w', encoding='utf-8') as f:
        for item in data:
            json.dump(item, f, ensure_ascii=False)
            f.write('\n')
    
    print(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {file_path}")


def download_and_process_go_emotions():
    """ä¸‹è½½å¹¶å¤„ç† GoEmotions æ•°æ®é›†"""
    print("\n" + "="*60)
    print("ğŸ“¥ å¼€å§‹ä¸‹è½½ GoEmotions æ•°æ®é›†...")
    print("="*60)
    
    # åŠ è½½æ•°æ®é›†
    ds = load_dataset("go_emotions")
    
    # è·å–æ‰€æœ‰æƒ…ç»ªç±»åˆ«
    choices = ds["train"].features["labels"].feature.names
    print(f"\nğŸ“‹ æƒ…ç»ªç±»åˆ«å…± {len(choices)} ä¸ª:")
    print(f"   {', '.join(choices[:10])}...")
    
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    project_root = Path(__file__).parent.parent.parent
    
    # å¤„ç†è®­ç»ƒé›†
    train_data = convert_go_emotions_to_format(ds["train"], "train", choices)
    train_path = project_root / "data" / "classification" / "train.jsonl"
    save_to_jsonl(train_data, str(train_path))
    
    # å¤„ç†éªŒè¯é›†
    val_data = convert_go_emotions_to_format(ds["validation"], "validation", choices)
    val_path = project_root / "data" / "classification" / "validation.jsonl"
    save_to_jsonl(val_data, str(val_path))
    
    return choices


def download_and_process_long_emotion():
    """ä¸‹è½½å¹¶å¤„ç† LongEmotion æ•°æ®é›†çš„æµ‹è¯•é›†"""
    print("\n" + "="*60)
    print("ğŸ“¥ å¼€å§‹ä¸‹è½½ LongEmotion æ•°æ®é›†ï¼ˆclassification éƒ¨åˆ†ï¼‰...")
    print("="*60)
    
    try:
        # æ–¹æ³•1: å°è¯•ç›´æ¥åŠ è½½ classification æ•°æ®æ–‡ä»¶
        from huggingface_hub import hf_hub_download
        
        # ä¸‹è½½ classification æ•°æ®æ–‡ä»¶
        print("æ­£åœ¨ä» Hugging Face ä¸‹è½½ Emotion_Classification.jsonl...")
        file_path = hf_hub_download(
            repo_id="LongEmotion/LongEmotion",
            filename="Emotion Classification/Emotion_Classification.jsonl",
            repo_type="dataset"
        )
        
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root = Path(__file__).parent.parent.parent
        
        # å¤„ç†æµ‹è¯•é›†
        print(f"\nğŸ”„ æ­£åœ¨è½¬æ¢ test æ•°æ®é›†...")
        test_data = []
        
        # è¯»å– JSONL æ–‡ä»¶
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc="å¤„ç† test"):
                if line.strip():
                    example = json.loads(line)
                    test_data.append(example)
        
        test_path = project_root / "data" / "classification" / "test.jsonl"
        save_to_jsonl(test_data, str(test_path))
        
        print(f"âœ… test è½¬æ¢å®Œæˆï¼Œå…± {len(test_data)} æ¡æ•°æ®")
        
    except Exception as e:
        print(f"âš ï¸  ä¸‹è½½ LongEmotion æ•°æ®é›†æ—¶å‡ºé”™: {e}")
        print("   å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•...")
        
        # æ–¹æ³•2: å°è¯•ä½¿ç”¨ load_dataset åŠ è½½å•ä¸ªæ•°æ®æ–‡ä»¶
        try:
            print("   ä½¿ç”¨å¤‡ç”¨æ–¹æ³•åŠ è½½æ•°æ®...")
            ds = load_dataset(
                "json",
                data_files={
                    "test": "hf://datasets/LongEmotion/LongEmotion/Emotion Classification/Emotion_Classification.jsonl"
                }
            )
            
            project_root = Path(__file__).parent.parent.parent
            test_data = list(ds["test"])
            
            test_path = project_root / "data" / "classification" / "test.jsonl"
            save_to_jsonl(test_data, str(test_path))
            
            print(f"âœ… test è½¬æ¢å®Œæˆï¼Œå…± {len(test_data)} æ¡æ•°æ®")
            
        except Exception as e2:
            print(f"âš ï¸  å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥äº†: {e2}")
            print("   è¯·æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥")


def create_combined_emotion_sample(sample1, sample2, all_choices):
    """
    åˆå¹¶ä¸¤ä¸ªæ ·æœ¬ï¼Œåˆ›å»ºç»„åˆæƒ…ç»ªæ ·æœ¬
    ä½¿è®­ç»ƒé›†æ›´æ¥è¿‘æµ‹è¯•é›†ï¼ˆåŒ…å«ç»„åˆæƒ…ç»ªï¼‰
    """
    # åˆå¹¶æ–‡æœ¬ï¼ˆæ¨¡æ‹Ÿé•¿æ–‡æœ¬ï¼‰
    combined_context = f"{sample1['Context']} {sample2['Context']}"
    
    # ç»„åˆæƒ…ç»ªæ ‡ç­¾
    emotion1 = sample1['Answer']
    emotion2 = sample2['Answer']
    
    # é¿å…é‡å¤
    if emotion1 == emotion2:
        combined_answer = emotion1
    else:
        # æŒ‰å­—æ¯é¡ºåºæ’åˆ—ï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
        emotions = sorted([emotion1, emotion2])
        combined_answer = f"{emotions[0]} & {emotions[1]}"
    
    # åˆ›å»ºæ–°çš„Choicesï¼ˆåŒ…å«åŸå§‹æƒ…ç»ªå’Œç»„åˆæƒ…ç»ªï¼‰
    # éšæœºé€‰æ‹©4-6ä¸ªå…¶ä»–æƒ…ç»ªä½œä¸ºå¹²æ‰°é¡¹
    num_distractors = random.randint(3, 6)
    distractors = random.sample([c for c in all_choices if c not in [emotion1, emotion2]], 
                                 min(num_distractors, len(all_choices) - 2))
    
    choices = [emotion1, emotion2]
    if emotion1 != emotion2:
        choices.append(combined_answer)
    choices.extend(distractors)
    random.shuffle(choices)
    
    return {
        'id': sample1['id'],
        'Context': combined_context,
        'Subject': sample1['Subject'],
        'Choices': choices,
        'Answer': combined_answer
    }


def augment_training_data(train_data, augment_ratio=0.3, seed=42):
    """
    å¢å¼ºè®­ç»ƒæ•°æ®ï¼Œç”Ÿæˆç»„åˆæƒ…ç»ªæ ·æœ¬
    
    Args:
        train_data: åŸå§‹è®­ç»ƒæ•°æ®
        augment_ratio: å¢å¼ºæ¯”ä¾‹ï¼ˆç”ŸæˆåŸå§‹æ•°æ®é‡çš„xå€ç»„åˆæ ·æœ¬ï¼‰
        seed: éšæœºç§å­
    
    Returns:
        å¢å¼ºåçš„è®­ç»ƒæ•°æ®
    """
    random.seed(seed)
    np.random.seed(seed)
    
    # è·å–æ‰€æœ‰å”¯ä¸€çš„æƒ…ç»ªæ ‡ç­¾
    all_emotions = set()
    for item in train_data:
        if 'Choices' in item:
            all_emotions.update(item['Choices'])
        if 'Answer' in item:
            all_emotions.add(item['Answer'])
    all_emotions = sorted(list(all_emotions))
    
    print(f"\nğŸ“Š æ•°æ®å¢å¼º:")
    print(f"   åŸå§‹æ•°æ®: {len(train_data)} æ¡")
    print(f"   æƒ…ç»ªç±»åˆ«: {len(all_emotions)} ä¸ª")
    
    # ç”Ÿæˆç»„åˆæƒ…ç»ªæ ·æœ¬
    num_augmented = int(len(train_data) * augment_ratio)
    augmented_samples = []
    
    print(f"   ç”Ÿæˆç»„åˆæƒ…ç»ªæ ·æœ¬: {num_augmented} æ¡...")
    
    for i in tqdm(range(num_augmented), desc="å¢å¼ºæ•°æ®"):
        # éšæœºé€‰æ‹©ä¸¤ä¸ªæ ·æœ¬
        sample1, sample2 = random.sample(train_data, 2)
        
        # åˆ›å»ºç»„åˆæ ·æœ¬
        combined_sample = create_combined_emotion_sample(sample1, sample2, all_emotions)
        combined_sample['id'] = len(train_data) + i
        
        augmented_samples.append(combined_sample)
    
    # åˆå¹¶åŸå§‹æ•°æ®å’Œå¢å¼ºæ•°æ®
    final_data = train_data + augmented_samples
    
    # æ‰“ä¹±é¡ºåº
    random.shuffle(final_data)
    
    # é‡æ–°åˆ†é…ID
    for i, item in enumerate(final_data):
        item['id'] = i
    
    # ç»Ÿè®¡ç»„åˆæƒ…ç»ªæ¯”ä¾‹
    combined_count = sum(1 for item in final_data if ' & ' in item['Answer'])
    
    print(f"\nâœ… æ•°æ®å¢å¼ºå®Œæˆ:")
    print(f"   åŸå§‹æ ·æœ¬: {len(train_data)}")
    print(f"   å¢å¼ºæ ·æœ¬: {len(augmented_samples)}")
    print(f"   æ€»æ ·æœ¬æ•°: {len(final_data)}")
    print(f"   ç»„åˆæƒ…ç»ªæ ·æœ¬: {combined_count} ({combined_count/len(final_data)*100:.1f}%)")
    
    return final_data


def main():
    """ä¸»å‡½æ•°"""
    # å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='æƒ…ç»ªåˆ†ç±»æ•°æ®é›†å‡†å¤‡è„šæœ¬')
    parser.add_argument('--augment', action='store_true',
                        help='æ˜¯å¦è¿›è¡Œæ•°æ®å¢å¼ºï¼ˆç”Ÿæˆç»„åˆæƒ…ç»ªæ ·æœ¬ï¼‰')
    parser.add_argument('--augment_ratio', type=float, default=0.3,
                        help='æ•°æ®å¢å¼ºæ¯”ä¾‹ï¼ˆé»˜è®¤0.3ï¼Œå³ç”Ÿæˆ30%%çš„ç»„åˆæ ·æœ¬ï¼‰')
    parser.add_argument('--seed', type=int, default=42,
                        help='éšæœºç§å­ï¼ˆé»˜è®¤42ï¼‰')
    args = parser.parse_args()
    
    print("\n" + "ğŸ¯" + "="*58)
    print("       æƒ…ç»ªåˆ†ç±»æ•°æ®é›†å‡†å¤‡è„šæœ¬")
    print("="*59 + "ğŸ¯\n")
    
    if args.augment:
        print("ğŸ“Œ æ•°æ®å¢å¼ºæ¨¡å¼: å¼€å¯")
        print(f"   å¢å¼ºæ¯”ä¾‹: {args.augment_ratio * 100:.0f}%")
    else:
        print("ğŸ“Œ æ•°æ®å¢å¼ºæ¨¡å¼: å…³é—­")
        print("   æç¤º: ä½¿ç”¨ --augment å‚æ•°å¯ç”¨æ•°æ®å¢å¼ºï¼Œæå‡ç»„åˆæƒ…ç»ªé¢„æµ‹å‡†ç¡®åº¦")
    
    # 1. å¤„ç† GoEmotions æ•°æ®é›†ï¼ˆè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼‰
    try:
        choices = download_and_process_go_emotions()
        print("\nâœ… GoEmotions æ•°æ®é›†å¤„ç†å®Œæˆï¼")
        
        # æ•°æ®å¢å¼ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if args.augment:
            # è¯»å–è®­ç»ƒæ•°æ®
            project_root = Path(__file__).parent.parent.parent
            train_path = project_root / "data" / "classification" / "train.jsonl"
            
            train_data = []
            with open(train_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        train_data.append(json.loads(line))
            
            # å¢å¼ºæ•°æ®
            augmented_data = augment_training_data(
                train_data, 
                augment_ratio=args.augment_ratio,
                seed=args.seed
            )
            
            # ä¿å­˜å¢å¼ºåçš„è®­ç»ƒæ•°æ®
            save_to_jsonl(augmented_data, str(train_path))
        
    except Exception as e:
        print(f"\nâŒ GoEmotions æ•°æ®é›†å¤„ç†å¤±è´¥: {e}")
        return
    
    # 2. å¤„ç† LongEmotion æ•°æ®é›†ï¼ˆæµ‹è¯•é›†ï¼‰
    try:
        download_and_process_long_emotion()
        print("\nâœ… LongEmotion æ•°æ®é›†å¤„ç†å®Œæˆï¼")
    except Exception as e:
        print(f"\nâš ï¸  LongEmotion æ•°æ®é›†å¤„ç†å‡ºç°é—®é¢˜: {e}")
    
    # å®Œæˆ
    print("\n" + "ğŸ‰" + "="*58)
    print("       æ‰€æœ‰æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
    print("="*59 + "ğŸ‰\n")
    
    # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶
    project_root = Path(__file__).parent.parent.parent
    data_dir = project_root / "data" / "classification"
    
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    for file in ["train.jsonl", "validation.jsonl", "test.jsonl"]:
        file_path = data_dir / file
        if file_path.exists():
            size = file_path.stat().st_size / 1024 / 1024  # MB
            print(f"   âœ“ {file} ({size:.2f} MB)")
        else:
            print(f"   âœ— {file} (æœªç”Ÿæˆ)")
    
    print("\nğŸ’¡ æç¤º: æ•°æ®é›†å·²ä¿å­˜åˆ° data/classification/ ç›®å½•")


if __name__ == "__main__":
    main()

