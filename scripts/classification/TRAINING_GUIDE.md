# æƒ…ç»ªåˆ†ç±»è®­ç»ƒæ•™ç¨‹

## ğŸ“‹ ç›®å½•
- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
- [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
- [æ¨¡å‹è®­ç»ƒ](#æ¨¡å‹è®­ç»ƒ)
- [æ¨¡å‹è¯„ä¼°](#æ¨¡å‹è¯„ä¼°)
- [æ¨ç†é¢„æµ‹](#æ¨ç†é¢„æµ‹)
- [è®­ç»ƒæŠ€å·§](#è®­ç»ƒæŠ€å·§)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–

```bash
pip install torch transformers datasets scikit-learn matplotlib seaborn
```

### 2. éªŒè¯å®‰è£…

```python
import torch
import transformers
print(f"PyTorch: {torch.__version__}")
print(f"Transformers: {transformers.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
```

---

## æ•°æ®å‡†å¤‡

### 1. è¿è¡Œæ•°æ®å‡†å¤‡è„šæœ¬

å¦‚æœè¿˜æ²¡æœ‰å‡†å¤‡æ•°æ®ï¼š

```bash
cd C:\Users\25138\project\LongEmotion
python scripts/classification/prepare_datasets.py
```

### 2. æ£€æŸ¥æ•°æ®æ–‡ä»¶

ç¡®è®¤ä»¥ä¸‹æ–‡ä»¶å·²ç”Ÿæˆï¼š
```
data/classification/
â”œâ”€â”€ train.jsonl        (è®­ç»ƒé›†ï¼Œçº¦43,411æ¡)
â”œâ”€â”€ validation.jsonl   (éªŒè¯é›†ï¼Œçº¦5,427æ¡)
â””â”€â”€ test.jsonl        (æµ‹è¯•é›†ï¼Œçº¦201æ¡)
```

### 3. æ•°æ®æ ¼å¼

æ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼š
```json
{
    "id": 0,
    "Context": "I love this new feature!",
    "Subject": "I",
    "Choices": ["admiration", "joy", "anger", ...],
    "Answer": "joy"
}
```

---

## æ¨¡å‹è®­ç»ƒ

### å¿«é€Ÿå¼€å§‹ï¼ˆæ¨èé…ç½®ï¼‰

```bash
python scripts/classification/train.py \
    --model_name roberta-large \
    --batch_size 8 \
    --num_epochs 5 \
    --learning_rate 2e-5 \
    --use_amp \
    --gradient_accumulation_steps 2 \
    --output_dir checkpoint/classification
```

### å®Œæ•´å‚æ•°è¯´æ˜

#### å¿…é€‰å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--train_data` | `data/classification/train.jsonl` | è®­ç»ƒæ•°æ®è·¯å¾„ |
| `--val_data` | `data/classification/validation.jsonl` | éªŒè¯æ•°æ®è·¯å¾„ |
| `--output_dir` | `checkpoint/classification` | æ¨¡å‹ä¿å­˜è·¯å¾„ |

#### æ¨¡å‹å‚æ•°

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `--model_name` | `roberta-large` | é¢„è®­ç»ƒæ¨¡å‹<br>â€¢ `roberta-large` (æ¨è)<br>â€¢ `roberta-base` (å¿«é€Ÿ)<br>â€¢ `microsoft/deberta-v3-large` (æœ€ä½³) |
| `--max_length` | `512` | æœ€å¤§åºåˆ—é•¿åº¦ |

#### è®­ç»ƒå‚æ•°

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `--batch_size` | `8` | æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®GPUè°ƒæ•´ï¼‰|
| `--num_epochs` | `5` | è®­ç»ƒè½®æ•° |
| `--learning_rate` | `2e-5` | å­¦ä¹ ç‡ |
| `--warmup_ratio` | `0.1` | é¢„çƒ­æ¯”ä¾‹ |
| `--weight_decay` | `0.01` | æƒé‡è¡°å‡ |

#### ä¼˜åŒ–å‚æ•°

| å‚æ•° | è¯´æ˜ |
|------|------|
| `--use_amp` | å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå¼ºçƒˆæ¨èï¼ŒèŠ‚çœ50%æ˜¾å­˜ï¼‰|
| `--gradient_accumulation_steps` | æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæ˜¾å­˜ä¸è¶³æ—¶å¢åŠ ï¼‰|
| `--patience` | æ—©åœè€å¿ƒå€¼ï¼ˆé»˜è®¤3ï¼‰|
| `--seed` | éšæœºç§å­ï¼ˆé»˜è®¤42ï¼‰|

### ä¸åŒé…ç½®ç¤ºä¾‹

#### é…ç½®1: é«˜æ€§èƒ½ï¼ˆGPU 24GB+ï¼‰

```bash
python scripts/classification/train.py \
    --model_name microsoft/deberta-v3-large \
    --batch_size 16 \
    --num_epochs 8 \
    --learning_rate 1e-5 \
    --use_amp \
    --patience 5
```

#### é…ç½®2: æ ‡å‡†é…ç½®ï¼ˆGPU 16GBï¼‰â­ æ¨è

```bash
python scripts/classification/train.py --model_name roberta-large --batch_size 4 --num_epochs 5 --learning_rate 2e-5 --use_amp --gradient_accumulation_steps 4 --max_length 256 å¼º
```
python scripts/classification/train.py --model_name roberta-base --batch_size 8 --num_epochs 5 --use_amp --gradient_accumulation_steps 2 ä¸€èˆ¬


è¯„ä¼°
# âœ… ä½¿ç”¨ best_model
python scripts/classification/evaluate.py --model_path checkpoint/classification/best_model --data_path data/classification/validation.jsonl

æ¨ç†
# âœ… ä½¿ç”¨ best_model

python scripts/classification/inference.py --model_path checkpoint/classification/best_model --test_data data/classification/test.jsonl
ç»§ç»­è®­ç»ƒ
# å¯ä»¥ä½¿ç”¨ latest_model
python scripts/classification/train.py --resume_from_checkpoint checkpoint/classification/latest_model --num_epochs 10


python scripts/classification/train.py --resume_from_checkpoint checkpoint/classification/latest_model --num_epochs 1 --batch_size 2 --gradient_accumulation_steps 4 --use_amp --num_workers 0 --max_length 512