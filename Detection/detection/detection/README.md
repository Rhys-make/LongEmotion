# Emotion Detection Model - LongEmotion

æƒ…æ„Ÿæ£€æµ‹æ¨¡å‹ï¼ŒåŸºäºBERTçš„ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»å™¨

## ğŸ“Š æ¨¡å‹ä¿¡æ¯

- **åŸºç¡€æ¨¡å‹**: bert-base-chinese
- **ä»»åŠ¡ç±»å‹**: 6åˆ†ç±»æƒ…æ„Ÿæ£€æµ‹
- **éªŒè¯å‡†ç¡®ç‡**: 91.47%
- **æ¡†æ¶**: PyTorch + Transformers

## ğŸ·ï¸ æƒ…æ„Ÿç±»åˆ«

æ¨¡å‹å¯ä»¥è¯†åˆ«ä»¥ä¸‹6ç§æƒ…æ„Ÿï¼š
- `sadness` (æ‚²ä¼¤)
- `joy` (å¿«ä¹)
- `love` (çˆ±)
- `anger` (æ„¤æ€’)
- `fear` (ææƒ§)
- `surprise` (æƒŠè®¶)

## ğŸ“ æ–‡ä»¶è¯´æ˜

```
detection_hug/
â”œâ”€â”€ model.pt                      # æ¨¡å‹æƒé‡æ–‡ä»¶
â”œâ”€â”€ config.json                   # æ¨¡å‹é…ç½®
â”œâ”€â”€ tokenizer_config.json         # åˆ†è¯å™¨é…ç½®
â”œâ”€â”€ vocab.txt                     # è¯è¡¨
â”œâ”€â”€ special_tokens_map.json       # ç‰¹æ®Šç¬¦å·æ˜ å°„
â”œâ”€â”€ detection_model.py            # æ¨¡å‹å®šä¹‰
â”œâ”€â”€ inference_example.py          # æ¨ç†ç¤ºä¾‹
â””â”€â”€ README.md                     # æœ¬æ–‡ä»¶
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

```bash
pip install torch transformers
```

### åŸºæœ¬ä½¿ç”¨

```python
import torch
from transformers import BertTokenizer
from detection_model import EmotionDetectionModel

# 1. åŠ è½½åˆ†è¯å™¨
tokenizer = BertTokenizer.from_pretrained(".")

# 2. åŠ è½½æ¨¡å‹
model = EmotionDetectionModel(
    model_name="bert-base-chinese",
    num_emotions=6
)
checkpoint = torch.load("model.pt", map_location="cpu")
model.load_state_dict(checkpoint)
model.eval()

# 3. é¢„æµ‹
text = "æˆ‘ä»Šå¤©å¾ˆå¼€å¿ƒï¼"
encoding = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)
outputs = model(**encoding)
predicted_emotion = torch.argmax(outputs['logits'], dim=-1).item()

# æƒ…æ„Ÿæ˜ å°„
emotions = ["sadness", "joy", "love", "anger", "fear", "surprise"]
print(f"é¢„æµ‹æƒ…æ„Ÿ: {emotions[predicted_emotion]}")
```

### ä½¿ç”¨æ¨ç†è„šæœ¬

```bash
python inference_example.py
```

## ğŸ“ˆ æ¨¡å‹æ€§èƒ½

- **æ•°æ®é›†**: dair-ai/emotion (ä¸­æ–‡æƒ…æ„Ÿæ•°æ®)
- **éªŒè¯å‡†ç¡®ç‡**: 91.47%
- **å¹³å‡ç½®ä¿¡åº¦**: 89.27%
- **æœ€å¤§åºåˆ—é•¿åº¦**: 512 tokens

## ğŸ”§ æŠ€æœ¯ç»†èŠ‚

### æ¨¡å‹æ¶æ„

```
EmotionDetectionModel
â”œâ”€â”€ BERT Encoder (bert-base-chinese)
â”‚   â””â”€â”€ 768-dim hidden states
â”œâ”€â”€ Dropout (p=0.1)
â”œâ”€â”€ Linear Layer (768 â†’ 384)
â”œâ”€â”€ ReLU Activation
â”œâ”€â”€ Dropout (p=0.1)
â””â”€â”€ Output Layer (384 â†’ 6)
```

### è®­ç»ƒå‚æ•°

- **ä¼˜åŒ–å™¨**: AdamW
- **å­¦ä¹ ç‡**: 2e-5
- **æ‰¹æ¬¡å¤§å°**: 16
- **æœ€å¤§é•¿åº¦**: 512

## ğŸ“ å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨æ­¤æ¨¡å‹ï¼Œè¯·æ³¨æ˜ï¼š
```
LongEmotion Detection Model
- åŸºäº bert-base-chinese
- è®­ç»ƒäº dair-ai/emotion æ•°æ®é›†
```

## ğŸ“§ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜ï¼Œè¯·é€šè¿‡é¡¹ç›®ä»“åº“è”ç³»ã€‚

---

**License**: éµå¾ª bert-base-chinese çš„è®¸å¯åè®®
**Created**: 2025

